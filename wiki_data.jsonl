{"id": "wiki_hyperspectral", "query_word": "hyperspectral", "title": "Hyperspectral imaging", "summary": "Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers, which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique \"fingerprints\" in the electromagnetic spectrum. Known as spectral signatures, these \"fingerprints\" enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields.", "text": "Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers, which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique \"fingerprints\" in the electromagnetic spectrum. Known as spectral signatures, these \"fingerprints\" enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields. Sensors Figuratively speaking, hyperspectral sensors collect information as a set of \"images.\" Each image represents a narrow wavelength range of the electromagnetic spectrum, also known as a spectral band. These \"images\" are combined to form a three-dimensional (x, y, λ) hyperspectral data cube for processing and analysis, where x and y represent two spatial dimensions of the scene, and λ represents the spectral dimension (comprising a range of wavelengths). Technically speaking, there are four ways for sensors to sample the hyperspectral cube: spatial scanning, spectral scanning, snapshot imaging, and spatio-spectral scanning. Hyperspectral cubes are generated from airborne sensors like NASA's Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), or from satellites like NASA's EO-1 with its hyperspectral instrument Hyperion. However, for many development and validation studies, handheld sensors are used. The precision of these sensors is typically measured in spectral resolution, which is the width of each band of the spectrum that is captured. If the scanner detects a large number of fairly narrow frequency bands, it is possible to identify objects even if they are only captured in a handful of pixels. However, spatial resolution is a factor in addition to spectral resolution. If the pixels are too large, then multiple objects are captured in the same pixel and become difficult to identify. If the pixels are too small, then the intensity captured by each sensor cell is low, and the decreased signal-to-noise ratio reduces the reliability of measured features. The acquisition and processing of hyperspectral images is also referred to as imaging spectroscopy or, with reference to the hyperspectral cube, as 3D spectroscopy. Scanning techniques There are four basic techniques for acquiring the three-dimensional (x, y, λ) dataset of a hyperspectral cube. The choice of technique depends on the specific application, seeing that each technique has context-dependent advantages and disadvantages. Spatial scanning In spatial scanning, each two-dimensional (2D) sensor output represents a full slit spectrum (x, λ). Hyperspectral imaging (HSI) devices for spatial scanning obtain slit spectra by projecting a strip of the scene onto a slit and dispersing the slit image with a prism or a grating. These systems have the drawback of having the image analyzed per lines (with a push broom scanner) and also having some mechanical parts integrated into the optical train. With these line-scan cameras, the spatial dimension is collected through platform movement or scanning. This requires stabilized mounts or accurate pointing information to 'reconstruct' the image. Nonetheless, line-scan systems are particularly common in remote sensing, where it is sensible to use mobile platforms. Line-scan systems are also used to scan materials moving by on a conveyor belt. A special case of line scanning is point scanning (with a whisk broom scanner), where a point-like aperture is used instead of a slit, and the sensor is essentially one-dimensional instead of 2D. Spectral scanning In spectral scanning, each 2D sensor output represents a monochromatic (i.e. single wavelength), spatial (x, y)-map of the scene. HSI devices for spectral scanning are typically based on optical band-pass filters (either tunable or fixed). The scene is spectrally scanned by exchanging one filter after another while the platform remains stationary. In such \"staring\", wavelength scanning systems, spectral smearing can occur if there is movement within the scene, invalidating spectral correlation/detection. Nonetheless, there is the advantage of being able to pick and choose spectral bands, and having a direct representation of the two spatial dimensions of the scene. If the imaging system is used on a moving platform, such as an airplane, acquired images at different wavelengths corresponds to different areas of the scene. The spatial features on each of the images may be used to realign the pixels. Non-scanning In non-scanning, a single 2D sensor output contains all spatial (x, y) and spectral (λ) data. HSI devices for non-scanning yield the full datacube at once, without any scanning. Figuratively speaking, a single snapshot represents a perspective projection of the datacube, from which its three-dimensional structure can be reconstructed. The most prominent benefits of these snapshot hyperspectral imaging systems are the snapshot advantage (higher light throughput) and shorter acquisition time. A number of systems have been designed, including computed tomographic imaging spectrometry (CTIS), fiber-reformatting imaging spectrometry (FRIS), integral field spectroscopy with lenslet arrays (IFS-L), multi-aperture integral field spectrometer (Hyperpixel Array), integral field spectroscopy with image slicing mirrors (IFS-S), image-replicating imaging spectrometry (IRIS), filter stack spectral decomposition (FSSD), coded aperture snapshot spectral imaging (CASSI), image mapping spectrometry (IMS), and multispectral Sagnac interferometry (MSI). However, computational effort and manufacturing costs are high. In an effort to reduce the computational demands and potentially the high cost of non-scanning hyperspectral instrumentation, prototype devices based on Multivariate Optical Computing have been demonstrated. These devices have been based on the Multivariate Optical Element spectral calculation engine or the Spatial Light Modulator spectral calculation engine. In these platforms, chemical information is calculated in the optical domain prior to imaging such that the chemical image relies on conventional camera systems with no further computing. As a disadvantage of these systems, no spectral information is ever acquired, i.e. only the chemical information, such that post processing or reanalysis is not possible. Spatiospectral scanning In spatiospectral scanning, each 2D sensor output represents a wavelength-coded (\"rainbow-colored\", λ = λ(y)), spatial (x, y)-map of the scene. A prototype for this technique, introduced in 2014, consists of a camera at some non-zero distance behind a basic slit spectroscope (slit + dispersive element). Advanced spatiospectral scanning systems can be obtained by placing a dispersive element before a spatial scanning system. Scanning can be achieved by moving the whole system relative to the scene, by moving the camera alone, or by moving the slit alone. Spatiospectral scanning unites some advantages of spatial and spectral scanning, thereby alleviating some of their disadvantages. Distinguishing hyperspectral from multispectral imaging Hyperspectral imaging is part of a class of techniques commonly referred to as spectral imaging or spectral analysis. The term \"hyperspectral imaging\" derives from the development of NASA's Airborne Imaging Spectrometer (AIS) and AVIRIS in the mid-1980s. Although NASA prefers the earlier term \"imaging spectroscopy\" over \"hyperspectral imaging,\" use of the latter term has become more prevalent in scientific and non-scientific language. In a peer reviewed letter, experts recommend using the terms \"imaging spectroscopy\" or \"spectral imaging\" and avoiding exaggerated prefixes such as \"hyper-,\" \"super-\" and \"ultra-,\" to prevent misnomers in discussion. Hyperspectral imaging is related to multispectral imaging. The distinction between hyper- and multi-band is sometimes based incorrectly on an arbitrary \"number of bands\" or on the type of measurement. Hyperspectral imaging (HSI) uses continuous and contiguous ranges of wavelengths (e.g. 400 - 1100 nm in steps of 1 nm) whilst multiband imaging (MSI) uses a subset of targeted wavelengths at chosen locations (e.g. 400 - 1100 nm in steps of 20 nm). Multiband imaging deals with several images at discrete and somewhat narrow bands. Being \"discrete and somewhat narrow\" is what distinguishes multispectral imaging in the visible wavelength from color photography. A multispectral sensor may have many bands covering the spectrum from the visible to the longwave infrared. Multispectral images do not produce the \"spectrum\" of an object. Landsat is a prominent practical example of multispectral imaging. Hyperspectral dea", "canonical_url": "https://en.wikipedia.org/wiki/Hyperspectral_imaging", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:38.913284", "metadata": {"word_count": 253, "text_length": 22815}}
{"id": "wiki_wavelength", "query_word": "wavelength", "title": "Wavelength", "summary": "In physics and mathematics, wavelength or spatial period of a wave or periodic function is the distance over which the wave's shape repeats. In other words, it is the distance between consecutive corresponding points of the same phase on the wave, such as two adjacent crests, troughs, or zero crossings. Wavelength is a characteristic of both traveling waves and standing waves, as well as other spatial wave patterns. The inverse of the wavelength is called the spatial frequency. Wavelength is commonly designated by the Greek letter lambda (λ). For a modulated wave, wavelength may refer to the carrier wavelength of the signal. The term wavelength may also apply to the repeating envelope of modulated waves or waves formed by interference of several sinusoids. Assuming a sinusoidal wave moving at a fixed wave speed, wavelength is inversely proportional to the frequency of the wave: waves with higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. Wavelength depends on the medium (for example, vacuum, air, or water) that a wave travels through. Examples of waves are sound waves, light, water waves, and periodic electrical signals in a conductor. A sound wave is a variation in air pressure, while in light and other electromagnetic radiation the strength of the electric and the magnetic field vary. Water waves are variations in the height of a body of water. In a crystal lattice vibration, atomic positions vary. The range of wavelengths or frequencies for wave phenomena is called a spectrum. The name originated with the visible light spectrum but now can be applied to the entire electromagnetic spectrum as well as to a sound spectrum or vibration spectrum.", "text": "In physics and mathematics, wavelength or spatial period of a wave or periodic function is the distance over which the wave's shape repeats. In other words, it is the distance between consecutive corresponding points of the same phase on the wave, such as two adjacent crests, troughs, or zero crossings. Wavelength is a characteristic of both traveling waves and standing waves, as well as other spatial wave patterns. The inverse of the wavelength is called the spatial frequency. Wavelength is commonly designated by the Greek letter lambda (λ). For a modulated wave, wavelength may refer to the carrier wavelength of the signal. The term wavelength may also apply to the repeating envelope of modulated waves or waves formed by interference of several sinusoids. Assuming a sinusoidal wave moving at a fixed wave speed, wavelength is inversely proportional to the frequency of the wave: waves with higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. Wavelength depends on the medium (for example, vacuum, air, or water) that a wave travels through. Examples of waves are sound waves, light, water waves, and periodic electrical signals in a conductor. A sound wave is a variation in air pressure, while in light and other electromagnetic radiation the strength of the electric and the magnetic field vary. Water waves are variations in the height of a body of water. In a crystal lattice vibration, atomic positions vary. The range of wavelengths or frequencies for wave phenomena is called a spectrum. The name originated with the visible light spectrum but now can be applied to the entire electromagnetic spectrum as well as to a sound spectrum or vibration spectrum. Sinusoidal waves In linear media, any wave pattern can be described in terms of the independent propagation of sinusoidal components. The wavelength λ of a sinusoidal waveform traveling at constant speed v {\\displaystyle v} is given by λ = v f , {\\displaystyle \\lambda ={\\frac {v}{f}}\\,\\,,} where v {\\displaystyle v} is called the phase speed (magnitude of the phase velocity) of the wave and f {\\displaystyle f} is the wave's frequency. In a dispersive medium, the phase speed itself depends upon the frequency of the wave, making the relationship between wavelength and frequency nonlinear. In the case of electromagnetic radiation—such as light—in free space, the phase speed is the speed of light, about 3×108 m/s. Thus the wavelength of a 100 MHz electromagnetic (radio) wave is about: 3×108 m/s divided by 108 Hz = 3 m. The wavelength of visible light ranges from deep red, roughly 700 nm, to violet, roughly 400 nm (for other examples, see electromagnetic spectrum). For sound waves in air, the speed of sound is 343 m/s (at room temperature and atmospheric pressure). The wavelengths of sound frequencies audible to the human ear (20 Hz–20 kHz) are thus between approximately 17 m and 17 mm, respectively. Somewhat higher frequencies are used by bats so they can resolve targets smaller than 17 mm. Wavelengths in audible sound are much longer than those in visible light. Standing waves A standing wave is an undulatory motion that stays in one place. A sinusoidal standing wave includes stationary points of no motion, called nodes, and the wavelength is twice the distance between nodes. The upper figure shows three standing waves in a box. The walls of the box are considered to require the wave to have nodes at the walls of the box (an example of boundary conditions), thus determining the allowed wavelengths. For example, for an electromagnetic wave, if the box has ideal conductive walls, the condition for nodes at the walls results because the conductive walls cannot support a tangential electric field, forcing the wave to have zero amplitude at the wall. The stationary wave can be viewed as the sum of two traveling sinusoidal waves of oppositely directed velocities. Consequently, wavelength, period, and wave velocity are related just as for a traveling wave. For example, the speed of light can be determined from observation of standing waves in a metal box containing an ideal vacuum. Mathematical representation Traveling sinusoidal waves are often represented mathematically in terms of their velocity v (in the x direction), frequency f and wavelength λ as: y ( x , t ) = A cos ⁡ ( 2 π ( x λ − f t ) ) = A cos ⁡ ( 2 π λ ( x − v t ) ) {\\displaystyle y(x,\\ t)=A\\cos \\left(2\\pi \\left({\\frac {x}{\\lambda }}-ft\\right)\\right)=A\\cos \\left({\\frac {2\\pi }{\\lambda }}(x-vt)\\right)} where y is the value of the wave at any position x and time t, and A is the amplitude of the wave. They are also commonly expressed in terms of wavenumber k (2π times the reciprocal of wavelength) and angular frequency ω (2π times the frequency) as: y ( x , t ) = A cos ⁡ ( k x − ω t ) = A cos ⁡ ( k ( x − v t ) ) {\\displaystyle y(x,\\ t)=A\\cos \\left(kx-\\omega t\\right)=A\\cos \\left(k(x-vt)\\right)} in which wavelength and wavenumber are related to velocity and frequency as: k = 2 π λ = 2 π f v = ω v , {\\displaystyle k={\\frac {2\\pi }{\\lambda }}={\\frac {2\\pi f}{v}}={\\frac {\\omega }{v}},} or λ = 2 π k = 2 π v ω = v f . {\\displaystyle \\lambda ={\\frac {2\\pi }{k}}={\\frac {2\\pi v}{\\omega }}={\\frac {v}{f}}.} In the second form given above, the phase (kx − ωt) is often generalized to (k ⋅ r − ωt), by replacing the wavenumber k with a wave vector that specifies the direction and wavenumber of a plane wave in 3-space, parameterized by position vector r. In that case, the wavenumber k, the magnitude of k, is still in the same relationship with wavelength as shown above, with v being interpreted as scalar speed in the direction of the wave vector. The first form, using reciprocal wavelength in the phase, does not generalize as easily to a wave in an arbitrary direction. Generalizations to sinusoids of other phases, and to complex exponentials, are also common; see plane wave. The typical convention of using the cosine phase instead of the sine phase when describing a wave is based on the fact that the cosine is the real part of the complex exponential in the wave A e i ( k x − ω t ) . {\\displaystyle Ae^{i\\left(kx-\\omega t\\right)}.} General media The speed of a wave depends upon the medium in which it propagates. In particular, the speed of light in a medium is less than in vacuum, which means that the same frequency will correspond to a shorter wavelength in the medium than in vacuum, as shown in the figure at right. This change in speed upon entering a medium causes refraction, or a change in direction of waves that encounter the interface between media at an angle. For electromagnetic waves, this change in the angle of propagation is governed by Snell's law. The wave velocity in one medium not only may differ from that in another, but the velocity typically varies with wavelength. As a result, the change in direction upon entering a different medium changes with the wavelength of the wave. For electromagnetic waves the speed in a medium is governed by its refractive index according to v = c n ( λ 0 ) , {\\displaystyle v={\\frac {c}{n(\\lambda _{0})}},} where c is the speed of light in vacuum and n(λ0) is the refractive index of the medium at wavelength λ0, where the latter is measured in vacuum rather than in the medium. The corresponding wavelength in the medium is λ = λ 0 n ( λ 0 ) . {\\displaystyle \\lambda ={\\frac {\\lambda _{0}}{n(\\lambda _{0})}}.} When wavelengths of electromagnetic radiation are quoted, the wavelength in vacuum usually is intended unless the wavelength is specifically identified as the wavelength in some other medium. In acoustics, where a medium is essential for the waves to exist, the wavelength value is given for a specified medium. The variation in speed of light with wavelength is known as dispersion, and is also responsible for the familiar phenomenon in which light is separated into component colours by a prism. Separation occurs when the refractive index inside the prism varies with wavelength, so different wavelengths propagate at different speeds inside the prism, causing them to refract at different angles. The mathematical relationship that describes how the speed of light within a medium varies with wavelength is known as a dispersion relation. Nonuniform media Wavelength can be a useful concept even if the wave is not periodic in space. For example, in an ocean wave approaching shore, shown in the figure, the incoming wave undulates with a varying local wavelength that depends in part on the depth of the sea floor compared to the wave height. The analysis of the wave can be based upon comparison of the local wavelength with the local water depth. Waves that are sinusoidal in time but propagate through a medium whose properties vary with position (an inhomogeneous medium) may propagate at a velocity that varies with position, and as a result may not be sinusoidal in space. The figure at right shows an example. As the wave slows down, the wavelength gets shorter and the amplitude increases; after a place of maximum response, the short wavelength is associated with a high loss and the wave dies out. The analysis of differential equations of such systems is often done approximately, using the WKB method (also known as the Liouville–Green method). The method integrates phase through space using a local wavenumber, which can be interpreted as indicating a \"local wavelength\" of the solution as a function of time and space. This method treats the system locally as if it were uniform with the local properties; in particular, the local wave velocity associated with a frequency is the only thing needed to estimate the corresponding local wavenumber or wavelength. In addition, the method computes a slowly changing amplitude to satisfy other constraints of the equations or of the physical system, such as for conservation of energy in the wave. Crystals Waves in crystalline solids are not continuous, because they are composed of vibrations of discrete particles arranged in a r", "canonical_url": "https://en.wikipedia.org/wiki/Wavelength", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:39.598425", "metadata": {"word_count": 275, "text_length": 20566}}
{"id": "wiki_scattering", "query_word": "scattering", "title": "Scattering", "summary": "In physics, scattering is a wide range of physical processes where moving particles or radiation of some form, such as light or sound, are forced to deviate from a straight trajectory by localized non-uniformities (including particles and radiation) in the medium through which they pass. In conventional use, this also includes deviation of reflected radiation from the angle predicted by the law of reflection. Reflections of radiation that undergo scattering are often called diffuse reflections and unscattered reflections are called specular (mirror-like) reflections. Originally, the term was confined to light scattering (going back at least as far as Isaac Newton in the 17th century). As more \"ray\"-like phenomena were discovered, the idea of scattering was extended to them, so that William Herschel could refer to the scattering of \"heat rays\" (not then recognized as electromagnetic in nature) in 1800. John Tyndall, a pioneer in light scattering research, noted the connection between light scattering and acoustic scattering in the 1870s. Near the end of the 19th century, the scattering of cathode rays (electron beams) and X-rays was observed and discussed. With the discovery of subatomic particles (e.g. Ernest Rutherford in 1911) and the development of quantum theory in the 20th century, the sense of the term became broader as it was recognized that the same mathematical frameworks used in light scattering could be applied to many other phenomena. Scattering can refer to the consequences of particle-particle collisions between molecules, atoms, electrons, photons and other particles. Examples include: cosmic ray scattering in the Earth's upper atmosphere; particle collisions inside particle accelerators; electron scattering by gas atoms in fluorescent lamps; and neutron scattering inside nuclear reactors. The types of non-uniformities which can cause scattering, sometimes known as scatterers or scattering centers, are too numerous to list, but a small sample includes particles, bubbles, droplets, density fluctuations in fluids, crystallites in polycrystalline solids, defects in monocrystalline solids, surface roughness, cells in organisms, and textile fibers in clothing. The effects of such features on the path of almost any type of propagating wave or moving particle can be described in the framework of scattering theory. Some areas where scattering and scattering theory are significant include radar sensing, medical ultrasound, semiconductor wafer inspection, polymerization process monitoring, acoustic tiling, free-space communications and computer-generated imagery. Particle-particle scattering theory is important in areas such as particle physics, atomic, molecular, and optical physics, nuclear physics and astrophysics. In particle physics the quantum interaction and scattering of fundamental particles is described by the Scattering Matrix or S-Matrix, introduced and developed by John Archibald Wheeler and Werner Heisenberg. Scattering is quantified using many different concepts, including scattering cross section (σ), attenuation coefficients, the bidirectional scattering distribution function (BSDF), S-matrices, and mean free path.", "text": "In physics, scattering is a wide range of physical processes where moving particles or radiation of some form, such as light or sound, are forced to deviate from a straight trajectory by localized non-uniformities (including particles and radiation) in the medium through which they pass. In conventional use, this also includes deviation of reflected radiation from the angle predicted by the law of reflection. Reflections of radiation that undergo scattering are often called diffuse reflections and unscattered reflections are called specular (mirror-like) reflections. Originally, the term was confined to light scattering (going back at least as far as Isaac Newton in the 17th century). As more \"ray\"-like phenomena were discovered, the idea of scattering was extended to them, so that William Herschel could refer to the scattering of \"heat rays\" (not then recognized as electromagnetic in nature) in 1800. John Tyndall, a pioneer in light scattering research, noted the connection between light scattering and acoustic scattering in the 1870s. Near the end of the 19th century, the scattering of cathode rays (electron beams) and X-rays was observed and discussed. With the discovery of subatomic particles (e.g. Ernest Rutherford in 1911) and the development of quantum theory in the 20th century, the sense of the term became broader as it was recognized that the same mathematical frameworks used in light scattering could be applied to many other phenomena. Scattering can refer to the consequences of particle-particle collisions between molecules, atoms, electrons, photons and other particles. Examples include: cosmic ray scattering in the Earth's upper atmosphere; particle collisions inside particle accelerators; electron scattering by gas atoms in fluorescent lamps; and neutron scattering inside nuclear reactors. The types of non-uniformities which can cause scattering, sometimes known as scatterers or scattering centers, are too numerous to list, but a small sample includes particles, bubbles, droplets, density fluctuations in fluids, crystallites in polycrystalline solids, defects in monocrystalline solids, surface roughness, cells in organisms, and textile fibers in clothing. The effects of such features on the path of almost any type of propagating wave or moving particle can be described in the framework of scattering theory. Some areas where scattering and scattering theory are significant include radar sensing, medical ultrasound, semiconductor wafer inspection, polymerization process monitoring, acoustic tiling, free-space communications and computer-generated imagery. Particle-particle scattering theory is important in areas such as particle physics, atomic, molecular, and optical physics, nuclear physics and astrophysics. In particle physics the quantum interaction and scattering of fundamental particles is described by the Scattering Matrix or S-Matrix, introduced and developed by John Archibald Wheeler and Werner Heisenberg. Scattering is quantified using many different concepts, including scattering cross section (σ), attenuation coefficients, the bidirectional scattering distribution function (BSDF), S-matrices, and mean free path. Single and multiple scattering When radiation is only scattered by one localized scattering center, this is called single scattering. It is more common that scattering centers are grouped together; in such cases, radiation may scatter many times, in what is known as multiple scattering. The main difference between the effects of single and multiple scattering is that single scattering can usually be treated as a random phenomenon, whereas multiple scattering, somewhat counterintuitively, can be modeled as a more deterministic process because the combined results of a large number of scattering events tend to average out. Multiple scattering can thus often be modeled well with diffusion theory. Because the location of a single scattering center is not usually well known relative to the path of the radiation, the outcome, which tends to depend strongly on the exact incoming trajectory, appears random to an observer. This type of scattering would be exemplified by an electron being fired at an atomic nucleus. In this case, the atom's exact position relative to the path of the electron is unknown and would be unmeasurable, so the exact trajectory of the electron after the collision cannot be predicted. Single scattering is therefore often described by probability distributions. With multiple scattering, the randomness of the interaction tends to be averaged out by a large number of scattering events, so that the final path of the radiation appears to be a deterministic distribution of intensity. This is exemplified by a light beam passing through thick fog. Multiple scattering is highly analogous to diffusion, and the terms multiple scattering and diffusion are interchangeable in many contexts. Optical elements designed to produce multiple scattering are thus known as diffusers. Coherent backscattering, an enhancement of backscattering that occurs when coherent radiation is multiply scattered by a random medium, is usually attributed to weak localization. Not all single scattering is random, however. A well-controlled laser beam can be exactly positioned to scatter off a microscopic particle with a deterministic outcome, for instance. Such situations are encountered in radar scattering as well, where the targets tend to be macroscopic objects such as people or aircraft. Similarly, multiple scattering can sometimes have somewhat random outcomes, particularly with coherent radiation. The random fluctuations in the multiply scattered intensity of coherent radiation are called speckles. Speckle also occurs if multiple parts of a coherent wave scatter from different centers. In certain rare circumstances, multiple scattering may only involve a small number of interactions such that the randomness is not completely averaged out. These systems are considered to be some of the most difficult to model accurately. The description of scattering and the distinction between single and multiple scattering are tightly related to wave–particle duality. Theory Scattering theory is a framework for studying and understanding the scattering of waves and particles. Wave scattering corresponds to the collision and scattering of a wave with some material object, for instance sunlight scattered by rain drops to form a rainbow. Scattering also includes the interaction of billiard balls on a table, the Rutherford scattering (or angle change) of alpha particles by gold nuclei, the Bragg scattering (or diffraction) of electrons and X-rays by a cluster of atoms, and the inelastic scattering of a fission fragment as it traverses a thin foil. More precisely, scattering consists of the study of how solutions of partial differential equations, propagating freely \"in the distant past\", come together and interact with one another or with a boundary condition, and then propagate away \"to the distant future\". The direct scattering problem is the problem of determining the distribution of scattered radiation/particle flux basing on the characteristics of the scatterer. The inverse scattering problem is the problem of determining the characteristics of an object (e.g., its shape, internal constitution) from measurement data of radiation or particles scattered from the object. Attenuation due to scattering When the target is a set of many scattering centers whose relative position varies unpredictably, it is customary to think of a range equation whose arguments take different forms in different application areas. In the simplest case consider an interaction that removes particles from the \"unscattered beam\" at a uniform rate that is proportional to the incident number of particles per unit area per unit time ( I {\\displaystyle I} ), i.e. that d I d x = − Q I {\\displaystyle {\\frac {dI}{dx}}=-QI\\,\\!} where Q is an interaction coefficient and x is the distance traveled in the target. The above ordinary first-order differential equation has solutions of the form: I = I o e − Q Δ x = I o e − Δ x λ = I o e − σ ( η Δ x ) = I o e − ρ Δ x τ , {\\displaystyle I=I_{o}e^{-Q\\Delta x}=I_{o}e^{-{\\frac {\\Delta x}{\\lambda }}}=I_{o}e^{-\\sigma (\\eta \\Delta x)}=I_{o}e^{-{\\frac {\\rho \\Delta x}{\\tau }}},} where Io is the initial flux, path length Δx ≡ x − xo, the second equality defines an interaction mean free path λ, the third uses the number of targets per unit volume η to define an area cross-section σ, and the last uses the target mass density ρ to define a density mean free path τ. Hence one converts between these quantities via Q = 1/λ = ησ = ρ/τ, as shown in the figure at left. In electromagnetic absorption spectroscopy, for example, interaction coefficient (e.g. Q in cm−1) is variously called opacity, absorption coefficient, and attenuation coefficient. In nuclear physics, area cross-sections (e.g. σ in barns or units of 10−24 cm2), density mean free path (e.g. τ in grams/cm2), and its reciprocal the mass attenuation coefficient (e.g. in cm2/gram) or area per nucleon are all popular, while in electron microscopy the inelastic mean free path (e.g. λ in nanometers) is often discussed instead. Elastic and inelastic scattering The term \"elastic scattering\" implies that the internal states of the scattering particles do not change, and hence they emerge unchanged from the scattering process. In inelastic scattering, by contrast, the particles' internal state is changed, which may amount to exciting some of the electrons of a scattering atom, or the complete annihilation of a scattering particle and the creation of entirely new particles. The example of scattering in quantum chemistry is particularly instructive, as the theory is reasonably complex while still having a good foundation on which to build an intuitive understanding. When two atoms are scattered off one another, one can understand them as being the bound state solutions of so", "canonical_url": "https://en.wikipedia.org/wiki/Scattering", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:40.443189", "metadata": {"word_count": 451, "text_length": 20557}}
{"id": "wiki_band", "query_word": "band", "title": "Band", "summary": "Band or BAND may refer to:", "text": "Band or BAND may refer to: Places Bánd, a village in Hungary Band, Iran, a village in Urmia County, West Azerbaijan Province, Iran Band, Mureș, a commune in Romania Band-e Majid Khan, a village in Bukan County, West Azerbaijan Province, Iran People Band (surname), various people with the surname Arts, entertainment, and media Music Musical ensemble, a group of people who perform instrumental or vocal music Band (rock and pop), a small ensemble that plays rock or pop Concert band, an ensemble of woodwind, brass, and percussion instruments Dansband, band playing popular music for a partner-dancing audience Jazz band, a musical ensemble that plays jazz music Marching band, a group of instrumental musicians who generally perform outdoors School band, a group of student musicians who rehearse and perform instrumental music The Band, a Canadian-American rock and roll group The Band (album), The Band's eponymous 1969 album \"Bands\" (song), by American rapper Comethazine \"The Band\", a 2002 single by Mando Diao Other uses in arts, entertainment and media Band, nickname of Brazilian television network Rede Bandeirantes The Band (film), a 1978 Israeli film The Band (musical), a 2017 musical by Tim Firth with the music of Take That \"Band\" (Not Going Out), a 2012 television episode Clothing, jewelry, and accessories Armband or arm band Smart band a band with electronic component Microsoft Band, a smart band with smartwatch features from the software giant Bandolier or bandoleer, an ammunition belt Bands (neckwear), two pieces of cloth fitted around the neck as part of formal clothing for clergy, academics, and lawyers Belt (clothing), a flexible band or strap, typically made of leather or heavy cloth, and worn around the waist Strap, an elongated flap or ribbon, usually of fabric or leather Wedding band, a metal ring indicating the wearer is married Military Bands (Italian Army irregulars), 19th- and 20th-century military units in the service of the Italian \"Regio Esercito\" Female order of the Band, a medieval military order native to Spain Order of the Band, a medieval military order native to Spain Science and technology Band (algebra), an idempotent semigroup Band (order theory), a solid subset of an ordered vector space that contains its supremums Band (radio), a range of frequencies or wavelengths in radio and radar, specifically: Frequency band LTE frequency bands used for cellphone data Shortwave bands UMTS frequency bands used for cellphones BAND (software), a mobile app that facilitates group communication Band cell, a type of white blood cell Bird banding, placing a numbered metal band on a bird's leg for identification Electronic band structure of electrons in solid-state physics Gastric band, a human weight-control measure Signaling (telecommunications): In-band signaling Out-of-band Birds Are Not Dinosaurs, or BAND, a controversial stance on the origin of birds Society and government Band (First Nations Canada), the primary unit of First Nations Government in Canada Band society, a small group of humans in a simple form of society Tribe (Native American), a tribe, band, nation, or other group or community of Indigenous peoples in the United States Other uses Resistance band Rubber band The Band (professional wrestling), the Total Nonstop Wrestling name for the professional wrestling stable New World Order See also All pages with titles beginning with Band All pages with titles containing Band Band of Brothers (disambiguation) Bandage Banding (disambiguation) Bandy (disambiguation) Bend (disambiguation) Drum and bugle corps (disambiguation) Herd, a social grouping of certain animals of the same species Ribbon (disambiguation) Stripe (disambiguation) Bands (disambiguation)", "canonical_url": "https://en.wikipedia.org/wiki/Band", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:41.079959", "metadata": {"word_count": 6, "text_length": 3738}}
{"id": "wiki_material", "query_word": "material", "title": "Material", "summary": "A material is a substance or mixture of substances that constitutes an object. Materials can be pure or impure, living or non-living matter. Materials can be classified on the basis of their physical and chemical properties, or on their geological origin or biological function. Materials science is the study of materials, their properties, and their applications. Raw materials can be processed in different ways to influence their properties, by purification, shaping or the introduction of other materials. New materials can be produced from raw materials by synthesis. In industry, materials are inputs to manufacturing processes to produce products or more complex materials, and the nature and quantity of materials used may form part of the calculation for the cost of a product or delivery under contract, such as where contract costs are calculated on a \"time and materials\" basis.", "text": "A material is a substance or mixture of substances that constitutes an object. Materials can be pure or impure, living or non-living matter. Materials can be classified on the basis of their physical and chemical properties, or on their geological origin or biological function. Materials science is the study of materials, their properties, and their applications. Raw materials can be processed in different ways to influence their properties, by purification, shaping or the introduction of other materials. New materials can be produced from raw materials by synthesis. In industry, materials are inputs to manufacturing processes to produce products or more complex materials, and the nature and quantity of materials used may form part of the calculation for the cost of a product or delivery under contract, such as where contract costs are calculated on a \"time and materials\" basis. Historical elements Materials chart the history of humanity. The system of the three prehistoric ages (Stone Age, Bronze Age, Iron Age) were succeeded by historical ages: steel age in the 19th century, polymer age in the middle of the following century (plastic age) and silicon age in the second half of the 20th century. Classification by use Materials can be broadly categorized in terms of their use, for example: Building materials are used for construction Building insulation materials are used to retain heat within buildings Refractory materials are used for high-temperature applications Nuclear materials are used for nuclear power and weapons Aerospace materials are used in aircraft and other aerospace applications Biomaterials are used for applications interacting with living systems Material selection is a process to determine which material should be used for a given application. Classification by structure The relevant structure of materials has a different length scale depending on the material. The structure and composition of a material can be determined by microscopy or spectroscopy. Microstructure In engineering, materials can be categorised according to their microscopic structure: Plastics: a wide range of synthetic or semi-synthetic materials that use polymers as a main ingredient. Ceramics: non-metal, inorganic solids Glasses: amorphous solids Crystals: a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. Metals: pure or combined chemical elements with specific chemical bonding behavior Alloys: a mixture of chemical elements of which at least one is often a metal. Polymers: materials based on long carbon or silicon chains Hybrids: Combinations of multiple materials, for example composites. Larger-scale structure A metamaterial is any material engineered to have a property that is not found in naturally occurring materials, usually by combining several materials to form a composite and / or tuning the shape, geometry, size, orientation and arrangement to achieve the desired property. In foams and textiles, the chemical structure is less relevant to immediately observable properties than larger-scale material features: the holes in foams, and the weave in textiles. Classification by properties Materials can be compared and classified by their large-scale physical properties. Mechanical properties Mechanical properties determine how a material responds to applied forces. Examples include: Stiffness, Strength, Toughness, and Hardness. Thermal properties Materials may degrade or undergo changes of properties at different temperatures. Thermal properties also include the material's thermal conductivity and heat capacity, relating to the transfer and storage of thermal energy by the material. Other properties Materials can be compared and categorized by any quantitative measure of their behavior under various conditions. Notable additional properties include the optical, electrical, and magnetic behavior of materials. See also Hyle, the Greek term, relevant for the philosophy of matter Category:Materials References == External links ==", "canonical_url": "https://en.wikipedia.org/wiki/Material", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:41.710993", "metadata": {"word_count": 139, "text_length": 4126}}
{"id": "wiki_reflectance", "query_word": "reflectance", "title": "Reflectance", "summary": "The reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at the boundary. Reflectance is a component of the response of the electronic structure of the material to the electromagnetic field of light, and is in general a function of the frequency, or wavelength, of the light, its polarization, and the angle of incidence. The dependence of reflectance on the wavelength is called a reflectance spectrum or spectral reflectance curve.", "text": "The reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at the boundary. Reflectance is a component of the response of the electronic structure of the material to the electromagnetic field of light, and is in general a function of the frequency, or wavelength, of the light, its polarization, and the angle of incidence. The dependence of reflectance on the wavelength is called a reflectance spectrum or spectral reflectance curve. Mathematical definitions Hemispherical reflectance The hemispherical reflectance of a surface, denoted R, is defined as R = Φ e r Φ e i , {\\displaystyle R={\\frac {\\Phi _{\\mathrm {e} }^{\\mathrm {r} }}{\\Phi _{\\mathrm {e} }^{\\mathrm {i} }}},} where Φer is the radiant flux reflected by that surface and Φei is the radiant flux received by that surface. Spectral hemispherical reflectance The spectral hemispherical reflectance in frequency and spectral hemispherical reflectance in wavelength of a surface, denoted Rν and Rλ respectively, are defined as R ν = Φ e , ν r Φ e , ν i , {\\displaystyle R_{\\nu }={\\frac {\\Phi _{\\mathrm {e} ,\\nu }^{\\mathrm {r} }}{\\Phi _{\\mathrm {e} ,\\nu }^{\\mathrm {i} }}},} R λ = Φ e , λ r Φ e , λ i , {\\displaystyle R_{\\lambda }={\\frac {\\Phi _{\\mathrm {e} ,\\lambda }^{\\mathrm {r} }}{\\Phi _{\\mathrm {e} ,\\lambda }^{\\mathrm {i} }}},} where Φe,νr is the spectral radiant flux in frequency reflected by that surface; Φe,νi is the spectral radiant flux in frequency received by that surface; Φe,λr is the spectral radiant flux in wavelength reflected by that surface; Φe,λi is the spectral radiant flux in wavelength received by that surface. Directional reflectance The directional reflectance of a surface, denoted RΩ, is defined as R Ω = L e , Ω r L e , Ω i , {\\displaystyle R_{\\Omega }={\\frac {L_{\\mathrm {e} ,\\Omega }^{\\mathrm {r} }}{L_{\\mathrm {e} ,\\Omega }^{\\mathrm {i} }}},} where Le,Ωr is the radiance reflected by that surface; Le,Ωi is the radiance received by that surface. This depends on both the reflected direction and the incoming direction. In other words, it has a value for every combination of incoming and outgoing directions. It is related to the bidirectional reflectance distribution function and its upper limit is 1. Another measure of reflectance, depending only on the outgoing direction, is I/F, where I is the radiance reflected in a given direction and F is the incoming radiance averaged over all directions, in other words, the total flux of radiation hitting the surface per unit area, divided by π. This can be greater than 1 for a glossy surface illuminated by a source such as the sun, with the reflectance measured in the direction of maximum radiance (see also Seeliger effect). Spectral directional reflectance The spectral directional reflectance in frequency and spectral directional reflectance in wavelength of a surface, denoted RΩ,ν and RΩ,λ respectively, are defined as R Ω , ν = L e , Ω , ν r L e , Ω , ν i , {\\displaystyle R_{\\Omega ,\\nu }={\\frac {L_{\\mathrm {e} ,\\Omega ,\\nu }^{\\mathrm {r} }}{L_{\\mathrm {e} ,\\Omega ,\\nu }^{\\mathrm {i} }}},} R Ω , λ = L e , Ω , λ r L e , Ω , λ i , {\\displaystyle R_{\\Omega ,\\lambda }={\\frac {L_{\\mathrm {e} ,\\Omega ,\\lambda }^{\\mathrm {r} }}{L_{\\mathrm {e} ,\\Omega ,\\lambda }^{\\mathrm {i} }}},} where Le,Ω,νr is the spectral radiance in frequency reflected by that surface; Le,Ω,νi is the spectral radiance received by that surface; Le,Ω,λr is the spectral radiance in wavelength reflected by that surface; Le,Ω,λi is the spectral radiance in wavelength received by that surface. Again, one can also define a value of I/F (see above) for a given wavelength. Reflectivity For homogeneous and semi-infinite (see halfspace) materials, reflectivity is the same as reflectance. Reflectivity is the square of the magnitude of the Fresnel reflection coefficient, which is the ratio of the reflected to incident electric field; as such the reflection coefficient can be expressed as a complex number as determined by the Fresnel equations for a single layer, whereas the reflectance is always a positive real number. For layered and finite media, according to the CIE, reflectivity is distinguished from reflectance by the fact that reflectivity is a value that applies to thick reflecting objects. When reflection occurs from thin layers of material, internal reflection effects can cause the reflectance to vary with surface thickness. Reflectivity is the limit value of reflectance as the sample becomes thick; it is the intrinsic reflectance of the surface, hence irrespective of other parameters such as the reflectance of the rear surface. Another way to interpret this is that the reflectance is the fraction of electromagnetic power reflected from a specific sample, while reflectivity is a property of the material itself, which would be measured on a perfect machine if the material filled half of all space. Surface type Given that reflectance is a directional property, most surfaces can be divided into those that give specular reflection and those that give diffuse reflection. For specular surfaces, such as glass or polished metal, reflectance is nearly zero at all angles except at the appropriate reflected angle; that is the same angle with respect to the surface normal in the plane of incidence, but on the opposing side. When the radiation is incident normal to the surface, it is reflected back into the same direction. For diffuse surfaces, such as matte white paint, reflectance is uniform; radiation is reflected in all angles equally or near-equally. Such surfaces are said to be Lambertian. Most practical objects exhibit a combination of diffuse and specular reflective properties. Liquid reflectance Reflection of light occurs at a boundary at which the index of refraction changes. Specular reflection is calculated by the Fresnel equations. Fresnel reflection is directional and therefore does not contribute significantly to albedo which primarily diffuses reflection. A liquid surface may be wavy. Reflectance may be adjusted to account for waviness. Grating efficiency The generalization of reflectance to a diffraction grating, which disperses light by wavelength, is called diffraction efficiency. Other radiometric coefficients See also Bidirectional reflectance distribution function Colorimetry Emissivity Lambert's cosine law Transmittance Sun path Light Reflectance Value Albedo Reststrahlen effect Lyddane–Sachs–Teller relation References External links Reflectivity of metals Archived 2016-03-04 at the Wayback Machine. Reflectance Data.", "canonical_url": "https://en.wikipedia.org/wiki/Reflectance", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:42.325897", "metadata": {"word_count": 86, "text_length": 6622}}
{"id": "wiki_hyperspectral_imaging", "query_word": "hyperspectral imaging", "title": "Hyperspectral imaging", "summary": "Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers, which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique \"fingerprints\" in the electromagnetic spectrum. Known as spectral signatures, these \"fingerprints\" enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields.", "text": "Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers, which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique \"fingerprints\" in the electromagnetic spectrum. Known as spectral signatures, these \"fingerprints\" enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields. Sensors Figuratively speaking, hyperspectral sensors collect information as a set of \"images.\" Each image represents a narrow wavelength range of the electromagnetic spectrum, also known as a spectral band. These \"images\" are combined to form a three-dimensional (x, y, λ) hyperspectral data cube for processing and analysis, where x and y represent two spatial dimensions of the scene, and λ represents the spectral dimension (comprising a range of wavelengths). Technically speaking, there are four ways for sensors to sample the hyperspectral cube: spatial scanning, spectral scanning, snapshot imaging, and spatio-spectral scanning. Hyperspectral cubes are generated from airborne sensors like NASA's Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), or from satellites like NASA's EO-1 with its hyperspectral instrument Hyperion. However, for many development and validation studies, handheld sensors are used. The precision of these sensors is typically measured in spectral resolution, which is the width of each band of the spectrum that is captured. If the scanner detects a large number of fairly narrow frequency bands, it is possible to identify objects even if they are only captured in a handful of pixels. However, spatial resolution is a factor in addition to spectral resolution. If the pixels are too large, then multiple objects are captured in the same pixel and become difficult to identify. If the pixels are too small, then the intensity captured by each sensor cell is low, and the decreased signal-to-noise ratio reduces the reliability of measured features. The acquisition and processing of hyperspectral images is also referred to as imaging spectroscopy or, with reference to the hyperspectral cube, as 3D spectroscopy. Scanning techniques There are four basic techniques for acquiring the three-dimensional (x, y, λ) dataset of a hyperspectral cube. The choice of technique depends on the specific application, seeing that each technique has context-dependent advantages and disadvantages. Spatial scanning In spatial scanning, each two-dimensional (2D) sensor output represents a full slit spectrum (x, λ). Hyperspectral imaging (HSI) devices for spatial scanning obtain slit spectra by projecting a strip of the scene onto a slit and dispersing the slit image with a prism or a grating. These systems have the drawback of having the image analyzed per lines (with a push broom scanner) and also having some mechanical parts integrated into the optical train. With these line-scan cameras, the spatial dimension is collected through platform movement or scanning. This requires stabilized mounts or accurate pointing information to 'reconstruct' the image. Nonetheless, line-scan systems are particularly common in remote sensing, where it is sensible to use mobile platforms. Line-scan systems are also used to scan materials moving by on a conveyor belt. A special case of line scanning is point scanning (with a whisk broom scanner), where a point-like aperture is used instead of a slit, and the sensor is essentially one-dimensional instead of 2D. Spectral scanning In spectral scanning, each 2D sensor output represents a monochromatic (i.e. single wavelength), spatial (x, y)-map of the scene. HSI devices for spectral scanning are typically based on optical band-pass filters (either tunable or fixed). The scene is spectrally scanned by exchanging one filter after another while the platform remains stationary. In such \"staring\", wavelength scanning systems, spectral smearing can occur if there is movement within the scene, invalidating spectral correlation/detection. Nonetheless, there is the advantage of being able to pick and choose spectral bands, and having a direct representation of the two spatial dimensions of the scene. If the imaging system is used on a moving platform, such as an airplane, acquired images at different wavelengths corresponds to different areas of the scene. The spatial features on each of the images may be used to realign the pixels. Non-scanning In non-scanning, a single 2D sensor output contains all spatial (x, y) and spectral (λ) data. HSI devices for non-scanning yield the full datacube at once, without any scanning. Figuratively speaking, a single snapshot represents a perspective projection of the datacube, from which its three-dimensional structure can be reconstructed. The most prominent benefits of these snapshot hyperspectral imaging systems are the snapshot advantage (higher light throughput) and shorter acquisition time. A number of systems have been designed, including computed tomographic imaging spectrometry (CTIS), fiber-reformatting imaging spectrometry (FRIS), integral field spectroscopy with lenslet arrays (IFS-L), multi-aperture integral field spectrometer (Hyperpixel Array), integral field spectroscopy with image slicing mirrors (IFS-S), image-replicating imaging spectrometry (IRIS), filter stack spectral decomposition (FSSD), coded aperture snapshot spectral imaging (CASSI), image mapping spectrometry (IMS), and multispectral Sagnac interferometry (MSI). However, computational effort and manufacturing costs are high. In an effort to reduce the computational demands and potentially the high cost of non-scanning hyperspectral instrumentation, prototype devices based on Multivariate Optical Computing have been demonstrated. These devices have been based on the Multivariate Optical Element spectral calculation engine or the Spatial Light Modulator spectral calculation engine. In these platforms, chemical information is calculated in the optical domain prior to imaging such that the chemical image relies on conventional camera systems with no further computing. As a disadvantage of these systems, no spectral information is ever acquired, i.e. only the chemical information, such that post processing or reanalysis is not possible. Spatiospectral scanning In spatiospectral scanning, each 2D sensor output represents a wavelength-coded (\"rainbow-colored\", λ = λ(y)), spatial (x, y)-map of the scene. A prototype for this technique, introduced in 2014, consists of a camera at some non-zero distance behind a basic slit spectroscope (slit + dispersive element). Advanced spatiospectral scanning systems can be obtained by placing a dispersive element before a spatial scanning system. Scanning can be achieved by moving the whole system relative to the scene, by moving the camera alone, or by moving the slit alone. Spatiospectral scanning unites some advantages of spatial and spectral scanning, thereby alleviating some of their disadvantages. Distinguishing hyperspectral from multispectral imaging Hyperspectral imaging is part of a class of techniques commonly referred to as spectral imaging or spectral analysis. The term \"hyperspectral imaging\" derives from the development of NASA's Airborne Imaging Spectrometer (AIS) and AVIRIS in the mid-1980s. Although NASA prefers the earlier term \"imaging spectroscopy\" over \"hyperspectral imaging,\" use of the latter term has become more prevalent in scientific and non-scientific language. In a peer reviewed letter, experts recommend using the terms \"imaging spectroscopy\" or \"spectral imaging\" and avoiding exaggerated prefixes such as \"hyper-,\" \"super-\" and \"ultra-,\" to prevent misnomers in discussion. Hyperspectral imaging is related to multispectral imaging. The distinction between hyper- and multi-band is sometimes based incorrectly on an arbitrary \"number of bands\" or on the type of measurement. Hyperspectral imaging (HSI) uses continuous and contiguous ranges of wavelengths (e.g. 400 - 1100 nm in steps of 1 nm) whilst multiband imaging (MSI) uses a subset of targeted wavelengths at chosen locations (e.g. 400 - 1100 nm in steps of 20 nm). Multiband imaging deals with several images at discrete and somewhat narrow bands. Being \"discrete and somewhat narrow\" is what distinguishes multispectral imaging in the visible wavelength from color photography. A multispectral sensor may have many bands covering the spectrum from the visible to the longwave infrared. Multispectral images do not produce the \"spectrum\" of an object. Landsat is a prominent practical example of multispectral imaging. Hyperspectral dea", "canonical_url": "https://en.wikipedia.org/wiki/Hyperspectral_imaging", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:44.139821", "metadata": {"word_count": 253, "text_length": 22815}}
{"id": "wiki_spectral_band", "query_word": "spectral band", "title": "Spectral band", "summary": "Spectral bands are regions of a given spectrum, having a specific range of wavelengths or frequencies. Most often, it refers to electromagnetic bands, regions of the electromagnetic spectrum. More generally, spectral bands may also be means in the spectra of other types of signals, e.g., noise spectrum. A frequency band is an interval in the frequency domain, limited by a lower frequency and an upper frequency. For example, it may refer to a radio band, such as wireless communication standards set by the International Telecommunication Union.", "text": "Spectral bands are regions of a given spectrum, having a specific range of wavelengths or frequencies. Most often, it refers to electromagnetic bands, regions of the electromagnetic spectrum. More generally, spectral bands may also be means in the spectra of other types of signals, e.g., noise spectrum. A frequency band is an interval in the frequency domain, limited by a lower frequency and an upper frequency. For example, it may refer to a radio band, such as wireless communication standards set by the International Telecommunication Union. In nuclear physics In nuclear physics, spectral bands refer to the electromagnetic emission of polyatomic systems, including condensed materials, large molecules, etc. Each spectral line corresponds to the difference in two energy levels of an atom. In molecules, these levels can split. When the number of atoms is large, one gets a continuum of energy levels, the so-called spectral bands. They are often labeled in the same way as the monatomic lines. The bands may overlap. In general, the energy spectrum can be given by a density function, describing the number of energy levels of the quantum system for a given interval. Spectral bands have constant density, and when the bands overlap, the corresponding densities are added. Band spectra is the name given to a group of lines that are closely spaced and arranged in a regular sequence that appears to be a band. It is a colored band, separated by dark spaces on the two sides and arranged in a regular sequence. In one band, there are various sharp and wider color lines, that are closer on one side and wider on other. The intensity in each band falls off from definite limits and indistinct on the other side. In complete band spectra, there is a number lines in a band. This spectra is produced when the emitting substance is in the molecular state. Therefore, they are also called molecular spectra. It is emitted by a molecule in vacuum tube, C-arc core with metallic salt. The band spectrum is the combination of many different spectral lines, resulting from molecular vibrational, rotational, and electronic transition. Spectroscopy studies spectral bands for astronomy and other purposes. Other applications Many systems are characterized by the spectral band to which they respond. For example: Musical instruments produce different ranges of notes within the hearing range. The electromagnetic spectrum can be divided into many different ranges such as visible light, infrared or ultraviolet radiation, radio waves, X-rays and so on, and each of these ranges can in turn be divided into smaller ranges. A radio communications signal must occupy a range of frequencies carrying most of its energy, called its bandwidth. A frequency band may represent one communication channel or be subdivided into many. Allocation of radio frequency ranges to different uses is a major function of radio spectrum allocation. See also == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_band", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:45.112594", "metadata": {"word_count": 86, "text_length": 2952}}
{"id": "wiki_spectral_reflectance", "query_word": "spectral reflectance", "title": "Reflectance", "summary": "The reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at the boundary. Reflectance is a component of the response of the electronic structure of the material to the electromagnetic field of light, and is in general a function of the frequency, or wavelength, of the light, its polarization, and the angle of incidence. The dependence of reflectance on the wavelength is called a reflectance spectrum or spectral reflectance curve.", "text": "The reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at the boundary. Reflectance is a component of the response of the electronic structure of the material to the electromagnetic field of light, and is in general a function of the frequency, or wavelength, of the light, its polarization, and the angle of incidence. The dependence of reflectance on the wavelength is called a reflectance spectrum or spectral reflectance curve. Mathematical definitions Hemispherical reflectance The hemispherical reflectance of a surface, denoted R, is defined as R = Φ e r Φ e i , {\\displaystyle R={\\frac {\\Phi _{\\mathrm {e} }^{\\mathrm {r} }}{\\Phi _{\\mathrm {e} }^{\\mathrm {i} }}},} where Φer is the radiant flux reflected by that surface and Φei is the radiant flux received by that surface. Spectral hemispherical reflectance The spectral hemispherical reflectance in frequency and spectral hemispherical reflectance in wavelength of a surface, denoted Rν and Rλ respectively, are defined as R ν = Φ e , ν r Φ e , ν i , {\\displaystyle R_{\\nu }={\\frac {\\Phi _{\\mathrm {e} ,\\nu }^{\\mathrm {r} }}{\\Phi _{\\mathrm {e} ,\\nu }^{\\mathrm {i} }}},} R λ = Φ e , λ r Φ e , λ i , {\\displaystyle R_{\\lambda }={\\frac {\\Phi _{\\mathrm {e} ,\\lambda }^{\\mathrm {r} }}{\\Phi _{\\mathrm {e} ,\\lambda }^{\\mathrm {i} }}},} where Φe,νr is the spectral radiant flux in frequency reflected by that surface; Φe,νi is the spectral radiant flux in frequency received by that surface; Φe,λr is the spectral radiant flux in wavelength reflected by that surface; Φe,λi is the spectral radiant flux in wavelength received by that surface. Directional reflectance The directional reflectance of a surface, denoted RΩ, is defined as R Ω = L e , Ω r L e , Ω i , {\\displaystyle R_{\\Omega }={\\frac {L_{\\mathrm {e} ,\\Omega }^{\\mathrm {r} }}{L_{\\mathrm {e} ,\\Omega }^{\\mathrm {i} }}},} where Le,Ωr is the radiance reflected by that surface; Le,Ωi is the radiance received by that surface. This depends on both the reflected direction and the incoming direction. In other words, it has a value for every combination of incoming and outgoing directions. It is related to the bidirectional reflectance distribution function and its upper limit is 1. Another measure of reflectance, depending only on the outgoing direction, is I/F, where I is the radiance reflected in a given direction and F is the incoming radiance averaged over all directions, in other words, the total flux of radiation hitting the surface per unit area, divided by π. This can be greater than 1 for a glossy surface illuminated by a source such as the sun, with the reflectance measured in the direction of maximum radiance (see also Seeliger effect). Spectral directional reflectance The spectral directional reflectance in frequency and spectral directional reflectance in wavelength of a surface, denoted RΩ,ν and RΩ,λ respectively, are defined as R Ω , ν = L e , Ω , ν r L e , Ω , ν i , {\\displaystyle R_{\\Omega ,\\nu }={\\frac {L_{\\mathrm {e} ,\\Omega ,\\nu }^{\\mathrm {r} }}{L_{\\mathrm {e} ,\\Omega ,\\nu }^{\\mathrm {i} }}},} R Ω , λ = L e , Ω , λ r L e , Ω , λ i , {\\displaystyle R_{\\Omega ,\\lambda }={\\frac {L_{\\mathrm {e} ,\\Omega ,\\lambda }^{\\mathrm {r} }}{L_{\\mathrm {e} ,\\Omega ,\\lambda }^{\\mathrm {i} }}},} where Le,Ω,νr is the spectral radiance in frequency reflected by that surface; Le,Ω,νi is the spectral radiance received by that surface; Le,Ω,λr is the spectral radiance in wavelength reflected by that surface; Le,Ω,λi is the spectral radiance in wavelength received by that surface. Again, one can also define a value of I/F (see above) for a given wavelength. Reflectivity For homogeneous and semi-infinite (see halfspace) materials, reflectivity is the same as reflectance. Reflectivity is the square of the magnitude of the Fresnel reflection coefficient, which is the ratio of the reflected to incident electric field; as such the reflection coefficient can be expressed as a complex number as determined by the Fresnel equations for a single layer, whereas the reflectance is always a positive real number. For layered and finite media, according to the CIE, reflectivity is distinguished from reflectance by the fact that reflectivity is a value that applies to thick reflecting objects. When reflection occurs from thin layers of material, internal reflection effects can cause the reflectance to vary with surface thickness. Reflectivity is the limit value of reflectance as the sample becomes thick; it is the intrinsic reflectance of the surface, hence irrespective of other parameters such as the reflectance of the rear surface. Another way to interpret this is that the reflectance is the fraction of electromagnetic power reflected from a specific sample, while reflectivity is a property of the material itself, which would be measured on a perfect machine if the material filled half of all space. Surface type Given that reflectance is a directional property, most surfaces can be divided into those that give specular reflection and those that give diffuse reflection. For specular surfaces, such as glass or polished metal, reflectance is nearly zero at all angles except at the appropriate reflected angle; that is the same angle with respect to the surface normal in the plane of incidence, but on the opposing side. When the radiation is incident normal to the surface, it is reflected back into the same direction. For diffuse surfaces, such as matte white paint, reflectance is uniform; radiation is reflected in all angles equally or near-equally. Such surfaces are said to be Lambertian. Most practical objects exhibit a combination of diffuse and specular reflective properties. Liquid reflectance Reflection of light occurs at a boundary at which the index of refraction changes. Specular reflection is calculated by the Fresnel equations. Fresnel reflection is directional and therefore does not contribute significantly to albedo which primarily diffuses reflection. A liquid surface may be wavy. Reflectance may be adjusted to account for waviness. Grating efficiency The generalization of reflectance to a diffraction grating, which disperses light by wavelength, is called diffraction efficiency. Other radiometric coefficients See also Bidirectional reflectance distribution function Colorimetry Emissivity Lambert's cosine law Transmittance Sun path Light Reflectance Value Albedo Reststrahlen effect Lyddane–Sachs–Teller relation References External links Reflectivity of metals Archived 2016-03-04 at the Wayback Machine. Reflectance Data.", "canonical_url": "https://en.wikipedia.org/wiki/Reflectance", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:46.068217", "metadata": {"word_count": 86, "text_length": 6622}}
{"id": "wiki_material_identification", "query_word": "material identification", "title": "Positive material identification", "summary": "Positive material identification (PMI) is the analysis of a material, this can be any material but is generally used for the analysis of metallic alloy to establish composition by reading the quantities by percentage of its constituent elements. Typical methods for PMI include X-ray fluorescence (XRF) and optical emission spectrometry (OES). PMI is a portable method of analysis and can be used in the field on components. X-ray fluorescence (XRF) PMI cannot detect small elements such as carbon. This means that when undertaking analysis of stainless steels such as grades 304 and 316 the low carbon 'L' variant can not be determined. This however can be analysed with optical emission spectrometry (OES) == References ==", "text": "Positive material identification (PMI) is the analysis of a material, this can be any material but is generally used for the analysis of metallic alloy to establish composition by reading the quantities by percentage of its constituent elements. Typical methods for PMI include X-ray fluorescence (XRF) and optical emission spectrometry (OES). PMI is a portable method of analysis and can be used in the field on components. X-ray fluorescence (XRF) PMI cannot detect small elements such as carbon. This means that when undertaking analysis of stainless steels such as grades 304 and 316 the low carbon 'L' variant can not be determined. This however can be analysed with optical emission spectrometry (OES) == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Positive_material_identification", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:46.667167", "metadata": {"word_count": 115, "text_length": 724}}
{"id": "wiki_spectral_signature", "query_word": "spectral signature", "title": "Spectral signature", "summary": "Spectral signature is the variation of reflectance or emittance of a material with respect to wavelengths (i.e., reflectance/emittance as a function of wavelength). The spectral signature of stars indicates the composition of the stellar atmosphere. The spectral signature of an object is a function of the incidental EM wavelength and material interaction with that section of the electromagnetic spectrum. The measurements can be made with various instruments, including a task specific spectrometer, although the most common method is separation of the red, green, blue and near infrared portion of the EM spectrum as acquired by digital cameras. Calibrating spectral signatures under specific illumination are collected in order to apply a correction to airborne or satellite imagery digital images. The user of one kind of spectroscope looks through it at a tube of ionized gas. The user sees specific lines of colour falling on a graduated scale. Each substance will have its own unique pattern of spectral lines. Most remote sensing applications process digital images to extract spectral signatures at each pixel and use them to divide the image in groups of similar pixels (segmentation) using different approaches. As a last step, they assign a class to each group (classification) by comparing with known spectral signatures. Depending on pixel resolution, a pixel can represent many spectral signature \"mixed\" together - that is why much remote sensing analysis is done to \"unmix mixtures\". Ultimately correct matching of spectral signature recorded by image pixel with spectral signature of existing elements leads to accurate classification in remote sensing.", "text": "Spectral signature is the variation of reflectance or emittance of a material with respect to wavelengths (i.e., reflectance/emittance as a function of wavelength). The spectral signature of stars indicates the composition of the stellar atmosphere. The spectral signature of an object is a function of the incidental EM wavelength and material interaction with that section of the electromagnetic spectrum. The measurements can be made with various instruments, including a task specific spectrometer, although the most common method is separation of the red, green, blue and near infrared portion of the EM spectrum as acquired by digital cameras. Calibrating spectral signatures under specific illumination are collected in order to apply a correction to airborne or satellite imagery digital images. The user of one kind of spectroscope looks through it at a tube of ionized gas. The user sees specific lines of colour falling on a graduated scale. Each substance will have its own unique pattern of spectral lines. Most remote sensing applications process digital images to extract spectral signatures at each pixel and use them to divide the image in groups of similar pixels (segmentation) using different approaches. As a last step, they assign a class to each group (classification) by comparing with known spectral signatures. Depending on pixel resolution, a pixel can represent many spectral signature \"mixed\" together - that is why much remote sensing analysis is done to \"unmix mixtures\". Ultimately correct matching of spectral signature recorded by image pixel with spectral signature of existing elements leads to accurate classification in remote sensing. See also Spectroscopy Spectral imaging Hyperspectral imaging Multispectral image == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_signature", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:47.310474", "metadata": {"word_count": 254, "text_length": 1771}}
{"id": "wiki_spectral_resolution", "query_word": "spectral resolution", "title": "Spectral resolution", "summary": "The spectral resolution of a spectrograph, or, more generally, of a frequency spectrum, is a measure of its ability to resolve features in the electromagnetic spectrum. It is usually denoted by Δ λ {\\displaystyle \\Delta \\lambda } , and is closely related to the resolving power of the spectrograph, defined as R = λ Δ λ , {\\displaystyle R={\\frac {\\lambda }{\\Delta \\lambda }},} where Δ λ {\\displaystyle \\Delta \\lambda } is the smallest difference in wavelengths that can be distinguished at a wavelength of λ {\\displaystyle \\lambda } . For example, the Space Telescope Imaging Spectrograph (STIS) can distinguish features 0.17 nm apart at a wavelength of 1000 nm, giving it a resolution of 0.17 nm and a resolving power of about 5,900. An example of a high resolution spectrograph is the Cryogenic High-Resolution IR Echelle Spectrograph (CRIRES+) installed at ESO's Very Large Telescope, which has a spectral resolving power of up to 100,000.", "text": "The spectral resolution of a spectrograph, or, more generally, of a frequency spectrum, is a measure of its ability to resolve features in the electromagnetic spectrum. It is usually denoted by Δ λ {\\displaystyle \\Delta \\lambda } , and is closely related to the resolving power of the spectrograph, defined as R = λ Δ λ , {\\displaystyle R={\\frac {\\lambda }{\\Delta \\lambda }},} where Δ λ {\\displaystyle \\Delta \\lambda } is the smallest difference in wavelengths that can be distinguished at a wavelength of λ {\\displaystyle \\lambda } . For example, the Space Telescope Imaging Spectrograph (STIS) can distinguish features 0.17 nm apart at a wavelength of 1000 nm, giving it a resolution of 0.17 nm and a resolving power of about 5,900. An example of a high resolution spectrograph is the Cryogenic High-Resolution IR Echelle Spectrograph (CRIRES+) installed at ESO's Very Large Telescope, which has a spectral resolving power of up to 100,000. Doppler effect The spectral resolution can also be expressed in terms of physical quantities, such as velocity; then it describes the difference between velocities Δ v {\\displaystyle \\Delta v} that can be distinguished through the Doppler effect. Then, the resolution is Δ v {\\displaystyle \\Delta v} and the resolving power is R = c Δ v , {\\displaystyle R={\\frac {c}{\\Delta v}},} where c {\\displaystyle c} is the speed of light. The STIS example above then has a spectral resolution of 51. IUPAC definition IUPAC defines resolution in optical spectroscopy as the minimum wavenumber, wavelength or frequency difference between two lines in a spectrum that can be distinguished. Resolving power, R, is given by the transition wavenumber, wavelength or frequency, divided by the resolution. See also Angular resolution Resolution (mass spectrometry) References Further reading Kim Quijano, J., et al. (2003), STIS Instrument Handbook, Version 7.0, (Baltimore: STScI) Frank L. Pedrotti, S.J. (2007), Introduction to optics, 3rd version, (San Francisco)", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_resolution", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:48.254481", "metadata": {"word_count": 154, "text_length": 1991}}
{"id": "wiki_hyperspectral_camera", "query_word": "hyperspectral camera", "title": "Hyperspectral imaging", "summary": "Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers, which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique \"fingerprints\" in the electromagnetic spectrum. Known as spectral signatures, these \"fingerprints\" enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields.", "text": "Hyperspectral imaging collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers, which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique \"fingerprints\" in the electromagnetic spectrum. Known as spectral signatures, these \"fingerprints\" enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields. Sensors Figuratively speaking, hyperspectral sensors collect information as a set of \"images.\" Each image represents a narrow wavelength range of the electromagnetic spectrum, also known as a spectral band. These \"images\" are combined to form a three-dimensional (x, y, λ) hyperspectral data cube for processing and analysis, where x and y represent two spatial dimensions of the scene, and λ represents the spectral dimension (comprising a range of wavelengths). Technically speaking, there are four ways for sensors to sample the hyperspectral cube: spatial scanning, spectral scanning, snapshot imaging, and spatio-spectral scanning. Hyperspectral cubes are generated from airborne sensors like NASA's Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), or from satellites like NASA's EO-1 with its hyperspectral instrument Hyperion. However, for many development and validation studies, handheld sensors are used. The precision of these sensors is typically measured in spectral resolution, which is the width of each band of the spectrum that is captured. If the scanner detects a large number of fairly narrow frequency bands, it is possible to identify objects even if they are only captured in a handful of pixels. However, spatial resolution is a factor in addition to spectral resolution. If the pixels are too large, then multiple objects are captured in the same pixel and become difficult to identify. If the pixels are too small, then the intensity captured by each sensor cell is low, and the decreased signal-to-noise ratio reduces the reliability of measured features. The acquisition and processing of hyperspectral images is also referred to as imaging spectroscopy or, with reference to the hyperspectral cube, as 3D spectroscopy. Scanning techniques There are four basic techniques for acquiring the three-dimensional (x, y, λ) dataset of a hyperspectral cube. The choice of technique depends on the specific application, seeing that each technique has context-dependent advantages and disadvantages. Spatial scanning In spatial scanning, each two-dimensional (2D) sensor output represents a full slit spectrum (x, λ). Hyperspectral imaging (HSI) devices for spatial scanning obtain slit spectra by projecting a strip of the scene onto a slit and dispersing the slit image with a prism or a grating. These systems have the drawback of having the image analyzed per lines (with a push broom scanner) and also having some mechanical parts integrated into the optical train. With these line-scan cameras, the spatial dimension is collected through platform movement or scanning. This requires stabilized mounts or accurate pointing information to 'reconstruct' the image. Nonetheless, line-scan systems are particularly common in remote sensing, where it is sensible to use mobile platforms. Line-scan systems are also used to scan materials moving by on a conveyor belt. A special case of line scanning is point scanning (with a whisk broom scanner), where a point-like aperture is used instead of a slit, and the sensor is essentially one-dimensional instead of 2D. Spectral scanning In spectral scanning, each 2D sensor output represents a monochromatic (i.e. single wavelength), spatial (x, y)-map of the scene. HSI devices for spectral scanning are typically based on optical band-pass filters (either tunable or fixed). The scene is spectrally scanned by exchanging one filter after another while the platform remains stationary. In such \"staring\", wavelength scanning systems, spectral smearing can occur if there is movement within the scene, invalidating spectral correlation/detection. Nonetheless, there is the advantage of being able to pick and choose spectral bands, and having a direct representation of the two spatial dimensions of the scene. If the imaging system is used on a moving platform, such as an airplane, acquired images at different wavelengths corresponds to different areas of the scene. The spatial features on each of the images may be used to realign the pixels. Non-scanning In non-scanning, a single 2D sensor output contains all spatial (x, y) and spectral (λ) data. HSI devices for non-scanning yield the full datacube at once, without any scanning. Figuratively speaking, a single snapshot represents a perspective projection of the datacube, from which its three-dimensional structure can be reconstructed. The most prominent benefits of these snapshot hyperspectral imaging systems are the snapshot advantage (higher light throughput) and shorter acquisition time. A number of systems have been designed, including computed tomographic imaging spectrometry (CTIS), fiber-reformatting imaging spectrometry (FRIS), integral field spectroscopy with lenslet arrays (IFS-L), multi-aperture integral field spectrometer (Hyperpixel Array), integral field spectroscopy with image slicing mirrors (IFS-S), image-replicating imaging spectrometry (IRIS), filter stack spectral decomposition (FSSD), coded aperture snapshot spectral imaging (CASSI), image mapping spectrometry (IMS), and multispectral Sagnac interferometry (MSI). However, computational effort and manufacturing costs are high. In an effort to reduce the computational demands and potentially the high cost of non-scanning hyperspectral instrumentation, prototype devices based on Multivariate Optical Computing have been demonstrated. These devices have been based on the Multivariate Optical Element spectral calculation engine or the Spatial Light Modulator spectral calculation engine. In these platforms, chemical information is calculated in the optical domain prior to imaging such that the chemical image relies on conventional camera systems with no further computing. As a disadvantage of these systems, no spectral information is ever acquired, i.e. only the chemical information, such that post processing or reanalysis is not possible. Spatiospectral scanning In spatiospectral scanning, each 2D sensor output represents a wavelength-coded (\"rainbow-colored\", λ = λ(y)), spatial (x, y)-map of the scene. A prototype for this technique, introduced in 2014, consists of a camera at some non-zero distance behind a basic slit spectroscope (slit + dispersive element). Advanced spatiospectral scanning systems can be obtained by placing a dispersive element before a spatial scanning system. Scanning can be achieved by moving the whole system relative to the scene, by moving the camera alone, or by moving the slit alone. Spatiospectral scanning unites some advantages of spatial and spectral scanning, thereby alleviating some of their disadvantages. Distinguishing hyperspectral from multispectral imaging Hyperspectral imaging is part of a class of techniques commonly referred to as spectral imaging or spectral analysis. The term \"hyperspectral imaging\" derives from the development of NASA's Airborne Imaging Spectrometer (AIS) and AVIRIS in the mid-1980s. Although NASA prefers the earlier term \"imaging spectroscopy\" over \"hyperspectral imaging,\" use of the latter term has become more prevalent in scientific and non-scientific language. In a peer reviewed letter, experts recommend using the terms \"imaging spectroscopy\" or \"spectral imaging\" and avoiding exaggerated prefixes such as \"hyper-,\" \"super-\" and \"ultra-,\" to prevent misnomers in discussion. Hyperspectral imaging is related to multispectral imaging. The distinction between hyper- and multi-band is sometimes based incorrectly on an arbitrary \"number of bands\" or on the type of measurement. Hyperspectral imaging (HSI) uses continuous and contiguous ranges of wavelengths (e.g. 400 - 1100 nm in steps of 1 nm) whilst multiband imaging (MSI) uses a subset of targeted wavelengths at chosen locations (e.g. 400 - 1100 nm in steps of 20 nm). Multiband imaging deals with several images at discrete and somewhat narrow bands. Being \"discrete and somewhat narrow\" is what distinguishes multispectral imaging in the visible wavelength from color photography. A multispectral sensor may have many bands covering the spectrum from the visible to the longwave infrared. Multispectral images do not produce the \"spectrum\" of an object. Landsat is a prominent practical example of multispectral imaging. Hyperspectral dea", "canonical_url": "https://en.wikipedia.org/wiki/Hyperspectral_imaging", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:51.317528", "metadata": {"word_count": 253, "text_length": 22815}}
{"id": "wiki_spectral_analysis", "query_word": "spectral analysis", "title": "Spectral analysis", "summary": "Spectral analysis or spectrum analysis is analysis in terms of a spectrum of frequencies or related quantities such as energies, eigenvalues, etc. In specific areas it may refer to: Spectroscopy in chemistry and physics, a method of analyzing the properties of matter from their electromagnetic interactions Spectral estimation, in statistics and signal processing, an algorithm that estimates the strength of different frequency components (the power spectrum) of a time-domain signal. This may also be called frequency domain analysis Spectrum analyzer, a hardware device that measures the magnitude of an input signal versus frequency within the full frequency range of the instrument Spectral theory, in mathematics, a theory that extends eigenvalues and eigenvectors to linear operators on Hilbert space, and more generally to the elements of a Banach algebra In nuclear and particle physics, gamma spectroscopy, and high-energy astronomy, the analysis of the output of a pulse height analyzer for characteristic features such as spectral line, edges, and various physical processes producing continuum shapes", "text": "Spectral analysis or spectrum analysis is analysis in terms of a spectrum of frequencies or related quantities such as energies, eigenvalues, etc. In specific areas it may refer to: Spectroscopy in chemistry and physics, a method of analyzing the properties of matter from their electromagnetic interactions Spectral estimation, in statistics and signal processing, an algorithm that estimates the strength of different frequency components (the power spectrum) of a time-domain signal. This may also be called frequency domain analysis Spectrum analyzer, a hardware device that measures the magnitude of an input signal versus frequency within the full frequency range of the instrument Spectral theory, in mathematics, a theory that extends eigenvalues and eigenvectors to linear operators on Hilbert space, and more generally to the elements of a Banach algebra In nuclear and particle physics, gamma spectroscopy, and high-energy astronomy, the analysis of the output of a pulse height analyzer for characteristic features such as spectral line, edges, and various physical processes producing continuum shapes See also Multispectral analysis Harmonic analysis", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_analysis", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:52.341854", "metadata": {"word_count": 163, "text_length": 1164}}
{"id": "wiki_absorption_band", "query_word": "absorption band", "title": "Absorption band", "summary": "In spectroscopy, an absorption band is a range of wavelengths, frequencies or energies in the electromagnetic spectrum that are characteristic of a particular transition from initial to final state in a substance. According to quantum mechanics, atoms and molecules can only hold certain defined quantities of energy, or exist in specific states. When such quanta of electromagnetic radiation are emitted or absorbed by an atom or molecule, energy of the radiation changes the state of the atom or molecule from an initial state to a final state.", "text": "In spectroscopy, an absorption band is a range of wavelengths, frequencies or energies in the electromagnetic spectrum that are characteristic of a particular transition from initial to final state in a substance. According to quantum mechanics, atoms and molecules can only hold certain defined quantities of energy, or exist in specific states. When such quanta of electromagnetic radiation are emitted or absorbed by an atom or molecule, energy of the radiation changes the state of the atom or molecule from an initial state to a final state. Overview When electromagnetic radiation is absorbed by an atom or molecule, the energy of the radiation changes the state of the atom or molecule from an initial state to a final state. The number of states in a specific energy range is discrete for gaseous or diluted systems, with discrete energy levels. Condensed systems, like liquids or solids, have a continuous density of states distribution and often possess continuous energy bands. In order for a substance to change its energy it must do so in a series of \"steps\" by the absorption of a photon. This absorption process can move a particle, like an electron, from an occupied state to an empty or unoccupied state. It can also move a whole vibrating or rotating system, like a molecule, from one vibrational or rotational state to another or it can create a quasiparticle like a phonon or a plasmon in a solid. Electromagnetic transitions When a photon is absorbed, the electromagnetic field of the photon disappears as it initiates a change in the state of the system that absorbs the photon. Energy, momentum, angular momentum, magnetic dipole moment and electric dipole moment are transported from the photon to the system. Because there are conservation laws, that have to be satisfied, the transition has to meet a series of constraints. This results in a series of selection rules. It is not possible to make any transition that lies within the energy or frequency range that is observed. The strength of an electromagnetic absorption process is mainly determined by two factors. First, transitions that only change the magnetic dipole moment of the system are much weaker than transitions that change the electric dipole moment and that transitions to higher order moments, like quadrupole transitions, are weaker than dipole transitions. Second, not all transitions have the same transition matrix element, absorption coefficient or oscillator strength. For some types of bands or spectroscopic disciplines temperature and statistical mechanics plays an important role. For (far) infrared, microwave and radio frequency ranges the temperature dependent occupation numbers of states and the difference between Bose-Einstein statistics and Fermi-Dirac statistics determines the intensity of observed absorptions. For other energy ranges thermal motion effects, like Doppler broadening may determine the linewidth. Band and line shape A wide variety of absorption band and line shapes exist, and the analysis of the band or line shape can be used to determine information about the system that causes it. In many cases it is convenient to assume that a narrow spectral line is a Lorentzian or Gaussian, depending respectively on the decay mechanism or temperature effects like Doppler broadening. Analysis of the spectral density and the intensities, width and shape of spectral lines sometimes can yield a lot of information about the observed system like it is done with Mössbauer spectra. In systems with a very large number of states like macromolecules and large conjugated systems the separate energy levels can't always be distinguished in an absorption spectrum. If the line broadening mechanism is known and the shape of then spectral density is clearly visible in the spectrum, it is possible to get the desired data. Sometimes it is enough to know the lower or upper limits of the band or its position for an analysis. For condensed matter and solids the shape of absorption bands are often determined by transitions between states in their continuous density of states distributions. For crystals, the electronic band structure determines the density of states. In fluids, glasses and amorphous solids, there is no long range correlation and the dispersion relations are isotropic. For charge-transfer complexes and conjugated systems, the band width is complicated by a variety of factors, compared to condensed matter. Types Electronic transitions Electromagnetic transitions in atoms, molecules and condensed matter mainly take place at energies corresponding to the UV and visible part of the spectrum. Core electrons in atoms, and many other phenomena, are observed with different brands of XAS in the X-ray energy range. Electromagnetic transitions in atomic nuclei, as observed in Mössbauer spectroscopy, take place in the gamma ray part of the spectrum. The main factors that cause broadening of the spectral line into an absorption band of a molecular solid are the distributions of vibrational and rotational energies of the molecules in the sample (and also those of their excited states). In solid crystals the shape of absorption bands are determined by the density of states of initial and final states of electronic states or lattice vibrations, called phonons, in the crystal structure. In gas phase spectroscopy, the fine structure afforded by these factors can be discerned, but in solution-state spectroscopy, the differences in molecular micro environments further broaden the structure to give smooth bands. Electronic transition bands of molecules may be from tens to several hundred nanometers in breadth. Vibrational transitions Vibrational transitions and optical phonon transitions take place in the infrared part of the spectrum, at wavelengths of around 1-30 micrometres. Rotational transitions Rotational transitions take place in the far infrared and microwave regions. Other transitions Absorption bands in the radio frequency range are found in NMR spectroscopy. The frequency ranges and intensities are determined by the magnetic moment of the nuclei that are observed, the applied magnetic field and temperature occupation number differences of the magnetic states. Applications Materials with broad absorption bands are being applied in pigments, dyes and optical filters. Titanium dioxide, zinc oxide and chromophores are applied as UV absorbers and reflectors in sunscreen. Absorption bands of interest to the atmospheric physicist In oxygen: the Hopfield bands, very strong, between about 67 and 100 nanometres in the ultraviolet (named after John J. Hopfield); a diffuse system between 101.9 and 130 nanometres; the Schumann–Runge continuum, very strong, between 135 and 176 nanometres; the Schumann–Runge bands between 176 and 192.6 nanometres (named for Victor Schumann and Carl Runge); the Herzberg bands between 240 and 260 nanometres (named after Gerhard Herzberg); the atmospheric bands between 538 and 771 nanometres in the visible spectrum; including the oxygen δ (~580 nm), γ (~629 nm), B (~688 nm), and A-band (~759-771 nm) a system in the infrared at about 1000 nanometres, In ozone: the Hartley bands between 200 and 300 nanometres in the ultraviolet, with a very intense maximum absorption at 255 nanometres (named after Walter Noel Hartley); the Huggins bands, weak absorption between 320 and 360 nanometres (named after Sir William Huggins); the Chappuis bands (sometimes misspelled \"Chappius\"), a weak diffuse system between 375 and 650 nanometres in the visible spectrum (named after J. Chappuis); and the Wulf bands in the infrared beyond 700 nm, centered at 4,700, 9,600 and 14,100 nanometres, the latter being the most intense (named after Oliver R. Wulf). In nitrogen: The Lyman–Birge–Hopfield bands, sometimes known as the Birge–Hopfield bands, in the far ultraviolet: 140– 170 nm (named after Theodore Lyman, Raymond T. Birge, and John J. Hopfield) Absorption bands of interest to the radio technician Radio waves that travel through the atmosphere are affected by absorption bands. The International Telecommunication Union has a very detailed documentation for absorption behavior encountered in the radio bands and spectroscopic data for oxygen and water vapor. In water vapor: Resonance peak at 22.24 GHz, 1.35 cm, for which there is a K band (IEEE) in radio 183 GHz In oxygen: a peak around 5 millimeters (60 GHz), which has consequences for radio communication in the V band 118 GHz peak See also Franck–Condon principle Spectroscopy Spectral line == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Absorption_band", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:52.981796", "metadata": {"word_count": 87, "text_length": 8556}}
{"id": "wiki_spectral_dimension", "query_word": "spectral dimension", "title": "Spectral dimension", "summary": "The spectral dimension is a real-valued quantity that characterizes a spacetime geometry and topology. It characterizes a spread into space over time, e.g. an ink drop diffusing in a water glass or the evolution of a pandemic in a population. Its definition is as follow: if a phenomenon spreads as t n {\\displaystyle t^{n}} , with t {\\displaystyle t} the time, then the spectral dimension is 2 n {\\displaystyle 2n} . The spectral dimension depends on the topology of the space, e.g., the distribution of neighbors in a population, and the diffusion rate. In physics, the concept of spectral dimension is used, among other things, in quantum gravity, percolation theory, superstring theory, or quantum field theory.", "text": "The spectral dimension is a real-valued quantity that characterizes a spacetime geometry and topology. It characterizes a spread into space over time, e.g. an ink drop diffusing in a water glass or the evolution of a pandemic in a population. Its definition is as follow: if a phenomenon spreads as t n {\\displaystyle t^{n}} , with t {\\displaystyle t} the time, then the spectral dimension is 2 n {\\displaystyle 2n} . The spectral dimension depends on the topology of the space, e.g., the distribution of neighbors in a population, and the diffusion rate. In physics, the concept of spectral dimension is used, among other things, in quantum gravity, percolation theory, superstring theory, or quantum field theory. Examples The diffusion of ink in an isotropic homogeneous medium like still water evolves as t 3 / 2 {\\displaystyle t^{3/2}} , giving a spectral dimension of 3. Ink in a 2D Sierpiński triangle diffuses following a more complicated path and thus more slowly, as t 0.6826 {\\displaystyle t^{0.6826}} , giving a spectral dimension of 1.3652. See also Dimension Fractal dimension Hausdorff dimension == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_dimension", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:54.258743", "metadata": {"word_count": 116, "text_length": 1127}}
{"id": "wiki_spatial_resolution", "query_word": "spatial resolution", "title": "Spatial resolution", "summary": "In physics and geosciences, the term spatial resolution refers to distance between independent measurements, or the physical dimension that represents a pixel of the image. While in some instruments, like cameras and telescopes, spatial resolution is directly connected to angular resolution, other instruments, like synthetic aperture radar or a network of weather stations, produce data whose spatial sampling layout is more related to the Earth's surface, such as in remote sensing and satellite imagery.", "text": "In physics and geosciences, the term spatial resolution refers to distance between independent measurements, or the physical dimension that represents a pixel of the image. While in some instruments, like cameras and telescopes, spatial resolution is directly connected to angular resolution, other instruments, like synthetic aperture radar or a network of weather stations, produce data whose spatial sampling layout is more related to the Earth's surface, such as in remote sensing and satellite imagery. See also Image resolution Ground sample distance Level of detail Resel == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Spatial_resolution", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:54.873183", "metadata": {"word_count": 74, "text_length": 595}}
{"id": "wiki_noise_reduction", "query_word": "noise reduction", "title": "Noise reduction", "summary": "Noise reduction is the process of removing noise from a signal. Noise reduction techniques exist for audio and images. Noise reduction algorithms may distort the signal to some degree. Noise rejection is the ability of a circuit to isolate an undesired signal component from the desired signal component, as with common-mode rejection ratio. All signal processing devices, both analog and digital, have traits that make them susceptible to noise. Noise can be random with an even frequency distribution (white noise), or frequency-dependent noise introduced by a device's mechanism or signal processing algorithms. In electronic systems, a major type of noise is hiss created by random electron motion due to thermal agitation. These agitated electrons rapidly add and subtract from the output signal and thus create detectable noise. In the case of photographic film and magnetic tape, noise (both visible and audible) is introduced due to the grain structure of the medium. In photographic film, the size of the grains in the film determines the film's sensitivity, more sensitive film having larger-sized grains. In magnetic tape, the larger the grains of the magnetic particles (usually ferric oxide or magnetite), the more prone the medium is to noise. To compensate for this, larger areas of film or magnetic tape may be used to lower the noise to an acceptable level.", "text": "Noise reduction is the process of removing noise from a signal. Noise reduction techniques exist for audio and images. Noise reduction algorithms may distort the signal to some degree. Noise rejection is the ability of a circuit to isolate an undesired signal component from the desired signal component, as with common-mode rejection ratio. All signal processing devices, both analog and digital, have traits that make them susceptible to noise. Noise can be random with an even frequency distribution (white noise), or frequency-dependent noise introduced by a device's mechanism or signal processing algorithms. In electronic systems, a major type of noise is hiss created by random electron motion due to thermal agitation. These agitated electrons rapidly add and subtract from the output signal and thus create detectable noise. In the case of photographic film and magnetic tape, noise (both visible and audible) is introduced due to the grain structure of the medium. In photographic film, the size of the grains in the film determines the film's sensitivity, more sensitive film having larger-sized grains. In magnetic tape, the larger the grains of the magnetic particles (usually ferric oxide or magnetite), the more prone the medium is to noise. To compensate for this, larger areas of film or magnetic tape may be used to lower the noise to an acceptable level. In general Noise reduction algorithms tend to alter signals to a greater or lesser degree. The local signal-and-noise orthogonalization algorithm can be used to avoid changes to the signals. In seismic exploration Boosting signals in seismic data is especially crucial for seismic imaging, inversion, and interpretation, thereby greatly improving the success rate in oil & gas exploration. The useful signal that is smeared in the ambient random noise is often neglected and thus may cause fake discontinuity of seismic events and artifacts in the final migrated image. Enhancing the useful signal while preserving edge properties of the seismic profiles by attenuating random noise can help reduce interpretation difficulties and misleading risks for oil and gas detection. In audio Tape hiss is a performance-limiting issue in analog tape recording. This is related to the particle size and texture used in the magnetic emulsion that is sprayed on the recording media, and also to the relative tape velocity across the tape heads. Four types of noise reduction exist: single-ended pre-recording, single-ended hiss reduction, single-ended surface noise reduction, and codec or dual-ended systems. Single-ended pre-recording systems (such as Dolby HX Pro), work to affect the recording medium at the time of recording. Single-ended hiss reduction systems (such as DNL or DNR) work to reduce noise as it occurs, including both before and after the recording process as well as for live broadcast applications. Single-ended surface noise reduction (such as CEDAR and the earlier SAE 5000A, Burwen TNE 7000, and Packburn 101/323/323A/323AA and 325) is applied to the playback of phonograph records to address scratches, pops, and surface non-linearities. Single-ended dynamic range expanders like the Phase Linear Autocorrelator Noise Reduction and Dynamic Range Recovery System (Models 1000 and 4000) can reduce various noise from old recordings. Dual-ended systems (such as Dolby noise-reduction system or dbx) have a pre-emphasis process applied during recording and then a de-emphasis process applied during playback. Modern digital sound recordings no longer need to worry about tape hiss so analog-style noise reduction systems are not necessary. However, an interesting twist is that dither systems actually add noise to a signal to improve its quality. Compander-based noise reduction systems Dual-ended compander noise reduction systems have a pre-emphasis process applied during recording and then a de-emphasis process applied at playback. Systems include the professional systems Dolby A and Dolby SR by Dolby Laboratories, dbx Professional and dbx Type I by dbx, Donald Aldous' EMT NoiseBX, Burwen Noise Eliminator, Telefunken's telcom c4 and MXR Innovations' MXR as well as the consumer systems Dolby NR, Dolby B, Dolby C and Dolby S, dbx Type II, Telefunken's High Com and Nakamichi's High-Com II, Toshiba's (Aurex AD-4) adres, JVC's ANRS and Super ANRS, Fisher/Sanyo's Super D, SNRS, and the Hungarian/East-German Ex-Ko system. In some compander systems, the compression is applied during professional media production and only the expansion is applied by the listener; for example, systems like dbx disc, High-Com II, CX 20 and UC used for vinyl recordings and Dolby FM, High Com FM and FMX used in FM radio broadcasting. The first widely used audio noise reduction technique was developed by Ray Dolby in 1966. Intended for professional use, Dolby Type A was an encode/decode system in which the amplitude of frequencies in four bands was increased during recording (encoding), then decreased proportionately during playback (decoding). In particular, when recording quiet parts of an audio signal, the frequencies above 1 kHz would be boosted. This had the effect of increasing the signal-to-noise ratio on tape up to 10 dB depending on the initial signal volume. When it was played back, the decoder reversed the process, in effect reducing the noise level by up to 10 dB. The Dolby B system (developed in conjunction with Henry Kloss) was a single-band system designed for consumer products. The Dolby B system, while not as effective as Dolby A, had the advantage of remaining listenable on playback systems without a decoder. The Telefunken High Com integrated circuit U401BR could be utilized to work as a mostly Dolby B–compatible compander as well. In various late-generation High Com tape decks the Dolby-B emulating D NR Expander functionality worked not only for playback, but, as an undocumented feature, also during recording. dbx was a competing analog noise reduction system developed by David E. Blackmer, founder of Dbx, Inc. It used a root-mean-squared (RMS) encode/decode algorithm with the noise-prone high frequencies boosted, and the entire signal fed through a 2:1 compander. dbx operated across the entire audible bandwidth and unlike Dolby B was unusable without a decoder. However, it could achieve up to 30 dB of noise reduction. Since analog video recordings use frequency modulation for the luminance part (composite video signal in direct color systems), which keeps the tape at saturation level, audio-style noise reduction is unnecessary. Dynamic noise limiter and dynamic noise reduction Dynamic noise limiter (DNL) is an audio noise reduction system originally introduced by Philips in 1971 for use on cassette decks. Its circuitry is also based on a single chip. It was further developed into dynamic noise reduction (DNR) by National Semiconductor to reduce noise levels on long-distance telephony. First sold in 1981, DNR is frequently confused with the far more common Dolby noise-reduction system. Unlike Dolby and dbx Type I and Type II noise reduction systems, DNL and DNR are playback-only signal processing systems that do not require the source material to first be encoded. They can be used to remove background noise from any audio signal, including magnetic tape recordings and FM radio broadcasts, reducing noise by as much as 10 dB. They can also be used in conjunction with other noise reduction systems, provided that they are used prior to applying DNR to prevent DNR from causing the other noise reduction system to mistrack. One of DNR's first widespread applications was in the GM Delco car stereo systems in US GM cars introduced in 1984. It was also used in factory car stereos in Jeep vehicles in the 1980s, such as the Cherokee XJ. Today, DNR, DNL, and similar systems are most commonly encountered as a noise reduction system in microphone systems. Other approaches A second class of algorithms work in the time-frequency domain using some linear or nonlinear filters that have local characteristics and are often called time-frequency filters. Noise can therefore be also removed by use of spectral editing tools, which work in this time-frequency domain, allowing local modifications without affecting nearby signal energy. This can be done manually much like in a paint program drawing pictures. Another way is to define a dynamic threshold for filtering noise, that is derived from the local signal, again with respect to a local time-frequency region. Everything below the threshold will be filtered, everything above the threshold, like partials of a voice or wanted noise, will be untouched. The region is typically defined by the location of the signal's instantaneous frequency, as most of the signal energy to be preserved is concentrated about it. Yet another approach is the automatic noise limiter and noise blanker commonly found on HAM radio transceivers, CB radio transceivers, etc. Both of the aforementioned filters can be used separately, or in conjunction with each other at the same time, depending on the transceiver itself. Software programs Most digital audio workstations (DAWs) and audio editing software have one or more noise reduction functions. In images Images taken with digital cameras or conventional film cameras will pick up noise from a variety of sources. Further use of these images will often require that the noise be reduced either for aesthetic purposes or for practical purposes such as computer vision. Types In salt and pepper noise (sparse light and dark disturbances), also known as impulse noise, pixels in the image are very different in color or intensity from their surrounding pixels; the defining characteristic is that the value of a noisy pixel bears no relation to the color of surrounding pixels. When viewed, the image contains dark and white dots, hence the term salt and pepper noise. Generally, this type of noise will only affect a small number of image pixels. Typical sources include f", "canonical_url": "https://en.wikipedia.org/wiki/Noise_reduction", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:56.035970", "metadata": {"word_count": 218, "text_length": 18080}}
{"id": "wiki_atmospheric_correction", "query_word": "atmospheric correction", "title": "Atmospheric correction", "summary": "Atmospheric correction is the process of adjusting images taken by satellite or airborne sensors to remove distortions caused by the atmosphere. These distortions—mainly due to the scattering and absorption of sunlight by particles and gases—can affect how accurately the sensor captures the true reflectance (or brightness) of the Earth's surface. In remote sensing, atmospheric effects can significantly alter the spectral characteristics of the radiation detected by sensors. This occurs because light must pass through the atmosphere twice—first as sunlight traveling to the Earth's surface, and again as reflected light returning to the sensor—undergoing both absorption and scattering along the way. These distortions can affect the accuracy of surface reflectance measurements and are typically corrected through a range of physical and statistical methods.", "text": "Atmospheric correction is the process of adjusting images taken by satellite or airborne sensors to remove distortions caused by the atmosphere. These distortions—mainly due to the scattering and absorption of sunlight by particles and gases—can affect how accurately the sensor captures the true reflectance (or brightness) of the Earth's surface. In remote sensing, atmospheric effects can significantly alter the spectral characteristics of the radiation detected by sensors. This occurs because light must pass through the atmosphere twice—first as sunlight traveling to the Earth's surface, and again as reflected light returning to the sensor—undergoing both absorption and scattering along the way. These distortions can affect the accuracy of surface reflectance measurements and are typically corrected through a range of physical and statistical methods. Examples of atmospheric correction methods See also Adaptive optics References External links Google books result Google books result", "canonical_url": "https://en.wikipedia.org/wiki/Atmospheric_correction", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:56.647832", "metadata": {"word_count": 122, "text_length": 998}}
{"id": "wiki_radiometric_calibration", "query_word": "radiometric calibration", "title": "Radiometric calibration", "summary": "Radiometric calibration is a general term used in science and technology for any set of calibration techniques in support of the measurement of electromagnetic radiation and atomic particle radiation. These can be for instance, in the field of radiometry or the measurement of ionising radiation radiated from a source.", "text": "Radiometric calibration is a general term used in science and technology for any set of calibration techniques in support of the measurement of electromagnetic radiation and atomic particle radiation. These can be for instance, in the field of radiometry or the measurement of ionising radiation radiated from a source. Ionising radiation Ionising radiation is non-visible and requires the use of ionisation detectors such as the Geiger Muller counter or ion chamber for its detection and measurement. Instruments are calibrated using standards traceable to national laboratory radiation standards, such as those at The National Physical Laboratory in the UK. Count rate measurements are normally associated with the detection of particles, such as alpha particles and beta particles. However, for gamma ray and X-ray dose measurements a unit such as the gray or sievert is normally used. The following table shows ionising radiation quantities in SI and non-SI units. Satellite sensor calibration Spectral data acquired by satellite sensors are influenced by a number of factors, such as atmospheric absorption, scattering, sensor-target-illumination geometry, sensor calibration, and image data processing procedures, which tend to change through time. Targets in multi-date scenes are extremely variable and have been nearly impossible to compare in an automated mode. In order to detect genuine landscape changes as revealed by changes in surface reflectance from multi-date satellite images, it is necessary to carry out radiometric correction. Two approaches to radiometric correction are possible: absolute and relative. The absolute approach requires the use of ground measurements at the time of data acquisition for atmospheric correction and sensor calibration. This is not only costly but also impractical when archival satellite image data are used for change analysis. The relative approach to radiometric correction, known as relative radiometric normalization (RRN), is preferred because no in-situ atmospheric data at the time of satellite overpasses are required. This method involves normalizing or rectifying the intensities or digital numbers (DN) of multi-date images band-by-band to a reference image selected by the analyst. The normalized images would appear as if they were acquired with the same sensor under similar atmospheric and illumination conditions to those of the reference image. See also Counts per minute Radiometric resolution References Olsen, Doug; Dou, Changyong; Zhang, Xiaodong; Hu, Lianbo; Kim Hojin; Hildum, Edward. 2010. \"Radiometric Calibration for AgCam\" Remote Sens. 2, no. 2: 464–477. D. Hall; G. Riggs; V. Salomonson. (1995). \"Development of methods for mapping global snow cover using moderate resolution imaging spectroradiometer data.\" Remote Sensing of Environment. 54, no. 2: 127–140.", "canonical_url": "https://en.wikipedia.org/wiki/Radiometric_calibration", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:57.291440", "metadata": {"word_count": 49, "text_length": 2842}}
{"id": "wiki_dimensionality_reduction", "query_word": "dimensionality reduction", "title": "Dimensionality reduction", "summary": "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. Methods are commonly divided into linear and nonlinear approaches. Linear approaches can be further divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.", "text": "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. Methods are commonly divided into linear and nonlinear approaches. Linear approaches can be further divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses. Feature selection The process of feature selection aims to find a suitable subset of the input variables (features, or attributes) for the task at hand. The three strategies are: the filter strategy (e.g., information gain), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors). Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space. Feature projection Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning. Principal component analysis (PCA) The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proved on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors. Non-negative matrix factorization (NMF) NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, such as astronomy. NMF is well known since the multiplicative update rule by Lee & Seung, which has been continuously developed: the inclusion of uncertainties, the consideration of missing data and parallel computation, sequential construction which leads to the stability and linearity of NMF, as well as other updates including handling missing data in digital image processing. With a stable component basis during construction, and a linear modeling process, sequential NMF is able to preserve the flux in direct imaging of circumstellar structures in astronomy, as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar discs. In comparison with PCA, NMF does not remove the mean of the matrices, which leads to physical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al. Kernel PCA Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA. Graph-based kernel PCA Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space) while maximizing the distances between points that are not nearest neighbors. An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis. A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feedforward neural networks with a bottleneck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation. Linear discriminant analysis (LDA) Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. Generalized discriminant analysis (GDA) GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support-vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter. Autoencoder Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation. t-SNE T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for the visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well. UMAP Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant. Dimension reduction For high-dimensional datasets, dimension reduction is usually performed prior to applying a k-nearest neighbors (k-NN) algorithm in order to mitigate the curse of dimensionality. Feature extraction and dimension reduction can be combined in one step, using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques to pre-process the data, followed by clustering via k-NN on feature vectors in a reduced-dimension space. In machine learning, this process is also called low-dimensional embedding. For high-dimensional datasets (e.g., when performing similarity search on live video streams, DNA data, or high-dimensional time series), running a fast approximate k-NN search using locality-sensitive hashing, random projection, \"sketches\", or other high-dimensional similarity search techniques from the VLDB conference toolbox may be the only feasible option. Applications A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved. See also Notes References External links JMLR Special Issue on Variable and Feature Selection ELastic MAPs Archived 2011-07-20 at the Wayback Machine Locally Linear Embedding Visual Comparison of various dimensionality reduction methods A Global Geometric Framework for Nonlinear Dimensionality Reduction", "canonical_url": "https://en.wikipedia.org/wiki/Dimensionality_reduction", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:10:58.933569", "metadata": {"word_count": 137, "text_length": 9383}}
{"id": "wiki_principal_component_analysis", "query_word": "principal component analysis", "title": "Principal component analysis", "summary": "Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing. The data are linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified. The principal components of a collection of points in a real coordinate space are a sequence of p {\\displaystyle p} unit vectors, where the i {\\displaystyle i} -th vector is the direction of a line that best fits the data while being orthogonal to the first i − 1 {\\displaystyle i-1} vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions (i.e., principal components) constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.", "text": "Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing. The data are linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified. The principal components of a collection of points in a real coordinate space are a sequence of p {\\displaystyle p} unit vectors, where the i {\\displaystyle i} -th vector is the direction of a line that best fits the data while being orthogonal to the first i − 1 {\\displaystyle i-1} vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions (i.e., principal components) constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science. Overview When performing PCA, the first principal component of a set of p {\\displaystyle p} variables is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through p {\\displaystyle p} iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The i {\\displaystyle i} -th principal component can be taken as a direction orthogonal to the first i − 1 {\\displaystyle i-1} principal components that maximizes the variance of the projected data. For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain-specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed. History PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 19th century), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis), Eckart–Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science (Lorenz, 1956), empirical eigenfunction decomposition (Sirovich, 1987), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics. Intuition PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small. To find the axes of the ellipsoid, we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values. These transformed values are used instead of the original observed values for each of the variables. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors. Once this is done, each of the mutually-orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform the covariance matrix into a diagonalized form, in which the diagonal elements represent the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues. Biplots and scree plots (degree of explained variance) are used to interpret findings of the PCA. Details PCA is defined as an orthogonal linear transformation on a real inner product space that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. Consider an n × p {\\displaystyle n\\times p} data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor). Mathematically, the transformation is defined by a set of size l {\\displaystyle l} (where l {\\displaystyle l} is usually selected to be strictly less than p {\\displaystyle p} to reduce dimensionality) of p {\\displaystyle p} -dimensional vectors of weights or coefficients w ( k ) = ( w 1 , … , w p ) ( k ) {\\displaystyle \\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}} that map each row vector x ( i ) = ( x 1 , … , x p ) ( i ) {\\displaystyle \\mathbf {x} _{(i)}=(x_{1},\\dots ,x_{p})_{(i)}} of X to a new vector of principal component scores t ( i ) = ( t 1 , … , t l ) ( i ) {\\displaystyle \\mathbf {t} _{(i)}=(t_{1},\\dots ,t_{l})_{(i)}} , given by t k ( i ) = x ( i ) ⋅ w ( k ) f o r i = 1 , … , n k = 1 , … , l {\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,l} in such a way that the individual variables t 1 , … , t l {\\displaystyle t_{1},\\dots ,t_{l}} of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector. The above may equivalently be written in matrix form as T = X W {\\displaystyle \\mathbf {T} =\\mathbf {X} \\mathbf {W} } where T i k = t k ( i ) {\\displaystyle {\\mathbf {T} }_{ik}={t_{k}}_{(i)}} , X i j = x j ( i ) {\\displaystyle {\\mathbf {X} }_{ij}={x_{j}}_{(i)}} , and W j k = w j ( k ) {\\displaystyle {\\mathbf {W} }_{jk}={w_{j}}_{(k)}} . First component In order to maximize variance, the first weight vector w(1) thus has to satisfy w ( 1 ) = arg ⁡ max ‖ w ‖ = 1 { ∑ i ( t 1 ) ( i ) 2 } = arg ⁡ max ‖ w ‖ = 1 { ∑ i ( x ( i ) ⋅ w ) 2 } {\\displaystyle \\mathbf {w} _{(1)}=\\arg \\max _{\\Vert \\mathbf {w} \\Vert =1}\\,\\left\\{\\sum _{i}(t_{1})_{(i)}^{2}\\right\\}=\\arg \\max _{\\Vert \\mathbf {w} \\Vert =1}\\,\\left\\{\\sum _{i}\\left(\\mathbf {x} _{(i)}\\cdot \\mathbf {w} \\right)^{2}\\right\\}} Equivalently, writing this in matrix form gives w ( 1 ) = arg ⁡ max ‖ w ‖ = 1 { ‖ X w ‖ 2 } = arg ⁡ max ‖ w ‖ = 1 { w T X T X w } {\\displaystyle \\mathbf {w} _{(1)}=\\arg \\max _{\\left\\|\\mathbf {w} \\right\\|=1}\\left\\{\\left\\|\\mathbf {Xw} \\right\\|^{2}\\right\\}=\\arg \\max _{\\left\\|\\mathbf {w} \\right\\|=1}\\left\\{\\mathbf {w} ^{\\mathsf {T}}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {Xw} \\right\\}} Since w(1) has been defined to be a unit vector, it equivalently also satisfies w ( 1 ) = arg ⁡ max { w T X T X w w T w } {\\displaystyle \\mathbf {w} _{(1)}=\\arg \\max \\left\\{{\\frac {\\mathbf {w} ^{\\mathsf {T}}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {Xw} }{\\mathbf {w} ^{\\mathsf {T}}\\mathbf {w} }}\\right\\}} The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector. With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) ⋅ w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) ⋅ w(1)} w(1). Further components The k-th component can be found by subtracting the first k − 1 principal components from X: X ^ k = X − ∑ s = 1 k − 1 X w ( s ) w ( s ) T {\\displaystyle \\mathbf {\\hat {X}} _{k}=\\mathbf {X} -\\sum _{s=1}^{k-1}\\mathbf {X} \\mathbf {w} _{(s)}\\mathbf {w} _{(s)}^{\\mathsf {T}}} and then finding the weight vector which extracts the maximum variance from this new data matrix w ( k ) = a r g m a x ‖ w ‖ = 1 ⁡ { ‖ X ^ k w ‖ 2 } = arg ⁡ max { w T X ^ k T X ^ k w w T w } {\\displaystyle \\mathbf {w} _{(k)}=\\mathop {\\operatorname {arg\\,max} } _{\\left\\|\\mathbf {w} \\right\\|=1}\\left\\{\\left\\|\\mathbf {\\hat {X}} _{k}\\mathbf {w} \\right\\|^{2}\\right\\}=\\arg \\max \\left\\{{\\tfrac {\\mathbf {w} ^{\\mathsf {T}}\\mathbf {\\hat {X}} _{k}^{\\mathsf {T}}\\mathbf {\\hat {X}} _{k}\\mathbf {w} }{\\mathbf {w} ^{T}\\mathbf {w} }}\\right\\}} It turns out that this gives the remaining eige", "canonical_url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:00.343097", "metadata": {"word_count": 184, "text_length": 74742}}
{"id": "wiki_independent_component_analysis", "query_word": "independent component analysis", "title": "Independent component analysis", "summary": "In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other. ICA was invented by Jeanny Hérault and Christian Jutten in 1985. ICA is a special case of blind source separation. A common example application of ICA is the \"cocktail party problem\" of listening in on one person's speech in a noisy room.", "text": "In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other. ICA was invented by Jeanny Hérault and Christian Jutten in 1985. ICA is a special case of blind source separation. A common example application of ICA is the \"cocktail party problem\" of listening in on one person's speech in a noisy room. Introduction Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources. The question then is whether it is possible to separate these contributing sources from the observed total signal. When the statistical independence assumption is correct, blind ICA separation of a mixed signal gives very good results. It is also used for signals that are not supposed to be generated by mixing for analysis purposes. A simple application of ICA is the \"cocktail party problem\", where the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room. Usually the problem is simplified by assuming no time delays or echoes. Note that a filtered and delayed signal is a copy of a dependent component, and thus the statistical independence assumption is not violated. Mixing weights for constructing the M {\\textstyle M} observed signals from the N {\\textstyle N} components can be placed in an M × N {\\textstyle M\\times N} matrix. An important thing to consider is that if N {\\textstyle N} sources are present, at least N {\\textstyle N} observations (e.g. microphones if the observed signal is audio) are needed to recover the original signals. When there are an equal number of observations and source signals, the mixing matrix is square ( M = N {\\textstyle M=N} ). Other cases of underdetermined ( M < N {\\textstyle M<N} ) and overdetermined ( M > N {\\textstyle M>N} ) have been investigated. The success of ICA separation of mixed signals relies on two assumptions and three effects of mixing source signals. Two assumptions: The source signals are independent of each other. The values in each source signal have non-Gaussian distributions. Three effects of mixing source signals: Independence: As per assumption 1, the source signals are independent; however, their signal mixtures are not. This is because the signal mixtures share the same source signals. Normality: According to the Central Limit Theorem, the distribution of a sum of independent random variables with finite variance tends towards a Gaussian distribution.Loosely speaking, a sum of two independent random variables usually has a distribution that is closer to Gaussian than any of the two original variables. Here we consider the value of each signal as the random variable. Complexity: The temporal complexity of any signal mixture is greater than that of its simplest constituent source signal. Those principles contribute to the basic establishment of ICA. If the signals extracted from a set of mixtures are independent and have non-Gaussian distributions or have low complexity, then they must be source signals. Another common example is image steganography, where ICA is used to embed one image within another. For instance, two grayscale images can be linearly combined to create mixed images in which the hidden content is visually imperceptible. ICA can then be used to recover the original source images from the mixtures. This technique underlies digital watermarking, which allows the embedding of ownership information into images, as well as more covert applications such as undetected information transmission. The method has even been linked to real-world cyberespionage cases. In such applications, ICA serves to unmix the data based on statistical independence, making it possible to extract hidden components that are not apparent in the observed data. Steganographic techniques, including those potentially involving ICA-based analysis, have been used in real-world cyberespionage cases. In 2010, the FBI uncovered a Russian spy network known as the \"Illegals Program\" (Operation Ghost Stories), where agents used custom-built steganography tools to conceal encrypted text messages within image files shared online. In another case, a former General Electric engineer, Xiaoqing Zheng, was convicted in 2022 for economic espionage. Zheng used steganography to exfiltrate sensitive turbine technology by embedding proprietary data within image files for transfer to entities in China. Defining component independence ICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm. The two broadest definitions of independence for ICA are Minimization of mutual information Maximization of non-Gaussianity The Minimization-of-Mutual information (MMI) family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy. The non-Gaussianity family of ICA algorithms, motivated by the central limit theorem, uses kurtosis and negentropy. Typical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Mathematical definitions Linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case. General Derivation In the classical ICA model, it is assumed that the observed data x i ∈ R m {\\displaystyle \\mathbf {x} _{i}\\in \\mathbb {R} ^{m}} at time t i {\\displaystyle t_{i}} is generated from source signals s i ∈ R m {\\displaystyle \\mathbf {s} _{i}\\in \\mathbb {R} ^{m}} via a linear transformation x i = A s i {\\displaystyle \\mathbf {x} _{i}=A\\mathbf {s} _{i}} , where A {\\displaystyle A} is an unknown, invertible mixing matrix. To recover the source signals, the data is first centered (zero mean), and then whitened so that the transformed data has unit covariance. This whitening reduces the problem from estimating a general matrix A {\\displaystyle A} to estimating an orthogonal matrix V {\\displaystyle V} , significantly simplifying the search for independent components. If the covariance matrix of the centered data is Σ x = A A ⊤ {\\displaystyle \\Sigma _{x}=AA^{\\top }} , then using the eigen-decomposition Σ x = Q D Q ⊤ {\\displaystyle \\Sigma _{x}=QDQ^{\\top }} , the whitening transformation can be taken as D − 1 / 2 Q ⊤ {\\displaystyle D^{-1/2}Q^{\\top }} . This step ensures that the recovered sources are uncorrelated and of unit variance, leaving only the task of rotating the whitened data to maximize statistical independence. This general derivation underlies many ICA algorithms and is foundational in understanding the ICA model. Reduced Mixing Problem Independent component analysis (ICA) addresses the problem of recovering a set of unobserved source signals s i = ( s i 1 , s i 2 , … , s i m ) T {\\displaystyle s_{i}=(s_{i1},s_{i2},\\dots ,s_{im})^{T}} from observed mixed signals x i = ( x i 1 , x i 2 , … , x i m ) T {\\displaystyle x_{i}=(x_{i1},x_{i2},\\dots ,x_{im})^{T}} , based on the linear mixing model: x i = A s i , {\\displaystyle x_{i}=A\\,s_{i},} where the A {\\displaystyle A} is an m × m {\\displaystyle m\\times m} invertible matrix called the mixing matrix, s i {\\displaystyle s_{i}} represents the m‑dimensional vector containing the values of the sources at time t i {\\displaystyle t_{i}} , and x i {\\displaystyle x_{i}} is the corresponding vector of observed values at time t i {\\displaystyle t_{i}} . The goal is to estimate both A {\\displaystyle A} and the source signals { s i } {\\displaystyle \\{s_{i}\\}} solely from the observed data { x i } {\\displaystyle \\{x_{i}\\}} . After centering, the Gram matrix is computed as: ( X ∗ ) T X ∗ = Q D Q T , {\\displaystyle (X^{*})^{T}X^{*}=Q\\,D\\,Q^{T},} where D is a diagonal matrix with positive entries (assuming X ∗ {\\displaystyle X^{*}} has maximum rank), and Q is an orthogonal matrix. Writing the SVD of the mixing matrix A = U Σ V T {\\displaystyle A=U\\Sigma V^{T}} and comparing with A A T = U Σ 2 U T {\\displaystyle AA^{T}=U\\Sigma ^{2}U^{T}} the mixing A has the form A = Q D 1 / 2 V T . {\\displaystyle A=Q\\,D^{1/2}\\,V^{T}.} So, the normalized source values satisfy s i ∗ = V y i ∗ {\\displaystyle s_{i}^{*}=V\\,y_{i}^{*}} , where y i ∗ = D − 1 2 Q T x i ∗ . {\\displaystyle y_{i}^{*}=D^{-{\\tfrac {1}{2}}}Q^{T}x_{i}^{*}.} Thus, ICA reduces to finding the orthogonal matrix V {\\displaystyle V} . This matrix can be computed using optimization techniques via projection pursuit methods (see Projection Pursuit). Well-known algorithms for ICA include infomax, FastICA, JADE, and kernel-independent component analysis, among others. In general, ICA cannot identify the actual number of source signals, a uniquely correct ordering of the source signals, nor the proper scaling (including sign) of the source signals. ICA is important to blind signal separation and has many practical applications. It is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent. Linear noiseless ICA The components x i {\\displaystyle x_{i}} of the observed random vector x = ( x 1 , … , x m ) T {\\displaystyle {\\boldsymbol {x}}=(x_{1},\\ldots ,x_{m})^{T}} a", "canonical_url": "https://en.wikipedia.org/wiki/Independent_component_analysis", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:01.415447", "metadata": {"word_count": 83, "text_length": 37227}}
{"id": "wiki_abundance_estimation", "query_word": "abundance estimation", "title": "Abundance estimation", "summary": "Abundance estimation comprises all statistical methods for estimating the number of individuals in a population. In ecology, this may be anything from estimating the number of daisies in a field to estimating the number of blue whales in the ocean.", "text": "Abundance estimation comprises all statistical methods for estimating the number of individuals in a population. In ecology, this may be anything from estimating the number of daisies in a field to estimating the number of blue whales in the ocean. Plot sampling Mark-recapture Distance sampling References Further reading Distance Sampling: Estimating Abundance of Biological Populations – S. T. Buckland, D. R. Anderson, K. P. Burnham, J. L. Laake Estimating Abundance of African Wildlife: An Aid to Adaptive Management – Hugo Jachmann Advanced Distance Sampling: Estimating Abundance of Biological Populations Geostatistics for Estimating Fish Abundance – J. Rivoirard, J. Simmonds, K. G. Foote, P. Fernandes, N. Bez Estimating the Abundance of Arboreal Forage Lichens: User's Guide : a Guide ... – Susan K. Stevenson, Harold M. Armleder, Art N. Lance", "canonical_url": "https://en.wikipedia.org/wiki/Abundance_estimation", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:02.725979", "metadata": {"word_count": 40, "text_length": 854}}
{"id": "wiki_normalized_difference_vegetation_index", "query_word": "normalized difference vegetation index", "title": "Normalized difference vegetation index", "summary": "The normalized difference vegetation index (NDVI) is a widely used metric for quantifying the health and density of vegetation using sensor data. It is calculated from spectrometric data at two specific bands: red and near-infrared. The spectrometric data is usually sourced from remote sensors, such as satellites. The metric is popular in industry because of its accuracy. It has a high correlation with the true state of vegetation on the ground. The index is easy to interpret: NDVI will be a value between -1 and 1. An area with nothing growing in it will have an NDVI of zero. NDVI will increase in proportion to vegetation growth. An area with dense, healthy vegetation will have an NDVI of one. NDVI values less than 0 suggest a lack of dry land. An ocean will yield an NDVI of -1", "text": "The normalized difference vegetation index (NDVI) is a widely used metric for quantifying the health and density of vegetation using sensor data. It is calculated from spectrometric data at two specific bands: red and near-infrared. The spectrometric data is usually sourced from remote sensors, such as satellites. The metric is popular in industry because of its accuracy. It has a high correlation with the true state of vegetation on the ground. The index is easy to interpret: NDVI will be a value between -1 and 1. An area with nothing growing in it will have an NDVI of zero. NDVI will increase in proportion to vegetation growth. An area with dense, healthy vegetation will have an NDVI of one. NDVI values less than 0 suggest a lack of dry land. An ocean will yield an NDVI of -1 Brief history The exploration of outer space started in earnest with the launch of Sputnik 1 by the Soviet Union on 4 October 1957. This was the first man-made satellite orbiting the Earth. Subsequent successful launches, both in the Soviet Union (e.g., the Sputnik and Cosmos programs), and in the U.S. (e.g., the Explorer program), quickly led to the design and operation of dedicated meteorological satellites. These are orbiting platforms embarking instruments specially designed to observe the Earth's atmosphere and surface with a view to improve weather forecasting. Starting in 1960, the TIROS series of satellites embarked television cameras and radiometers. This was later (1964 onwards) followed by the Nimbus satellites and the family of Advanced Very High Resolution Radiometer instruments on board the National Oceanic and Atmospheric Administration (NOAA) platforms. The latter measures the reflectance of the planet in red and near-infrared bands, as well as in the thermal infrared. In parallel, NASA developed the Earth Resources Technology Satellite (ERTS), which became the precursor to the Landsat program. These early sensors had minimal spectral resolution, but tended to include bands in the red and near-infrared, which are useful to distinguish vegetation and clouds, amongst other targets. With the launch of the first ERTS satellite – which was soon to be renamed Landsat 1 – on July 23, 1972 with its MultiSpectral Scanner (MSS) NASA funded a number of investigations to determine its capabilities for Earth remote sensing. One of those early studies was directed toward examining the spring vegetation green-up and subsequent summer and fall dry-down (the so-called \"vernal advancement and retrogradation\") throughout the north to south expanse of the Great Plains region of the central U.S. This region covered a wide range of latitudes from the southern tip of Texas to the U.S.-Canada border, which resulted in a wide range of solar zenith angles at the time of the satellite observations. The researchers for this Great Plains study (PhD student Donald Deering and his advisor Dr. Robert Hass) found that their ability to correlate, or quantify, the biophysical characteristics of the rangeland vegetation of this region from the satellite spectral signals was confounded by these differences in solar zenith angle across this strong latitudinal gradient. With the assistance of a resident mathematician (Dr. John Schell), they studied solutions to this dilemma and subsequently developed the ratio of the difference of the red and infrared radiances over their sum as a means to adjust for or \"normalize\" the effects of the solar zenith angle. Originally, they called this ratio the \"Vegetation Index\" (and another variant, the square-root transformation of the difference-sum ratio, the \"Transformed Vegetation Index\"); but as several other remote sensing researchers were identifying the simple red/infrared ratio and other spectral ratios as the \"vegetation index,\" they eventually began to identify the difference/sum ratio formulation as the normalized difference vegetation index. The earliest reported use of NDVI in the Great Plains study was in 1973 by Rouse et al. (Dr. John Rouse was the Director of the Remote Sensing Center of Texas A&M University where the Great Plains study was conducted). However, they were preceded in formulating a normalized difference spectral index by Kriegler et al. in 1969. Soon after the launch of ERTS-1 (Landsat-1), Compton Tucker of NASA's Goddard Space Flight Center produced a series of early scientific journal articles describing uses of the NDVI. Thus, NDVI was one of the most successful of many attempts to simply and quickly identify vegetated areas and their \"condition,\" and it remains the most well-known and used index to detect live green plant canopies in multispectral remote sensing data. Once the feasibility to detect vegetation had been demonstrated, users tended to also use the NDVI to quantify the photosynthetic capacity of plant canopies. This, however, can be a rather more complex undertaking if not done properly, as is discussed below. Rationale Live green plants absorb solar radiation in the photosynthetically active radiation (PAR) spectral region, which they use as a source of energy in the process of photosynthesis. Leaf cells have also evolved to re-emit solar radiation in the near-infrared spectral region (which carries approximately half of the total incoming solar energy), because the photon energy at wavelengths longer than about 700 nanometers is too low to synthesize organic molecules. A strong absorption at these wavelengths would only result in overheating the plant and possibly damaging the tissues. Hence, live green plants appear relatively dark in the PAR and relatively bright in the near-infrared. By contrast, clouds and snow tend to be rather bright in the red (as well as other visible wavelengths) and quite dark in the near-infrared. The pigment in plant leaves, chlorophyll, strongly absorbs visible light (from 400 to 700 nm) for use in photosynthesis. The cell structure of the leaves, on the other hand, strongly reflects near-infrared light (from 700 to 1100 nm). The more leaves a plant has, the more these wavelengths of light are affected. Since early instruments of Earth Observation, such as NASA's ERTS and NOAA's AVHRR, acquired data in visible and near-infrared, it was natural to exploit the strong differences in plant reflectance to determine their spatial distribution in these satellite images. The NDVI is calculated from these individual measurements as follows: NDVI = ( NIR − Red ) ( NIR + Red ) {\\displaystyle {\\mbox{NDVI}}={\\frac {({\\mbox{NIR}}-{\\mbox{Red}})}{({\\mbox{NIR}}+{\\mbox{Red}})}}} where Red and NIR stand for the spectral reflectance measurements acquired in the red (visible) and near-infrared regions, respectively. These spectral reflectances are themselves ratios of the reflected radiation to the incoming radiation in each spectral band individually, hence they take on values between 0 and 1. By design, the NDVI itself thus varies between -1 and +1. NDVI is functionally, but not linearly, equivalent to the simple infrared/red ratio (NIR/VIS). The advantage of NDVI over a simple infrared/red ratio is therefore generally limited to any possible linearity of its functional relationship with vegetation properties (e.g. biomass). The simple ratio (unlike NDVI) is always positive, which may have practical advantages, but it also has a mathematically infinite range (0 to infinity), which can be a practical disadvantage as compared to NDVI. Also in this regard, note that the VIS term in the numerator of NDVI only scales the result, thereby creating negative values. NDVI is functionally and linearly equivalent to the ratio NIR / (NIR+VIS), which ranges from 0 to 1 and is thus never negative nor limitless in range. But the most important concept in the understanding of the NDVI algebraic formula is that, despite its name, it is a transformation of a spectral ratio (NIR/VIS), and it has no functional relationship to a spectral difference (NIR-VIS). In general, if there is much more reflected radiation in near-infrared wavelengths than in visible wavelengths, then the vegetation in that pixel is likely to be dense and may contain some type of forest. Subsequent work has shown that the NDVI is directly related to the photosynthetic capacity and hence energy absorption of plant canopies. Although the index can take negative values, even in densely populated urban areas the NDVI usually has a (small) positive value. Negative values are more likely to be observed in the atmosphere and some specific materials. Performance and limitations It can be seen from its mathematical definition that the NDVI of an area containing a dense vegetation canopy will tend to positive values (say 0.3 to 0.8) while clouds and snow fields will be characterized by negative values of this index. Other targets on Earth visible from space include: free standing water (e.g., oceans, seas, lakes and rivers) which have a rather low reflectance in both spectral bands (at least away from shores) and thus result in very low positive or even slightly negative NDVI values, soils which generally exhibit a near-infrared spectral reflectance somewhat larger than the red, and thus tend to also generate rather small positive NDVI values (say 0.1 to 0.2). In addition to the simplicity of the algorithm and its capacity to broadly distinguish vegetated areas from other surface types, the NDVI also has the advantage of compressing the size of the data to be manipulated by a factor 2 (or more), since it replaces the two spectral bands by a single new field (eventually coded on 8 bits instead of the 10 or more bits of the original data). The NDVI has been widely used in applications for which it was not originally designed. Using the NDVI for quantitative assessments (as opposed to qualitative surveys as indicated above) raises a number of issues that may seriously limit the actual usefulness of this index if they are not properly addressed. The following subsections review some of these issues. Mathematically, the sum and ", "canonical_url": "https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:05.162689", "metadata": {"word_count": 138, "text_length": 19520}}
{"id": "wiki_enhanced_vegetation_index", "query_word": "enhanced vegetation index", "title": "Enhanced vegetation index", "summary": "The enhanced vegetation index (EVI) is an 'optimized' vegetation index designed to enhance the vegetation signal with improved sensitivity in high biomass regions and improved vegetation monitoring through a de-coupling of the canopy background signal and a reduction in atmosphere influences. EVI is computed following this equation: EVI = G × ( NIR − Red ) ( NIR + C 1 × Red − C 2 × Blue + L ) {\\displaystyle {\\text{EVI}}=G\\times {\\frac {({\\text{NIR}}-{\\text{Red}})}{({\\text{NIR}}+C_{1}\\times {\\text{Red}}-C_{2}\\times {\\text{Blue}}+L)}}} where: NIR, Red, and Blue are atmospherically-corrected and partially atmosphere-corrected (Rayleigh and ozone absorption) surface reflectances L is the canopy background adjustment that addresses non-linear, differential NIR and red radiant transfer through a canopy, and C1, C2 are the coefficients of the aerosol resistance term, which uses the blue band to correct for aerosol influences in the red band. G is a gain factor. The coefficients adopted in the MODIS-EVI algorithm are: L=1, C1 = 6, C2 = 7.5, and G = 2.5. Whereas the Normalized Difference Vegetation Index (NDVI) is chlorophyll sensitive, the EVI is more responsive to canopy structural variations, including leaf area index (LAI), canopy type, plant physiognomy, and canopy architecture. The two vegetation indices complement each other in global vegetation studies and improve upon the detection of vegetation changes and extraction of canopy biophysical parameters. Another difference between Normalized Difference Vegetation Index (NDVI) and EVI is that in the presence of snow, NDVI decreases, while EVI increases (Huete, 2002). Starting 2000, and after the launch of the two MODIS sensors on Terra (satellite) and Aqua (satellite) by NASA, EVI was adopted as a standard product by NASA and became extremely popular with users due to its ability to eliminate background and atmosphere noises, as well as its non saturation, a typical NDVI problem. EVI is currently distributed for free by the USGS LP DAAC.", "text": "The enhanced vegetation index (EVI) is an 'optimized' vegetation index designed to enhance the vegetation signal with improved sensitivity in high biomass regions and improved vegetation monitoring through a de-coupling of the canopy background signal and a reduction in atmosphere influences. EVI is computed following this equation: EVI = G × ( NIR − Red ) ( NIR + C 1 × Red − C 2 × Blue + L ) {\\displaystyle {\\text{EVI}}=G\\times {\\frac {({\\text{NIR}}-{\\text{Red}})}{({\\text{NIR}}+C_{1}\\times {\\text{Red}}-C_{2}\\times {\\text{Blue}}+L)}}} where: NIR, Red, and Blue are atmospherically-corrected and partially atmosphere-corrected (Rayleigh and ozone absorption) surface reflectances L is the canopy background adjustment that addresses non-linear, differential NIR and red radiant transfer through a canopy, and C1, C2 are the coefficients of the aerosol resistance term, which uses the blue band to correct for aerosol influences in the red band. G is a gain factor. The coefficients adopted in the MODIS-EVI algorithm are: L=1, C1 = 6, C2 = 7.5, and G = 2.5. Whereas the Normalized Difference Vegetation Index (NDVI) is chlorophyll sensitive, the EVI is more responsive to canopy structural variations, including leaf area index (LAI), canopy type, plant physiognomy, and canopy architecture. The two vegetation indices complement each other in global vegetation studies and improve upon the detection of vegetation changes and extraction of canopy biophysical parameters. Another difference between Normalized Difference Vegetation Index (NDVI) and EVI is that in the presence of snow, NDVI decreases, while EVI increases (Huete, 2002). Starting 2000, and after the launch of the two MODIS sensors on Terra (satellite) and Aqua (satellite) by NASA, EVI was adopted as a standard product by NASA and became extremely popular with users due to its ability to eliminate background and atmosphere noises, as well as its non saturation, a typical NDVI problem. EVI is currently distributed for free by the USGS LP DAAC. Two-band EVI Two reasons drive the search for a two-band EVI: Extending the EVI back in time, using the AVHRR record. The AVHRR sensors lacks a blue band, hence using a three-band EVI version is not possible. This could potentially lead to a 30-year EVI record that complements the NDVI record. The blue band has always been problematic, and its Signal to Noise ratio (S/N) quite poor. This is mainly due to the nature of the reflected energy in this part of the spectrum over land, which is extremely low. Additionally, the original motivation for the inclusion of the blue band (NDVI uses only NIR and red) in the 1990s was to mitigate atmospheric aerosol interference. However, since that time, sensor level atmospheric adjustment has improved substantially minimizing the marginal impact of blue band on accuracy. We'll call the two-band EVI \"EVI2\", and the three-band EVI simply \"EVI\". A number of EVI2 approaches are available; the one of Jiang et al. 2008 is: Define EVI2 as a two-band index in the form of EVI2 = f ( Red , NIR ) = G × ( NIR − Red ) ( L + NIR + C × Red ) {\\displaystyle {\\text{EVI2}}=f({\\text{Red}},{\\text{NIR}})=G\\times {({\\text{NIR}}-{\\text{Red}}) \\over (L+{\\text{NIR}}+C\\times {\\text{Red}})}} Find coefficients G, L, and C (organic) to minimize the difference between EVI2 and EVI. They play a similar role to the analogous factors in EVI, but are not actually grounded in physics but found by mathematics. This leads to multiple (infinite) solutions but a few (vaguely physics-based) conditions could be imposed on the solution to generate the best coefficients. With MODIS data, we have EVI2 = 2.5 × ( NIR − Red ) ( NIR + 2.4 ∗ Red + 1 ) {\\displaystyle {\\text{EVI2}}=2.5\\times {({\\text{NIR}}-{\\text{Red}}) \\over ({\\text{NIR}}+2.4*{\\text{Red}}+1)}} Jiang's EVI2 has the best similarity with the 3-band EVI, particularly when atmospheric effects are insignificant and data quality is good. EVI2 can be used for sensors without a blue band, such as the Advanced Very High Resolution Radiometer (AVHRR), and may reveal different vegetation dynamics in comparison with the current AVHRR NDVI dataset. There exist some other EVI2s, one being that of Miura 2008 for ASTER: EVI2 = 2.4 × ( NIR − Red ) ( NIR + Red + 1 ) {\\displaystyle {\\text{EVI2}}=2.4\\times {({\\text{NIR}}-{\\text{Red}}) \\over ({\\text{NIR}}+{\\text{Red}}+1)}} The ASTER sensors have a different spectral range compared to the MODIS ones. Application of EVI An example of the utility of EVI was reported by Huete et al. (2006). Previously, the Amazon rainforest was viewed as having a monotonous growing season, where there is no particular seasonality to plant growth. Using the MODIS EVI product, Huete et al. showed that the Amazon forest exhibits a distinct increase in growth during the dry season. This phenomenon has implications for our understanding of the carbon cycle and sinks in the region, though it is unclear whether this is a long-standing pattern or an emergent shift associated with climate change. References Kim, Y., Huete, A. R., Miura, T., Jiang, Z. (2010). Spectral compatibility of vegetation indices across sensors: band decomposition analysis with Hyperion data, J. Appl. Remote Sens, 4(1), 043520, {{doi:10.1117/1.3400635}}. External links Terrestrial Biophysics and Remote Sensing Group at the University of Arizona The USGS Land Processes Distributed Active Archive Center the MODIS Vegetation Product suite MODIS NATIONAL AERONAUTICS AND SPACE ADMINISTRATION", "canonical_url": "https://en.wikipedia.org/wiki/Enhanced_vegetation_index", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:06.159870", "metadata": {"word_count": 305, "text_length": 5493}}
{"id": "wiki_normalized_difference_water_index", "query_word": "normalized difference water index", "title": "Normalized difference water index", "summary": "Normalized Difference Water Index (NDWI) may refer to one of at least two remote sensing-derived indexes related to liquid water: One is used to monitor changes in water content of leaves, using near-infrared (NIR) and short-wave infrared (SWIR) wavelengths, proposed by Gao in 1996: NDWI = ( X n i r − X s w i r ) ( X n i r + X s w i r ) {\\displaystyle {\\mbox{NDWI}}={\\frac {(Xnir-Xswir)}{(Xnir+Xswir)}}} Another is used to monitor changes related to water content in water bodies, using green and NIR wavelengths, defined by McFeeters (1996): NDWI = ( X g r e e n − X n i r ) ( X g r e e n + X n i r ) {\\displaystyle {\\mbox{NDWI}}={\\frac {(Xgreen-Xnir)}{(Xgreen+Xnir)}}}", "text": "Normalized Difference Water Index (NDWI) may refer to one of at least two remote sensing-derived indexes related to liquid water: One is used to monitor changes in water content of leaves, using near-infrared (NIR) and short-wave infrared (SWIR) wavelengths, proposed by Gao in 1996: NDWI = ( X n i r − X s w i r ) ( X n i r + X s w i r ) {\\displaystyle {\\mbox{NDWI}}={\\frac {(Xnir-Xswir)}{(Xnir+Xswir)}}} Another is used to monitor changes related to water content in water bodies, using green and NIR wavelengths, defined by McFeeters (1996): NDWI = ( X g r e e n − X n i r ) ( X g r e e n + X n i r ) {\\displaystyle {\\mbox{NDWI}}={\\frac {(Xgreen-Xnir)}{(Xgreen+Xnir)}}} Overview In remote sensing, ratio image or spectral ratioing are enhancement techniques in which a raster pixel from one spectral band is divided by the corresponding value in another band. Both the indexes above share this same functional form; the choice of bands used is what makes them appropriate for a specific purpose. If looking to monitor vegetation in drought affected areas, then it is advisable to use NDWI index proposed by Gao utilizing NIR and SWIR. The SWIR reflectance in this index reflects changes in both the vegetation water content and the spongy mesophyll structure in vegetation canopies. The NIR reflectance is affected by leaf internal structure and leaf dry matter content, but not by water content. The combination of the NIR with the SWIR removes variations induced by leaf internal structure and leaf dry matter content, improving the accuracy in retrieving the vegetation water content. NDWI concept as formulated by Gao combining reflectance of NIR and SWIR is more common and has wider range of application. It can be used for exploring water content at single leaf level as well as canopy/satellite level. The range of application of NDWI (Gao, 1996) spreads from agricultural monitoring for crop irrigation and pasture management to forest monitoring for assessing fire risk and live fuel moisture particularly relevant in the context of climate change. Different SWIR bands can be used to characterize the water absorption in generalized form of NDWI as shown in eq. 1. Two major water absorption features in SWIR spectral region are centered near 1450 nm and 1950 nm while two minor absorption features are centered near 970 and 1200 nm in a living vegetation spectrum. Sentinel-2 MSI has two spectral bands in SWIR region: band 11 (central wavelength 1610 nm) and band 12 (central wavelength 2200 nm). Spectral band in NIR region with similar 20 m ground resolution is band 8A (central wavelength 865 nm). Sentinel-2 NDWI for agricultural monitoring of drought and irrigation management can be constructed using either combinations: band 8A (864 nm) and band 11 (1610 nm) band 8A (864 nm) and band 12 (2200 nm) Both formulations are suitable. Sentinel-2 NDWI for waterbody detection can be constructed by using: \"Green\" Band 3 (559 nm) and \"NIR\" Band 8A (864 nm) McFeeters index: If looking for water bodies or change in water level (e.g. flooding), then it is advisable to use the green and NIR spectral bands or green and SWIR spectral bands. Modification of normalised difference water index (MNDWI) has been suggested for improved detection of open water by replacing NIR spectral band with SWIR. Interpretation Visual or digital interpretation of the output image/raster created is similar to NDVI: -1 to 0 - Bright surface with no vegetation or water content +1 - represent water content For the second variant of the NDWI, another threshold can also be found in that avoids creating false alarms in urban areas: < 0.3 - Non-water >= 0.3 - Water. See also Normalized Difference Red Edge Index Normalized difference vegetation index References External links (NDWI for crop monitoring: index by Gao, 1996) (MODIS NDWI calculation) (Landsat NDWI calculation) (regarding the McFeeters index for water bodies) (Modification of the McFeeters index for improved detection of water bodies)", "canonical_url": "https://en.wikipedia.org/wiki/Normalized_difference_water_index", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:06.819842", "metadata": {"word_count": 126, "text_length": 3999}}
{"id": "wiki_clay_minerals", "query_word": "clay minerals", "title": "Clay mineral", "summary": "Clay minerals are hydrous aluminium phyllosilicates (e.g. kaolin, Al2Si2O5(OH)4), sometimes with variable amounts of iron, magnesium, alkali metals, alkaline earths, and other cations found on or near some planetary surfaces. Clay minerals form in the presence of water and have been important to life, and many theories of abiogenesis involve them. They are important constituents of soils, and have been useful to humans since ancient times in agriculture and manufacturing.", "text": "Clay minerals are hydrous aluminium phyllosilicates (e.g. kaolin, Al2Si2O5(OH)4), sometimes with variable amounts of iron, magnesium, alkali metals, alkaline earths, and other cations found on or near some planetary surfaces. Clay minerals form in the presence of water and have been important to life, and many theories of abiogenesis involve them. They are important constituents of soils, and have been useful to humans since ancient times in agriculture and manufacturing. Properties Clay is a very fine-grained geologic material that develops plasticity when wet, but becomes hard, brittle and non–plastic upon drying or firing. It is a very common material, and is the oldest known ceramic. Prehistoric humans discovered the useful properties of clay and used it for making pottery. The chemistry of clay, including its capacity to retain nutrient cations such as potassium and ammonium, is important to soil fertility. Because the individual particles in clay are less than 4 micrometers (0.00016 in) in size, they cannot be characterized by ordinary optical or physical methods. The crystallographic structure of clay minerals became better understood in the 1930s with advancements in the x-ray diffraction (XRD) technique indispensable to deciphering their crystal lattice. Clay particles were found to be predominantly sheet silicate (phyllosilicate) minerals, now grouped together as clay minerals. Their structure is based on flat hexagonal sheets similar to those of the mica group of minerals. Standardization in terminology arose during this period as well, with special attention given to similar words that resulted in confusion, such as sheet and plane. Because clay minerals are usually (but not necessarily) ultrafine-grained, special analytical techniques are required for their identification and study. In addition to X-ray crystallography, these include electron diffraction methods, various spectroscopic methods such as Mössbauer spectroscopy, infrared spectroscopy, Raman spectroscopy, and SEM-EDS or automated mineralogy processes. These methods can be augmented by polarized light microscopy, a traditional technique establishing fundamental occurrences or petrologic relationships. Occurrence Clay minerals are common weathering products (including weathering of feldspar) and low-temperature hydrothermal alteration products. Clay minerals are very common in soils, in fine-grained sedimentary rocks such as shale, mudstone, and siltstone and in fine-grained metamorphic slate and phyllite. Given the requirement of water, clay minerals are relatively rare in the Solar System, though they occur extensively on Earth where water has interacted with other minerals and organic matter. Clay minerals have been detected at several locations on Mars, including Echus Chasma, Mawrth Vallis, the Memnonia quadrangle and the Elysium quadrangle. Spectrography has confirmed their presence on celestial bodies including the dwarf planet Ceres, asteroid 101955 Bennu, and comet Tempel 1, as well as Jupiter's moon Europa. Structure Like all phyllosilicates, clay minerals are characterised by two-dimensional sheets of corner-sharing SiO4 tetrahedra or AlO4 octahedra. The sheet units have the chemical composition (Al, Si)3O4. Each silica tetrahedron shares three of its vertex oxygen ions with other tetrahedra, forming a hexagonal array in two dimensions. The fourth oxygen ion is not shared with another tetrahedron and all of the tetrahedra \"point\" in the same direction; i.e. all of the unshared oxygen ions are on the same side of the sheet. These unshared oxygen ions are called apical oxygen ions. In clays, the tetrahedral sheets are always bonded to octahedral sheets formed from small cations, such as aluminum or magnesium, and coordinated by six oxygen atoms. The unshared vertex from the tetrahedral sheet also forms part of one side of the octahedral sheet, but an additional oxygen atom is located above the gap in the tetrahedral sheet at the center of the six tetrahedra. This oxygen atom is bonded to a hydrogen atom forming an OH group in the clay structure. Clays can be categorized depending on the way that tetrahedral and octahedral sheets are packaged into layers. If there is only one tetrahedral and one octahedral group in each layer the clay is known as a 1:1 clay. The alternative, known as a 2:1 clay, has two tetrahedral sheets with the unshared vertex of each sheet pointing towards each other and forming each side of the octahedral sheet. Bonding between the tetrahedral and octahedral sheets requires that the tetrahedral sheet becomes corrugated or twisted, causing ditrigonal distortion to the hexagonal array, and the octahedral sheet is flattened. This minimizes the overall bond-valence distortions of the crystallite. Depending on the composition of the tetrahedral and octahedral sheets, the layer will have no charge or will have a net negative charge. If the layers are charged this charge is balanced by interlayer cations such as Na+ or K+ or by a lone octahedral sheet. The interlayer may also contain water. The crystal structure is formed from a stack of layers interspaced with the interlayers. Classification Clay minerals can be classified as 1:1 or 2:1. A 1:1 clay would consist of one tetrahedral sheet and one octahedral sheet, and examples would be kaolinite and serpentine. A 2:1 clay consists of an octahedral sheet sandwiched between two tetrahedral sheets, and examples are talc, vermiculite, and montmorillonite. The layers in 1:1 clays are uncharged and are bonded by hydrogen bonds between layers, but 2:1 layers have a net negative charge and may be bonded together either by individual cations (such as potassium in illite or sodium or calcium in smectites) or by positively charged octahedral sheets (as in chlorites). Clay minerals include the following groups: Kaolin group which includes the minerals kaolinite, dickite, halloysite, and nacrite (polymorphs of Al2Si2O5(OH)4). Some sources include the kaolinite-serpentine group due to structural similarities. Smectite group which includes dioctahedral smectites, such as montmorillonite, nontronite and beidellite, and trioctahedral smectites, such as saponite. In 2013, analytical tests by the Curiosity rover found results consistent with the presence of smectite clay minerals on the planet Mars. Illite group which includes the clay-micas. Illite is the only common mineral in this group. Chlorite group includes a wide variety of similar minerals with considerable chemical variation. Other 2:1 clay types exist such as palygorskite (also known as attapulgite) and sepiolite, clays with long water channels internal to their structure. Mixed layer clay variations exist for most of the above groups. Ordering is described as a random or regular order and is further described by the term reichweite, which is German for range or reach. Literature articles will refer to an R1 ordered illite-smectite, for example. This type would be ordered in an illite-smectite-illite-smectite (ISIS) fashion. R0 on the other hand describes random ordering, and other advanced ordering types are also found (R3, etc.). Mixed layer clay minerals which are perfect R1 types often get their own names. R1 ordered chlorite-smectite is known as corrensite, R1 illite-smectite is rectorite. X-ray rf(001) is the spacing between layers in nanometers, as determined by X-ray crystallography. Glycol (mg/g) is the adsorption capacity for glycol, which occupies the interlayer sites when the clay is exposed to a vapor of ethylene glycol at 60 °C (140 °F) for eight hours. CEC is the cation exchange capacity of the clay. K2O (%) is the percent content of potassium oxide in the clay. DTA describes the differential thermal analysis curve of the clay. Clay and the origins of life The clay hypothesis for the origin of life was proposed by Graham Cairns-Smith in 1985. It postulates that complex organic molecules arose gradually on pre-existing, non-organic replication surfaces of silicate crystals in contact with an aqueous solution. The clay mineral montmorillonite has been shown to catalyze the polymerization of RNA in aqueous solution from nucleotide monomers, and the formation of membranes from lipids. In 1998, Hyman Hartman proposed that \"the first organisms were self-replicating iron-rich clays which fixed carbon dioxide into oxalic acid and other dicarboxylic acids. This system of replicating clays and their metabolic phenotype then evolved into the sulfide rich region of the hot spring acquiring the ability to fix nitrogen. Finally phosphate was incorporated into the evolving system which allowed the synthesis of nucleotides and phospholipids.\" Biomedical applications of clays The structural and compositional versatility of clay minerals gives them interesting biological properties. Due to disc-shaped and charged surfaces, clay interacts with a range of drugs, protein, polymers, DNA, or other macromolecules. Some of the applications of clays include drug delivery, tissue engineering, and bioprinting. Mortar applications Clay minerals can be incorporated in lime-metakaolin mortars to improve mechanical properties. Electrochemical separation helps to obtain modified saponite-containing products with high smectite-group minerals concentrations, lower mineral particles size, more compact structure, and greater surface area. These characteristics open possibilities for the manufacture of high-quality ceramics and heavy-metal sorbents from saponite-containing products. Furthermore, tail grinding occurs during the preparation of the raw material for ceramics; this waste reprocessing is of high importance for the use of clay pulp as a neutralizing agent, as fine particles are required for the reaction. Experiments on the histosol deacidification with the alkaline clay slurry demonstrated that neutralization with the average pH level of 7.1 is reached at 30% of the pulp added and an experimental site with perennial grasses proved the e", "canonical_url": "https://en.wikipedia.org/wiki/Clay_mineral", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:08.127931", "metadata": {"word_count": 70, "text_length": 12879}}
{"id": "wiki_iron_oxides", "query_word": "iron oxides", "title": "Iron oxide", "summary": "An iron oxide is a chemical compound composed of iron and oxygen. Several iron oxides are recognized. Often they are non-stoichiometric. Ferric oxyhydroxides are a related class of compounds, perhaps the best known of which is rust. Iron oxides and oxyhydroxides are widespread in nature and play an important role in many geological and biological processes. They are used as iron ores, pigments, catalysts, and in thermite, and occur in hemoglobin. Iron oxides are inexpensive and durable pigments in paints, coatings and colored concretes. Colors commonly available are in the \"earthy\" end of the yellow/orange/red/brown/black range. When used as a food coloring, it has E number E172. The earliest applications of paint served purely ornamental purposes. Consequently, pigment lacking any adhesive agent—composed mainly of iron oxide was employed in prehistoric cave art around the 15,000s BC in parts of Asia.", "text": "An iron oxide is a chemical compound composed of iron and oxygen. Several iron oxides are recognized. Often they are non-stoichiometric. Ferric oxyhydroxides are a related class of compounds, perhaps the best known of which is rust. Iron oxides and oxyhydroxides are widespread in nature and play an important role in many geological and biological processes. They are used as iron ores, pigments, catalysts, and in thermite, and occur in hemoglobin. Iron oxides are inexpensive and durable pigments in paints, coatings and colored concretes. Colors commonly available are in the \"earthy\" end of the yellow/orange/red/brown/black range. When used as a food coloring, it has E number E172. The earliest applications of paint served purely ornamental purposes. Consequently, pigment lacking any adhesive agent—composed mainly of iron oxide was employed in prehistoric cave art around the 15,000s BC in parts of Asia. Stoichiometries Iron oxides feature as ferrous (Fe(II)) or ferric (Fe(III)) or both. They adopt octahedral or tetrahedral coordination geometry. Only a few oxides are significant at the earth's surface, particularly wüstite, magnetite, and hematite. Oxides of FeII FeO: iron(II) oxide, wüstite Mixed oxides of FeII and FeIII Fe3O4: Iron(II,III) oxide, magnetite Fe4O5 Fe5O6 Fe5O7 Fe25O32 Fe13O19 Oxides of FeIII Fe2O3: iron(III) oxide α-Fe2O3: alpha phase, hematite β-Fe2O3: beta phase γ-Fe2O3: gamma phase, maghemite ε-Fe2O3: epsilon phase Thermal expansion Oxide-hydroxides goethite (α-FeOOH) akaganéite (β-FeOOH) lepidocrocite (γ-FeOOH) feroxyhyte (δ-FeOOH) ferrihydrite (Fe5HO8 · 4 H2O approx., or 5 Fe2O3 · 9 H2O, better recast as FeOOH · 0.4 H2O) high-pressure pyrite-structured FeOOH. Once dehydration is triggered, this phase may form FeO2Hx (0 < x < 1). green rust (FeIIIxFeIIyOH3x + y − z (A−)z where A− is Cl− or 0.5 SO2−4) Reactions In blast furnaces and related factories, iron oxides are converted to the metal. Typical reducing agents are various forms of carbon. A representative reaction starts with ferric oxide: 2 Fe2O3 + 3 C → 4 Fe + 3 CO2 In nature Iron is stored in many organisms in the form of ferritin, which is a ferrous oxide encased in a solubilizing protein sheath. Species of bacteria, including Shewanella oneidensis, Geobacter sulfurreducens and Geobacter metallireducens, use iron oxides as terminal electron acceptors. Uses Almost all iron ores are oxides, so in that sense these materials are important precursors to iron metal and its many alloys. Iron oxides are important pigments, coming in a variety of colors (black, red, yellow). Among their many advantages, they are inexpensive, strongly colored, and nontoxic. Magnetite is a component of magnetic recording tapes. See also Great Oxidation Event Iron cycle Iron oxide nanoparticle Limonite List of inorganic pigments Iron(II) hydroxide References External links Information from 4thNano-Oxides, Inc. on Fe2O3. The Iron One-Pot Reaction Iron Oxide Pigments Statistics and Information CDC – NIOSH Pocket Guide to Chemical Hazards", "canonical_url": "https://en.wikipedia.org/wiki/Iron_oxide", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:08.879348", "metadata": {"word_count": 140, "text_length": 3036}}
{"id": "wiki_carbonate_minerals", "query_word": "carbonate minerals", "title": "Carbonate mineral", "summary": "Carbonate minerals are those minerals containing the carbonate ion, CO2−3.", "text": "Carbonate minerals are those minerals containing the carbonate ion, CO2−3. Carbonate divisions Anhydrous carbonates Calcite group: trigonal Calcite CaCO3 Gaspéite (Ni,Mg,Fe2+)CO3 Magnesite MgCO3 Otavite CdCO3 Rhodochrosite MnCO3 Siderite FeCO3 Smithsonite ZnCO3 Spherocobaltite CoCO3 Aragonite group: orthorhombic Aragonite CaCO3 Cerussite PbCO3 Strontianite SrCO3 Witherite BaCO3 Rutherfordine UO2CO3 Natrite Na2CO3 Anhydrous carbonates with compound formulas Dolomite group: trigonal Ankerite CaFe(CO3)2 Dolomite CaMg(CO3)2 Huntite Mg3Ca(CO3)4 Minrecordite CaZn(CO3)2 Barytocalcite BaCa(CO3)2 Carbonates with hydroxyl or halogen Carbonate with hydroxide: monoclinic Azurite Cu3(CO3)2(OH)2 Hydrocerussite Pb3(CO3)2(OH)2 Malachite Cu2CO3(OH)2 Rosasite (Cu,Zn)2CO3(OH)2 Phosgenite Pb2(CO3)Cl2 Hydrozincite Zn5(CO3)2(OH)6 Aurichalcite (Zn,Cu)5(CO3)2(OH)6 Hydrated carbonates Hydromagnesite Mg5(CO3)4(OH)2.4H2O Ikaite CaCO3·6(H2O) Lansfordite MgCO3·5(H2O) Monohydrocalcite CaCO3·H2O Natron Na2CO3·10(H2O) Zellerite Ca(UO2)(CO3)2·5(H2O) The carbonate class in both the Dana and the Strunz classification systems include the nitrates. Nickel–Strunz classification -05- carbonates IMA-CNMNC proposes a new hierarchical scheme (Mills et al., 2009). This list uses the classification of Nickel–Strunz (mindat.org, 10 ed, pending publication). Abbreviations: \"*\" – discredited (IMA/CNMNC status). \"?\" – questionable/doubtful (IMA/CNMNC status). \"REE\" – Rare-earth element (Sc, Y, La, Ce, Pr, Nd, Pm, Sm, Eu, Gd, Tb, Dy, Ho, Er, Tm, Yb, Lu) \"PGE\" – Platinum-group element (Ru, Rh, Pd, Os, Ir, Pt) 03.C Aluminofluorides, 06 Borates, 08 Vanadates (04.H V Vanadates), 09 Silicates: Neso: insular (from Greek νησος nēsos, island) Soro: grouping (from Greek σωροῦ sōros, heap, mound (especially of corn)) Cyclo: ring Ino: chain (from Greek ις , fibre) Phyllo: sheet (from Greek φύλλον phyllon, leaf) Tekto: three-dimensional framework Nickel–Strunz code scheme: NN.XY.##x NN: Nickel–Strunz mineral class number X: Nickel–Strunz mineral division letter Y: Nickel–Strunz mineral family letter ##x: Nickel–Strunz mineral/group number, x add-on letter Class: carbonates 05.A Carbonates without additional anions, without H2O 05.AA Alkali carbonates: 05 Zabuyelite; 10 Gregoryite, 10 Natrite; 15 Nahcolite, 20 Kalicinite, 25 Teschemacherite, 30 Wegscheiderite 05.AB Alkali-earth (and other M2+) carbonates: 05 Calcite, 05 Gaspeite, 05 Magnesite, 05 Rhodochrosite, 05 Otavite, 05 Spherocobaltite, 05 Siderite, 05 Smithsonite; 10 Ankerite, 10 Dolomite, 10 Kutnohorite, 10 Minrecordite; 15 Cerussite, 15 Aragonite, 15 Strontianite, 15 Witherite; 20 Vaterite, 25 Huntite, 30 Norsethite, 35 Alstonite; 40 Olekminskite, 40 Paralstonite; 45 Barytocalcite, 50 Carbocernaite, 55 Benstonite, 60 Juangodoyite 05.AC Alkali and alkali-earth carbonates: 05 Eitelite, 10 Nyerereite, 10 Natrofairchildite, 10 Zemkorite; 15 Butschliite, 20 Fairchildite, 25 Shortite; 30 Sanromanite, 30 Burbankite, 30 Calcioburbankite, 30 Khanneshite 05.AD With rare-earth elements (REE): 05 Sahamalite-(Ce); 15 Rémondite-(Ce), 15 Petersenite-(Ce), 15 Rémondite-(La); 20 Paratooite-(La) 05.B Carbonates with additional anions, without H2O 05.BA With Cu, Co, Ni, Zn, Mg, Mn: 05 Azurite, 10 Chukanovite, 10 Malachite, 10 Georgeite, 10 Pokrovskite, 10 Nullaginite, 10 Glaukosphaerite, 10 Mcguinnessite, 10 Kolwezite, 10 Rosasite, 10 Zincrosasite; 15 Aurichalcite, 15 Hydrozincite; 20 Holdawayite, 25 Defernite; 30 Loseyite, 30 Sclarite 05.BB With alkalies, etc.: 05 Barentsite, 10 Dawsonite, 15 Tunisite, 20 Sabinaite 05.BC With alkali-earth cations: 05 Brenkite, 10 Rouvilleite, 15 Podlesnoite 05.BD With rare-earth elements (REE): 05 Cordylite-(Ce), 05 Lukechangite-(Ce); 10 Kukharenkoite-(La), 10 Kukharenkoite-(Ce), 10 Zhonghuacerite-(Ce); 15 Cebaite-(Nd), 15 Cebaite-(Ce); 20a Bastnasite-(Ce), 20a Bastnasite-(La), 20a Bastnasite-(Y), 20a Hydroxylbastnasite-(Ce), 20a Hydroxylbastnasite-(La), 20a Hydroxylbastnasite-(Nd), 20a Thorbastnasite, 20b Parisite-(Nd), 20b Parisite-(Ce), 20c Synchysite-(Ce), 20c Synchysite-(Nd), 20c Synchysite-(Y), 20d Rontgenite-(Ce); 25 Horvathite-(Y), 30 Qaqarssukite-(Ce), 35 Huanghoite-(Ce) 05.BE With Pb, Bi: 05 Shannonite, 10 Hydrocerussite, 15 Plumbonacrite, 20 Phosgenite, 25 Bismutite, 30 Kettnerite, 35 Beyerite 05.BF With (Cl), SO4, PO4, TeO3: 05 Northupite, 05 Ferrotychite, 05 Manganotychite, 05 Tychite; 10 Bonshtedtite, 10 Crawfordite, 10 Bradleyitev, 10 Sidorenkite, 15 Daqingshanite-(Ce), 20 Reederite-(Y), 25 Mineevite-(Y), 30 Brianyoungite, 35 Philolithite; 40 Macphersonitev, 40 Susannite, 40 Leadhillite 05.C Carbonates without additional anions, with H2O 05.CA With medium-sized cations: 05 Nesquehonite, 10 Lansfordite, 15 Barringtonite, 20 Hellyerite 05.CB With large cations (alkali and alkali-earth carbonates): 05 Thermonatrite, 10 Natron, 15 Trona, 20 Monohydrocalcite, 25 Ikaite, 30 Pirssonite, 35 Gaylussite, 40 Chalconatronite, 45 Baylissite, 50 Tuliokite 05.CC With rare-earth elements (REE): 05 Donnayite-(Y), 05 Mckelveyite-(Nd)*, 05 Mckelveyite-(Y), 05 Weloganite; 10 Tengerite-(Y), 15 Lokkaite-(Y); 20 Shomiokite-(Y), 20 IMA2008-069; 25 Calkinsite-(Ce), 25 Lanthanite-(Ce), 25 Lanthanite-(La), 25 Lanthanite-(Nd); 30 Adamsite-(Y), 35 Decrespignyite-(Y), 40 Galgenbergite-(Ce), 45 Ewaldite, 50 Kimuraite-(Y) 05.D Carbonates with additional anions, with H2O 05.DA With medium-sized cations: 05 Dypingite, 05 Giorgiosite, 05 Hydromagnesite, 05 Widgiemoolthalite; 10 Artinite, 10 Chlorartinite; 15 Otwayite, 20 Kambaldaite, 25 Callaghanite, 30 Claraite; 35 Hydroscarbroite, 35 Scarbroite; 40 Charmarite-3T, 40 Charmarite-2H, 40 Caresite, 40 Quintinite-2H, 40 Quintinite-3T; 45 Brugnatellite, 45 Barbertonite, 45 Chlormagaluminite, 45 Zaccagnaite, 45 Manasseite, 45 Sjogrenite; 50 Desautelsite, 50 Comblainite, 50 Hydrotalcite, 50 Pyroaurite, 50 Reevesite, 50 Stichtite, 50 Takovite; 55 Coalingite, 60 Karchevskyite, 65 Indigirite, 70 Zaratite 05.DB With large and medium-sized cations: 05 Alumohydrocalcite, 05 Para-alumohydrocalcite, 05 Nasledovite; 10 Dresserite, 10 Dundasite, 10 Strontiodresserite, 10 Petterdite, 10 Kochsandorite; 15 Hydrodresserite, 20 Schuilingite-(Nd), 25 Sergeevite, 30 Szymanskiite, 35 Montroyalite 05.DC With large cations: 05 Ancylite-(Ce), 05 Ancylite-(La), 05 Gysinite-(Nd), 05 Calcioancylite-(Ce), 05 Calcioancylite-(Nd), 05 Kozoite-(La), 05 Kozoite-(Nd); 10 Kamphaugite-(Y), 15 Sheldrickite, 20 Thomasclarkite-(Y), 25 Peterbaylissite, 30 Clearcreekite, 35 Niveolanite 05.E Uranyl carbonates 05.EA UO2:CO3 > 1:1: 10 Urancalcarite, 15 Wyartite, 20 Oswaldpeetersite, 25 Roubaultite, 30 Kamotoite-(Y), 35 Sharpite 05.EB UO2:CO3 = 1:1: 05 Rutherfordine, 10 Blatonite, 15 Joliotite, 20 Bijvoetite-(Y) 05.EC UO2:CO3 < 1:1 - 1:2: 05 Fontanite; 10 Metazellerite, 10 Zellerite 05.ED UO2:CO3 = 1:3: 05 Bayleyite, 10 Swartzite, 15 Albrechtschraufite, 20 Liebigite, 25 Rabbittite, 30 Andersonite, 35 Grimselite, 40 Widenmannite, 45 Znucalite, 50 Cejkaite 05.EE UO2:CO3 = 1:4: 05 Voglite, 10 Shabaite-(Nd) 05.EF UO2:CO3 = 1:5: 05 Astrocyanite-(Ce) 05.EG With SO4 or SiO4: 05 Schrockingerite, 10 Lepersonnite-(Gd) References External links Media related to Carbonates (minerals) at Wikimedia Commons", "canonical_url": "https://en.wikipedia.org/wiki/Carbonate_mineral", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:09.571439", "metadata": {"word_count": 10, "text_length": 7225}}
{"id": "wiki_geological_mapping", "query_word": "geological mapping", "title": "Geological map", "summary": "A geological map or geologic map is a special-purpose map made to show various geological features. Rock units or geologic strata are shown by color or symbols. Bedding planes and structural features such as faults, folds, are shown with strike and dip or trend and plunge symbols which give three-dimensional orientations features. Geological mapping is an interpretive process involving multiple types of information, from analytical data to personal observation, all synthesized and recorded by the geologist. Geologic observations have traditionally been recorded on paper, whether on standardized note cards, in a notebook, or on a map. Stratigraphic contour lines may be used to illustrate the surface of a selected stratum illustrating the subsurface topographic trends of the strata. Isopach maps detail the variations in thickness of stratigraphic units. It is not always possible to properly show this when the strata are extremely fractured, mixed, in some discontinuities, or where they are otherwise disturbed. Digital geological mapping is the process by which geological features are observed, analyzed, and recorded in the field and displayed in real-time on a computer or personal digital assistant (PDA). The primary function of this technology is to produce spatially referenced geological maps that can be utilized and updated while conducting field work.", "text": "A geological map or geologic map is a special-purpose map made to show various geological features. Rock units or geologic strata are shown by color or symbols. Bedding planes and structural features such as faults, folds, are shown with strike and dip or trend and plunge symbols which give three-dimensional orientations features. Geological mapping is an interpretive process involving multiple types of information, from analytical data to personal observation, all synthesized and recorded by the geologist. Geologic observations have traditionally been recorded on paper, whether on standardized note cards, in a notebook, or on a map. Stratigraphic contour lines may be used to illustrate the surface of a selected stratum illustrating the subsurface topographic trends of the strata. Isopach maps detail the variations in thickness of stratigraphic units. It is not always possible to properly show this when the strata are extremely fractured, mixed, in some discontinuities, or where they are otherwise disturbed. Digital geological mapping is the process by which geological features are observed, analyzed, and recorded in the field and displayed in real-time on a computer or personal digital assistant (PDA). The primary function of this technology is to produce spatially referenced geological maps that can be utilized and updated while conducting field work. Symbols Lithologies Rock units are typically represented by colors. Instead of (or in addition to) colors, certain symbols can be used. Different geological mapping agencies and authorities have different standards for the colors and symbols to be used for rocks of differing types and ages. Orientations Geologists take two major types of orientation measurements (using a hand compass, for example a Brunton compass): orientations of planes and orientations of lines. Orientations of planes are measured as a \"strike\" and \"dip\", while orientations of lines are measured as a \"trend\" and \"plunge\". Strike and dip symbols consist of a long \"strike\" line, which is perpendicular to the direction of greatest slope along the surface of the bed, and a shorter \"dip\" line on side of the strike line where the bed is going downwards. The angle that the bed makes with the horizontal, along the dip direction, is written next to the dip line. In the azimuthal system, strike and dip are often given as \"strike/dip\" (for example: 270/15, for a strike of west and a dip of 15 degrees below the horizontal). Trend and plunge are used for linear features, and their symbol is a single arrow on the map. The arrow is oriented in the downgoing direction of the linear feature (the \"trend\") and at the end of the arrow, the number of degrees that the feature lies below the horizontal (the \"plunge\") is noted. Trend and plunge are often notated as PLUNGE → TREND (for example: 34 → 86 indicates a feature that is angled at 34 degrees below the horizontal at an angle that is just east of true south). History The oldest preserved geological map is the Turin papyrus (1150 BCE), which shows the location of building stone and gold deposits in Egypt. The earliest geological map of the modern era is the 1771 \"Map of Part of Auvergne, or figures of, The Current of Lava in which Prisms, Balls, Etc. are Made from Basalt. To be used with Mr. Demarest's theories of this hard basalt. Engraved by Messr. Pasumot and Daily, Geological Engineers of the King.\" This map is based on Nicolas Desmarest's 1768 detailed study of the geology and eruptive history of the Auvergne volcanoes and a comparison with the columns of the Giant's Causeway of Ireland. He identified both landmarks as features of extinct volcanoes. The 1768 report was incorporated in the 1771 (French) Royal Academy of Science compendium. The first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure undertook the self-imposed task of making a geological survey of the United States. He traversed and mapped nearly every state in the Union. During the rigorous two-year period of his survey, he crossed and recrossed the Allegheny Mountains some 50 times. Maclure's map shows the distribution of five classes of rock in what are now only the eastern states of the present-day US. The first geological map of Great Britain was created by William Smith in 1815 using principles (Smith's laws) first formulated by Smith. Software history Mapping in the digital era In the 21st century, computer technology and software are becoming portable and powerful enough to take on some of the more mundane tasks a geologist must perform in the field, such as precisely locating oneself with a GPS unit, displaying multiple images (maps, satellite images, aerial photography, etc.), plotting strike and dip symbols, and color-coding different physical characteristics of a lithology or contact type (e.g., unconformity) between rock strata. Additionally, computers can now perform some tasks that were difficult to accomplish in the field, for example, handwriting or voice recognition and annotating photographs on the spot. Digital mapping has positive and negative effects on the mapping process; only an assessment of its impact on a geological mapping project as a whole shows whether it provides a net benefit. With the use of computers in the field, the recording of observations and basic data management changes dramatically. The use of digital mapping also affects when data analysis occurs in the mapping process, but does not greatly affect the process itself. Advantages Data entered by a geologist may have fewer errors than data transcribed by a data entry clerk. Data entry by geologists in the field may take less total time than subsequent data entry in the office, potentially reducing the overall time needed to complete a project. The spatial extent of real world objects and their attributes can be entered directly into a database with geographic information system (GIS) capability. Features can be automatically color-coded and symbolized based on set criteria. Multiple maps and imagery (geophysical maps, satellite images, orthophotos, etc.) can easily be carried and displayed on-screen. Geologists may upload each other's data files for the next day's field work as reference. Data analysis may start immediately after returning from the field, since the database has already been populated. Data can be constrained by dictionaries and dropdown menus to ensure that data are recorded systematically and that mandatory data are not forgotten Labour-saving tools and functionality can be provided in the field e.g. structure contours on the fly, and 3D visualisation Systems can be wirelessly connected to other digital field equipment (such as digital cameras and sensor webs) Disadvantages Computers and related items (extra batteries, stylus, cameras, etc.) must be carried in the field. Field data entry into the computer may take longer than physically writing on paper, possibly resulting in longer field programs. Data entered by multiple geologists may contain more inconsistencies than data entered by one person, making the database more difficult to query. Written descriptions convey to the reader detailed information through imagery that may not be communicated by the same data in parsed format. Geologists may be inclined to shorten text descriptions because they are difficult to enter (either by handwriting or voice recognition), resulting in loss of data. There are no original, hardcopy field maps or notes to archive. Paper is a more stable medium than digital format. Educational and scientific uses Some universities and secondary educators are integrating digital geological mapping into class work. For example, The GeoPad project describes the combination of technology, teaching field geology, and geological mapping in programs such as Bowling Green State University’s geology field camp. At Urbino University (Italy) it:Università di Urbino, Field Digital Mapping Techniques are integrated in Earth and Environmental Sciences courses since 2006 . The MapTeach program is designed to provide hands-on digital mapping for middle and high school students. Archived 2009-06-25 at the Wayback Machine The SPLINT project in the UK is using the BGS field mapping system as part of their teaching curriculum Digital mapping technology can be applied to traditional geological mapping, reconnaissance mapping, and surveying of geologic features. At international digital field data capture (DFDC) meetings, major geological surveys (e.g., British Geological Survey and Geological Survey of Canada) discuss how to harness and develop the technology. Many other geological surveys and private companies are also designing systems to conduct scientific and applied geological mapping of, for example, geothermal springs and mine sites. Equipment The initial cost of digital geologic computing and supporting equipment may be significant. In addition, equipment and software must be replaced occasionally due to damage, loss, and obsolescence. Products moving through the market are quickly discontinued as technology and consumer interests evolve. A product that works well for digital mapping may not be available for purchase the following year; however, testing multiple brands and generations of equipment and software is prohibitively expensive. Common essential features Some features of digital mapping equipment are common to both survey or reconnaissance mapping and “traditional” comprehensive mapping. The capture of less data-intensive reconnaissance mapping or survey data in the field can be accomplished by less robust databases and GIS programs, and hardware with a smaller screen size. Devices and software are intuitive to learn and easy to use Rugged, as typically defined by military standards (MIL-STD-810) and ingress protection ratings Waterproof Screen is easy to read in bright sunlight and on gray sky days Removable static memory cards can be used to back up data Memory on board is ", "canonical_url": "https://en.wikipedia.org/wiki/Geological_map", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:12.353418", "metadata": {"word_count": 206, "text_length": 17698}}
{"id": "wiki_precision_agriculture", "query_word": "precision agriculture", "title": "Precision agriculture", "summary": "Precision agriculture is a management strategy that gathers, processes and analyzes temporal, spatial and individual plant and animal data and combines it with other information to support management decisions according to estimated variability for improved resource use efficiency, productivity, quality, profitability and sustainability of agricultural production. It is used in both crop and livestock production. A central component of implementing this strategy is the satellite monitoring of agricultural machinery, which forms the basis for modern farm fleet management. This is achieved through the use of fleet telematics systems, where vehicles are equipped with a GPS tracking unit and an onboard controller that transmits telemetry data—such as location, speed, engine hours, and fuel consumption—to a central server for analysis. This stream of real-time data allows for the automation of agricultural operations and provides critical insights for improving diagnosis and decision-making. Beyond machinery, precision agriculture also analyzes the spatial variability within fields, such as how terrain attributes (geomorphology) and soil properties affect crop growth and water distribution (hydrology). The goal of precision agriculture research is to define a decision support system for whole farm management with the goal of optimizing returns on inputs while preserving resources. The practice of precision agriculture has been enabled by the advent of GPS and GNSS. The farmer's and/or researcher's ability to locate their precise position in a field allows for the creation of maps of the spatial variability of as many variables as can be measured (e.g. crop yield, terrain features/topography, organic matter content, moisture levels, nitrogen levels, pH, EC, Mg, K, and others). Similar data is collected by sensor arrays mounted on GPS-equipped combine harvesters. These arrays consist of real-time sensors that measure everything from chlorophyll levels to plant water status, along with multispectral imagery. This data is used in conjunction with satellite imagery by variable rate technology (VRT) including seeders, sprayers, etc. to optimally distribute resources. However, recent technological advances have enabled the use of real-time sensors directly in soil, which can wirelessly transmit data without the need of human presence. Precision agriculture can benefit from unmanned aerial vehicles, that are relatively inexpensive and can be operated by novice pilots. These agricultural drones can be equipped with multispectral or RGB cameras to capture many images of a field that can be stitched together using photogrammetric methods to create orthophotos. These multispectral images contain multiple values per pixel in addition to the traditional red, green blue values such as near infrared and red-edge spectrum values used to process and analyze vegetative indexes such as NDVI maps. These drones are capable of capturing imagery and providing additional geographical references such as elevation, which allows software to perform map algebra functions to build precise topography maps. These topographic maps can be used to correlate crop health with topography, the results of which can be used to optimize crop inputs such as water, fertilizer or chemicals such as herbicides and growth regulators through variable rate applications.", "text": "Precision agriculture is a management strategy that gathers, processes and analyzes temporal, spatial and individual plant and animal data and combines it with other information to support management decisions according to estimated variability for improved resource use efficiency, productivity, quality, profitability and sustainability of agricultural production. It is used in both crop and livestock production. A central component of implementing this strategy is the satellite monitoring of agricultural machinery, which forms the basis for modern farm fleet management. This is achieved through the use of fleet telematics systems, where vehicles are equipped with a GPS tracking unit and an onboard controller that transmits telemetry data—such as location, speed, engine hours, and fuel consumption—to a central server for analysis. This stream of real-time data allows for the automation of agricultural operations and provides critical insights for improving diagnosis and decision-making. Beyond machinery, precision agriculture also analyzes the spatial variability within fields, such as how terrain attributes (geomorphology) and soil properties affect crop growth and water distribution (hydrology). The goal of precision agriculture research is to define a decision support system for whole farm management with the goal of optimizing returns on inputs while preserving resources. The practice of precision agriculture has been enabled by the advent of GPS and GNSS. The farmer's and/or researcher's ability to locate their precise position in a field allows for the creation of maps of the spatial variability of as many variables as can be measured (e.g. crop yield, terrain features/topography, organic matter content, moisture levels, nitrogen levels, pH, EC, Mg, K, and others). Similar data is collected by sensor arrays mounted on GPS-equipped combine harvesters. These arrays consist of real-time sensors that measure everything from chlorophyll levels to plant water status, along with multispectral imagery. This data is used in conjunction with satellite imagery by variable rate technology (VRT) including seeders, sprayers, etc. to optimally distribute resources. However, recent technological advances have enabled the use of real-time sensors directly in soil, which can wirelessly transmit data without the need of human presence. Precision agriculture can benefit from unmanned aerial vehicles, that are relatively inexpensive and can be operated by novice pilots. These agricultural drones can be equipped with multispectral or RGB cameras to capture many images of a field that can be stitched together using photogrammetric methods to create orthophotos. These multispectral images contain multiple values per pixel in addition to the traditional red, green blue values such as near infrared and red-edge spectrum values used to process and analyze vegetative indexes such as NDVI maps. These drones are capable of capturing imagery and providing additional geographical references such as elevation, which allows software to perform map algebra functions to build precise topography maps. These topographic maps can be used to correlate crop health with topography, the results of which can be used to optimize crop inputs such as water, fertilizer or chemicals such as herbicides and growth regulators through variable rate applications. History Precision agriculture is a key component of the third wave of modern agricultural revolutions. The first agricultural revolution was the increase of mechanized agriculture, from 1900 to 1930. Each farmer produced enough food to feed about 26 people during this time. The 1960s prompted the Green Revolution with new methods of genetic modification, which led to each farmer feeding about 156 people. It is expected that by 2050, the global population will reach about 9.6 billion, and food production must effectively double from current levels in order to feed every mouth. With new technological advancements in the agricultural revolution of precision farming, each farmer will be able to feed 265 people on the same acreage. Overview The first wave of the precision agricultural revolution came in the forms of satellite and aerial imagery, weather prediction, variable rate fertilizer application, and crop health indicators. The second wave aggregates the machine data for even more precise planting, topographical mapping, and soil data. Precision agriculture aims to optimize field-level management with regard to: crop science: by matching farming practices more closely to crop needs (e.g. fertilizer inputs); environmental protection: by reducing environmental risks and footprint of farming (e.g. limiting leaching of nitrogen); economics: by boosting competitiveness through more efficient practices (e.g. improved management of fertilizer usage and other inputs). Precision agriculture also provides farmers with a wealth of information to: build up a record of their farm improve decision-making foster greater traceability enhance marketing of farm products improve lease arrangements and relationships with landlords enhance the inherent quality of farm products (e.g. protein level in bread-flour wheat) Prescriptive planting Prescriptive planting is a type of farming system that delivers data-driven planting advice that can determine variable planting rates to accommodate varying conditions across a single field, in order to maximize yield. It has been described as \"Big Data on the farm.\" Monsanto, DuPont and others are launching this technology in the US. Principles Precision agriculture uses many tools, but some of the basics include tractors, combines, sprayers, planters, and diggers, which are all considered auto-guidance systems. The small devices on the equipment that use GIS (geographic information system) are what makes precision agriculture what it is; the GIS system can be thought of as the “brain”. To be able to use precision agriculture, the equipment needs to be wired with the right technology and data systems. More tools include Variable rate technology (VRT), Global positioning system, Geographical information system, Grid sampling, and remote sensors. Geolocating Geolocating a field enables the farmer to overlay information gathered from the analysis of soils and residual nitrogen, and information on previous crops and soil resistivity. Geolocation is done in two ways The field is delineated using an in-vehicle GPS device as the farmer drives a tractor around the field. The field is delineated on a basemap derived from aerial or satellite imagery. The base images must have the right level of resolution and geometric quality to ensure that geolocation is sufficiently accurate. Variables Intra and inter-field variability may result from a number of factors. These include climatic conditions (hail, drought, rain, etc.), soils (texture, depth, nitrogen levels), cropping practices (no-till farming), weeds, and disease. Permanent indicators—chiefly soil indicators—provide farmers with information about the main environmental constants. Point indicators allow them to track a crop's status, i.e., to see whether diseases are developing, if the crop is suffering from water stress, nitrogen stress, or lodging, whether it has been damaged by ice, and so on. This information may come from weather stations and other sensors (soil electrical resistivity, detection with the naked eye, satellite imagery, etc.). Soil resistivity measurements combined with soil analysis make it possible to measure moisture content. Soil resistivity is also a relatively simple and cheap measurement. Strategies Using soil maps, farmers can pursue two strategies to adjust field inputs: Predictive approach: based on analysis of static indicators (soil, resistivity, field history, etc.) during the crop cycle. Control approach: information from static indicators is regularly updated during the crop cycle by: sampling: weighing biomass, measuring leaf chlorophyll content, weighing fruit, etc. remote sensing: measuring parameters like temperature (air/soil), humidity (air/soil/leaf), wind or stem diameter is possible thanks to Wireless Sensor Networks and Internet of things (IoT) proxy-detection: in-vehicle sensors measure leaf status; this requires the farmer to drive around the entire field. aerial or satellite remote sensing: multispectral imagery is acquired and processed to derive maps of crop biophysical parameters, including indicators of disease. Airborne instruments are able to measure the amount of plant cover and to distinguish between crops and weeds. Decisions may be based on decision-support models (crop simulation models and recommendation models) based on big data, but in the final analysis it is up to the farmer to decide in terms of business value and impacts on the environment- a role being taken over by artificial intelligence (AI) systems based on machine learning and artificial neural networks. It is important to realize why PA technology is or is not adopted, \"for PA technology adoption to occur the farmer has to perceive the technology as useful and easy to use. It might be insufficient to have positive outside data on the economic benefits of PA technology as perceptions of farmers have to reflect these economic considerations.\" Implementing practices New information and communication technologies make field-level crop management more operational and easier to achieve for farmers. Application of crop management decisions calls for agricultural equipment that supports variable-rate technology (VRT), for example varying seed density along with the variable-rate application (VRA) of nitrogen and phytosanitary products. Precision agriculture uses technology on agricultural equipment (e.g. tractors, sprayers, harvesters, etc.): positioning system (e.g. GPS trackers that use satellite signals to precisely determine a position on the globe); geographic information systems (GIS), i.e., software that makes sense o", "canonical_url": "https://en.wikipedia.org/wiki/Precision_agriculture", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:15.477722", "metadata": {"word_count": 488, "text_length": 22233}}
{"id": "wiki_environmental_monitoring", "query_word": "environmental monitoring", "title": "Environmental monitoring", "summary": "Environmental monitoring is the scope of processes and activities that are done to characterize and describe the state of the environment. It is used in the preparation of environmental impact assessments, and in many circumstances in which human activities may cause harmful effects on the natural environment. Monitoring strategies and programmes are generally designed to establish the current status of an environment or to establish a baseline and trends in environmental parameters. The results of monitoring are usually reviewed, analyzed statistically, and published. A monitoring programme is designed around the intended use of the data before monitoring starts. Environmental monitoring includes monitoring of air quality, soils and water quality. Many monitoring programmes are designed to not only establish the current state of the environment but also predict future conditions. In some cases this may involve collecting data related to events in the distant past such as gasses trapped in ancient glacier ice.", "text": "Environmental monitoring is the scope of processes and activities that are done to characterize and describe the state of the environment. It is used in the preparation of environmental impact assessments, and in many circumstances in which human activities may cause harmful effects on the natural environment. Monitoring strategies and programmes are generally designed to establish the current status of an environment or to establish a baseline and trends in environmental parameters. The results of monitoring are usually reviewed, analyzed statistically, and published. A monitoring programme is designed around the intended use of the data before monitoring starts. Environmental monitoring includes monitoring of air quality, soils and water quality. Many monitoring programmes are designed to not only establish the current state of the environment but also predict future conditions. In some cases this may involve collecting data related to events in the distant past such as gasses trapped in ancient glacier ice. Air quality monitoring Air pollutants are atmospheric substances—both naturally occurring and anthropogenic—which may potentially have a negative impact on the environment and organism health. With the evolution of new chemicals and industrial processes has come the introduction or elevation of pollutants in the atmosphere, as well as environmental research and regulations, increasing the demand for air quality monitoring. Air quality monitoring is challenging to enact as it requires the effective integration of multiple environmental data sources, which often originate from different environmental networks and institutions. These challenges require specialized observation equipment and tools to establish air pollutant concentrations, including sensor networks, geographic information system (GIS) models, and the Sensor Observation Service (SOS), a web service for querying real-time sensor data. Air dispersion models that combine topographic, emissions, and meteorological data to predict air pollutant concentrations are often helpful in interpreting air monitoring data. Additionally, consideration of anemometer data in the area between sources and the monitor often provides insights on the source of the air contaminants recorded by an air pollution monitor. Air quality monitors are operated by citizens, regulatory agencies, non-governmental organisations and researchers to investigate air quality and the effects of air pollution. Interpretation of ambient air monitoring data often involves a consideration of the spatial and temporal representativeness of the data gathered, and the health effects associated with exposure to the monitored levels. If the interpretation reveals concentrations of multiple chemical compounds, a unique \"chemical fingerprint\" of a particular air pollution source may emerge from analysis of the data. Air sampling Passive or \"diffusive\" air sampling depends on meteorological conditions such as wind to diffuse air pollutants to a sorbent medium. Passive samplers, such as diffusion tubes, have the advantage of typically being small, quiet, and easy to deploy, and they are particularly useful in air quality studies that determine key areas for future continuous monitoring. Air pollution can also be assessed by biomonitoring with organisms that bioaccumulate air pollutants, such as lichens, mosses, fungi, and other biomass. One of the benefits of this type of sampling is how quantitative information can be obtained via measurements of accumulated compounds, representative of the environment from which they came. However, careful considerations must be made in choosing the particular organism, how it's dispersed, and relevance to the pollutant. Other sampling methods include the use of a denuder, needle trap devices, and microextraction techniques. Soil monitoring Soil monitoring involves the collection and/or analysis of soil and its associated quality, constituents, and physical status to determine or guarantee its fitness for use. Soil faces many threats, including compaction, contamination, organic material loss, biodiversity loss, slope stability issues, erosion, salinization, and acidification. Soil monitoring helps characterize these threats and other potential risks to the soil, surrounding environments, animal health, and human health. Assessing these threats and other risks to soil can be challenging due to a variety of factors, including soil's heterogeneity and complexity, scarcity of toxicity data, lack of understanding of a contaminant's fate, and variability in levels of soil screening. This requires a risk assessment approach and analysis techniques that prioritize environmental protection, risk reduction, and, if necessary, remediation methods. Soil monitoring plays a significant role in that risk assessment, not only aiding in the identification of at-risk and affected areas but also in the establishment of base background values of soil. Soil monitoring has historically focused on more classical conditions and contaminants, including toxic elements (e.g., mercury, lead, and arsenic) and persistent organic pollutants (POPs). Historically, testing for these and other aspects of soil, however, has had its own set of challenges, as sampling in most cases is of a destructive in nature, requiring multiple samples over time. Additionally, procedural and analytical errors may be introduced due to variability among references and methods, particularly over time. However, as analytical techniques evolve and new knowledge about ecological processes and contaminant effects disseminate, the focus of monitoring will likely broaden over time and the quality of monitoring will continue to improve. Soil sampling The two primary types of soil sampling are grab sampling and composite sampling. Grab sampling involves the collection of an individual sample at a specific time and place, while composite sampling involves the collection of a homogenized mixture of multiple individual samples at either a specific place over different times or multiple locations at a specific time. Soil sampling may occur both at shallow ground levels or deep in the ground, with collection methods varying by level collected from. Scoops, augers, core barrel, and solid-tube samplers, and other tools are used at shallow ground levels, whereas split-tube, solid-tube, or hydraulic methods may be used in deep ground. Monitoring programmes Soil contamination monitoring Soil contamination monitoring helps researchers identify patterns and trends in contaminant deposition, movement, and effect. Human-based pressures such as tourism, industrial activity, urban sprawl, construction work, and inadequate agriculture/forestry practices can contribute to and make worse soil contamination and lead to the soil becoming unfit for its intended use. Both inorganic and organic pollutants may make their way to the soil, having a wide variety of detrimental effects. Soil contamination monitoring is therefore important to identify risk areas, set baselines, and identify contaminated zones for remediation. Monitoring efforts may range from local farms to nationwide efforts, such as those made by China in the late 2000s, providing details such as the nature of contaminants, their quantity, effects, concentration patterns, and remediation feasibility. Monitoring and analytical equipment will ideally will have high response times, high levels of resolution and automation, and a certain degree of self-sufficiency. Chemical techniques may be used to measure toxic elements and POPs using chromatography and spectrometry, geophysical techniques may assess physical properties of large terrains, and biological techniques may use specific organisms to gauge not only contaminant level but also byproducts of contaminant biodegradation. These techniques and others are increasingly becoming more efficient, and laboratory instrumentation is becoming more precise, resulting in more meaningful monitoring outcomes. Soil erosion monitoring Soil erosion monitoring helps researchers identify patterns and trends in soil and sediment movement. Monitoring programmes have varied over the years, from long-term academic research on university plots to reconnaissance-based surveys of biogeoclimatic areas. In most methods, however, the general focus is on identifying and measuring all the dominant erosion processes in a given area. Additionally, soil erosion monitoring may attempt to quantify the effects of erosion on crop productivity, though challenging \"because of the many complexities in the relationship between soils and plants and their management under a variable climate.\" Soil salinity monitoring Soil salinity monitoring helps researchers identify patterns and trends in soil salt content. Both the natural process of seawater intrusion and the human-induced processes of inappropriate soil and water management can lead to salinity problems in soil, with up to one billion hectares of land affected globally (as of 2013). Salinity monitoring at the local level may look closely at the root zone to gauge salinity impact and develop management options, whereas at the regional and national level salinity monitoring may help with identifying areas at-risk and aiding policymakers in tackling the issue before it spreads. The monitoring process itself may be performed using technologies such as remote sensing and geographic information systems (GIS) to identify salinity via greenness, brightness, and whiteness at the surface level. Direct analysis of soil up close, including the use of electromagnetic induction techniques, may also be used to monitor soil salinity. Water quality monitoring The water environment on Earth includes the seas and oceans, the rivers lakes, marshes, streams and the frozen waters in ice-caps and glaciers. Because of the importance of water to life, monitoring of water wherever it occurs is critical to the environmental well-being of the eart", "canonical_url": "https://en.wikipedia.org/wiki/Environmental_monitoring", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:16.368438", "metadata": {"word_count": 152, "text_length": 36779}}
{"id": "wiki_forest_monitoring", "query_word": "forest monitoring", "title": "Independent forest monitoring", "summary": "Independent forest monitoring (IFM) is a tool for assessing and strengthening legal compliance in the forest sector internationally. By complementing official forest law enforcement activities with the objectivity and public credibility of an independent third party, IFM can improve transparency in the short term while contributing to the development of a sound legislative and regulatory framework for responsible forest management. Transparency was emphasized as a key element in Reducing Emissions from Deforestation and forest Degradation (REDD) by the UN Framework Convention on Climate Change (UNFCCC) at the Conference of the Parties (COP-15) in Copenhagen in December 2009 in its decisions on methodological guidance for forest monitoring systems.", "text": "Independent forest monitoring (IFM) is a tool for assessing and strengthening legal compliance in the forest sector internationally. By complementing official forest law enforcement activities with the objectivity and public credibility of an independent third party, IFM can improve transparency in the short term while contributing to the development of a sound legislative and regulatory framework for responsible forest management. Transparency was emphasized as a key element in Reducing Emissions from Deforestation and forest Degradation (REDD) by the UN Framework Convention on Climate Change (UNFCCC) at the Conference of the Parties (COP-15) in Copenhagen in December 2009 in its decisions on methodological guidance for forest monitoring systems. Principles IFM has been defined by Global Witness as “the use of an independent third party that, by agreement with state authorities, provides an assessment of legal compliance, and observation of and guidance on official forest law enforcement systems.” IFM centres around the establishment of a partnership between an official ‘host institution’ responsible for oversight of the forest sector and an appointed monitoring organisation. The monitor's principal activity is to conduct field investigations to observe the work of the official law enforcement agency and to document illegal activity in the forest and related trade. These investigations result in the publication of authoritative information on forest operations, which is made widely available to all levels of government, industry, and civil society. By monitoring official forest law enforcement, IFM enables mechanisms of illegal activity and corruption to be identified. Monitors expect their evidence to be acted on and will pro-actively guard against entrenched resistance to improved governance. Use IFM has been undertaken in Cambodia, Cameroon, Republic of Congo, Honduras, and Nicaragua, and piloted in many other countries. IFM has also been called Independent Monitoring, Independent Observation, and Third Party Observation. References Notes Further reading Resource Extraction Monitoring, Independent Monitor IFM pages at Global Witness Cameroon IFM site The Overseas Development Institute's Verifor project on forest legality verification systems includes discussion of IFM External links IllegalLogging.info, a networking site providing background information on the key issues in the illegal logging debate", "canonical_url": "https://en.wikipedia.org/wiki/Independent_forest_monitoring", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:18.355915", "metadata": {"word_count": 107, "text_length": 2447}}
{"id": "wiki_medical_imaging", "query_word": "medical imaging", "title": "Medical imaging", "summary": "Medical imaging is the technique and process of imaging the interior of a body for clinical analysis and medical intervention, as well as visual representation of the function of some organs or tissues (physiology). Medical imaging seeks to reveal internal structures hidden by the skin and bones, as well as to diagnose and treat disease. Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. Although imaging of removed organs and tissues can be performed for medical reasons, such procedures are usually considered part of pathology instead of medical imaging. Measurement and recording techniques that are not primarily designed to produce images, such as electroencephalography (EEG), magnetoencephalography (MEG), electrocardiography (ECG), and others, represent other technologies that produce data susceptible to representation as a parameter graph versus time or maps that contain data about the measurement locations. In a limited comparison, these technologies can be considered forms of medical imaging in another discipline of medical instrumentation. As of 2010, 5 billion medical imaging studies had been conducted worldwide. Radiation exposure from medical imaging in 2006 made up about 50% of total ionizing radiation exposure in the United States. Medical imaging equipment is manufactured using technology from the semiconductor industry, including CMOS integrated circuit chips, power semiconductor devices, sensors such as image sensors (particularly CMOS sensors) and biosensors, and processors such as microcontrollers, microprocessors, digital signal processors, media processors and system-on-chip devices. As of 2015, annual shipments of medical imaging chips amount to 46 million units and $1.1 billion. The term \"noninvasive\" is used to denote a procedure where no instrument is introduced into a patient's body, which is the case for most imaging techniques used.", "text": "Medical imaging is the technique and process of imaging the interior of a body for clinical analysis and medical intervention, as well as visual representation of the function of some organs or tissues (physiology). Medical imaging seeks to reveal internal structures hidden by the skin and bones, as well as to diagnose and treat disease. Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. Although imaging of removed organs and tissues can be performed for medical reasons, such procedures are usually considered part of pathology instead of medical imaging. Measurement and recording techniques that are not primarily designed to produce images, such as electroencephalography (EEG), magnetoencephalography (MEG), electrocardiography (ECG), and others, represent other technologies that produce data susceptible to representation as a parameter graph versus time or maps that contain data about the measurement locations. In a limited comparison, these technologies can be considered forms of medical imaging in another discipline of medical instrumentation. As of 2010, 5 billion medical imaging studies had been conducted worldwide. Radiation exposure from medical imaging in 2006 made up about 50% of total ionizing radiation exposure in the United States. Medical imaging equipment is manufactured using technology from the semiconductor industry, including CMOS integrated circuit chips, power semiconductor devices, sensors such as image sensors (particularly CMOS sensors) and biosensors, and processors such as microcontrollers, microprocessors, digital signal processors, media processors and system-on-chip devices. As of 2015, annual shipments of medical imaging chips amount to 46 million units and $1.1 billion. The term \"noninvasive\" is used to denote a procedure where no instrument is introduced into a patient's body, which is the case for most imaging techniques used. History In 1972, engineer Godfrey Hounsfield from the British company EMI invented the X-ray computed tomography device for head diagnosis, which is commonly referred to as computed tomography (CT). The CT nucleus method is based on the projecting X-rays through a section of the human head, which are then processed by computer to reconstruct the cross-sectional image, known as image reconstruction. In 1975, EMI successfully developed a CT device for the entire body, enabling the clear acquisition of tomographic images of various parts of the human body. This revolutionary diagnostic technique earned Hounsfield and physicist Allan Cormack the Nobel Prize in Physiology or Medicine in 1979. Digital image processing technology for medical applications was inducted into the Space Foundation's Space Technology Hall of Fame in 1994. By 2010, over 5 billion medical imaging studies had been conducted worldwide. Radiation exposure from medical imaging in 2006 accounted for about 50% of total ionizing radiation exposure in the United States. Medical imaging equipment is manufactured using technology from the semiconductor industry, including CMOS integrated circuit chips, power semiconductor devices, sensors such as image sensors (particularly CMOS sensors) and biosensors, as well as processors like microcontrollers, microprocessors, digital signal processors, media processors and system-on-chip devices. As of 2015, annual shipments of medical imaging chips reached 46 million units, generating a market value of $1.1 billion. Types In the clinical context, \"invisible light\" medical imaging is generally equated to radiology or \"clinical imaging\". \"Visible light\" medical imaging involves digital video or still pictures that can be seen without special equipment. Dermatology and wound care are two modalities that use visible light imagery. Interpretation of medical images is generally undertaken by a physician specialising in radiology known as a radiologist; however, this may be undertaken by any healthcare professional who is trained and certified in radiological clinical evaluation. Increasingly interpretation is being undertaken by non-physicians, for example radiographers frequently train in interpretation as part of expanded practice. Diagnostic radiography designates the technical aspects of medical imaging and in particular the acquisition of medical images. The radiographer (also known as a radiologic technologist) is usually responsible for acquiring medical images of diagnostic quality; although other professionals may train in this area, notably some radiological interventions performed by radiologists are done so without a radiographer. As a field of scientific investigation, medical imaging constitutes a sub-discipline of biomedical engineering, medical physics or medicine depending on the context: Research and development in the area of instrumentation, image acquisition (e.g., radiography), modeling and quantification are usually the preserve of biomedical engineering, medical physics, and computer science; Research into the application and interpretation of medical images is usually the preserve of radiology and the medical sub-discipline relevant to medical condition or area of medical science (neuroscience, cardiology, psychiatry, psychology, etc.) under investigation. Many of the techniques developed for medical imaging also have scientific and industrial applications. Radiography Two forms of radiographic images are in use in medical imaging. Projection radiography and fluoroscopy, with the latter being useful for catheter guidance. These 2D techniques are still in wide use despite the advance of 3D tomography due to the low cost, high resolution, and depending on the application, lower radiation dosages with 2D technique. This imaging modality uses a wide beam of X-rays for image acquisition and is the first imaging technique available in modern medicine. Fluoroscopy produces real-time images of internal structures of the body in a similar fashion to radiography, but employs a constant input of X-rays, at a lower dose rate. Contrast media, such as barium, iodine, and air are used to visualize internal organs as they work. Fluoroscopy is also used in image-guided procedures when constant feedback during a procedure is required. An image receptor is required to convert the radiation into an image after it has passed through the area of interest. Early on, this was a fluorescing screen, which gave way to an Image Amplifier (IA) which was a large vacuum tube that had the receiving end coated with cesium iodide, and a mirror at the opposite end. Eventually the mirror was replaced with a TV camera. Projectional radiographs, more commonly known as X-rays, are often used to determine the type and extent of a fracture as well as for detecting pathological changes in the lungs. With the use of radio-opaque contrast media, such as barium, they can also be used to visualize the structure of the stomach and intestines – this can help diagnose ulcers or certain types of colon cancer. Magnetic resonance imaging A magnetic resonance imaging instrument (MRI scanner), or \"nuclear magnetic resonance (NMR) imaging\" scanner as it was originally known, uses powerful magnets to polarize and excite hydrogen nuclei (i.e., single protons) of water molecules in human tissue, producing a detectable signal which is spatially encoded, resulting in images of the body. The MRI machine emits a radio frequency (RF) pulse at the resonant frequency of the hydrogen atoms on water molecules. Radio frequency antennas (\"RF coils\") send the pulse to the area of the body to be examined. The RF pulse is absorbed by protons, causing their direction with respect to the primary magnetic field to change. When the RF pulse is turned off, the protons \"relax\" back to alignment with the primary magnet and emit radio-waves in the process. This radio-frequency emission from the hydrogen-atoms on water is what is detected and reconstructed into an image. The resonant frequency of a spinning magnetic dipole (of which protons are one example) is called the Larmor frequency and is determined by the strength of the main magnetic field and the chemical environment of the nuclei of interest. MRI uses three electromagnetic fields: a very strong (typically 1.5 to 3 teslas) static magnetic field to polarize the hydrogen nuclei, called the primary field; gradient fields that can be modified to vary in space and time (on the order of 1 kHz) for spatial encoding, often simply called gradients; and a spatially homogeneous radio-frequency (RF) field for manipulation of the hydrogen nuclei to produce measurable signals, collected through an RF antenna. Like CT, MRI traditionally creates a two-dimensional image of a thin \"slice\" of the body and is therefore considered a tomographic imaging technique. Modern MRI instruments are capable of producing images in the form of 3D blocks, which may be considered a generalization of the single-slice, tomographic, concept. Unlike CT, MRI does not involve the use of ionizing radiation and is therefore not associated with the same health hazards. For example, because MRI has only been in use since the early 1980s, there are no known long-term effects of exposure to strong static fields (this is the subject of some debate; see 'Safety' in MRI) and therefore there is no limit to the number of scans to which an individual can be subjected, in contrast with X-ray and CT. However, there are well-identified health risks associated with tissue heating from exposure to the RF field and the presence of implanted devices in the body, such as pacemakers. These risks are strictly controlled as part of the design of the instrument and the scanning protocols used. Because CT and MRI are sensitive to different tissue properties, the appearances of the images obtained with the two techniques differ markedly. In CT, X-rays must be blocked by some form of dense tissue to create an image, so the image quality when looking at so", "canonical_url": "https://en.wikipedia.org/wiki/Medical_imaging", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:20.874430", "metadata": {"word_count": 282, "text_length": 37674}}
{"id": "wiki_fluorescence_spectroscopy", "query_word": "fluorescence spectroscopy", "title": "Fluorescence spectroscopy", "summary": "Fluorescence spectroscopy (also known as fluorimetry or spectrofluorometry) is a type of electromagnetic spectroscopy that analyzes fluorescence from a sample. It involves using a beam of light, usually ultraviolet light, that excites the electrons in molecules of certain compounds and causes them to emit light; typically, but not necessarily, visible light. A complementary technique is absorption spectroscopy. In the special case of single molecule fluorescence spectroscopy, intensity fluctuations from the emitted light are measured from either single fluorophores, or pairs of fluorophores. Devices that measure fluorescence are called fluorometers.", "text": "Fluorescence spectroscopy (also known as fluorimetry or spectrofluorometry) is a type of electromagnetic spectroscopy that analyzes fluorescence from a sample. It involves using a beam of light, usually ultraviolet light, that excites the electrons in molecules of certain compounds and causes them to emit light; typically, but not necessarily, visible light. A complementary technique is absorption spectroscopy. In the special case of single molecule fluorescence spectroscopy, intensity fluctuations from the emitted light are measured from either single fluorophores, or pairs of fluorophores. Devices that measure fluorescence are called fluorometers. Theory Molecules have various states referred to as energy levels. Fluorescence spectroscopy is primarily concerned with electronic and vibrational states. Generally, the species being examined has a ground electronic state (a low energy state) of interest, and an excited electronic state of higher energy. Within each of these electronic states there are various vibrational states. In fluorescence, the species is first excited, by absorbing a photon, from its ground electronic state to one of the various vibrational states in the excited electronic state. Collisions with other molecules cause the excited molecule to lose vibrational energy until it reaches the lowest vibrational state of the excited electronic state. This process is often visualized with a Jablonski diagram. The molecule then drops down to one of the various vibrational levels of the ground electronic state again, emitting a photon in the process. As molecules may drop down into any of several vibrational levels in the ground state, the emitted photons will have different energies, and thus frequencies. Therefore, by analysing the different frequencies of light emitted in fluorescent spectroscopy, along with their relative intensities, the structure of the different vibrational levels can be determined. For atomic species, the process is similar; however, since atomic species do not have vibrational energy levels, the emitted photons are often at the same wavelength as the incident radiation. This process of re-emitting the absorbed photon is \"resonance fluorescence\" and while it is characteristic of atomic fluorescence, is seen in molecular fluorescence as well. In a typical fluorescence (emission) measurement, the excitation wavelength (the wavelength of the incident light used to excite the fluorophore) is fixed and the detection wavelength varies (producing an emission spectrum, while in a fluorescence excitation measurement the detection wavelength is fixed and the excitation wavelength is varied across a region of interest to produce an excitation spectrum. An excitation-emission matrix is obtained by recording the emission spectra resulting from a range of excitation wavelengths and combining them all together. This is a three dimensional surface data set: emission intensity as a function of excitation and emission wavelengths, and is typically depicted as a contour map. Instrumentation Two general types of instruments exist: filter fluorometers that use filters to isolate the incident light and fluorescent light and spectrofluorometers that use diffraction grating monochromators to isolate the incident light and fluorescent light. Both types use the following scheme: the light from an excitation source passes through a filter or monochromator, and strikes the sample. A proportion of the incident light is absorbed by the sample, and some of the molecules in the sample fluoresce. The fluorescent light is emitted in all directions. Some of this fluorescent light passes through a second filter or monochromator and reaches a detector, which is usually placed at 90° to the incident light beam to minimize the risk of transmitted or reflected incident light reaching the detector. Various light sources may be used as excitation sources, including lasers, LED, and lamps; xenon arcs and mercury-vapor lamps in particular. A laser only emits light of high irradiance at a very narrow wavelength interval, typically under 0.01 nm, which makes an excitation monochromator or filter unnecessary. The disadvantage of this method is that the wavelength of a laser cannot be changed by much. A mercury vapor lamp is a line lamp, meaning it emits light near peak wavelengths. By contrast, a xenon arc has a continuous emission spectrum with nearly constant intensity in the range from 300-800 nm and a sufficient irradiance for measurements down to just above 200 nm. Filters and/or monochromators may be used in fluorimeters. A monochromator transmits light of an adjustable wavelength with an adjustable tolerance. The most common type of monochromator utilizes a diffraction grating, that is, collimated light illuminates a grating and exits with a different angle depending on the wavelength. The monochromator can then be adjusted to select which wavelengths to transmit. For allowing anisotropy measurements, the addition of two polarization filters is necessary: One after the excitation monochromator or filter, and one before the emission monochromator or filter. As mentioned before, the fluorescence is most often measured at a 90° angle relative to the excitation light. This geometry is used instead of placing the sensor at the line of the excitation light at a 180° angle in order to avoid interference of the transmitted excitation light. No monochromator is perfect and it will transmit some stray light, that is, light with other wavelengths than the targeted. An ideal monochromator would only transmit light in the specified range and have a high wavelength-independent transmission. When measuring at a 90° angle, only the light scattered by the sample causes stray light. This results in a better signal-to-noise ratio, and lowers the detection limit by approximately a factor 10000, when compared to the 180° geometry. Furthermore, the fluorescence can also be measured from the front, which is often done for turbid or opaque samples . The detector can either be single-channeled or multichanneled. The single-channeled detector can only detect the intensity of one wavelength at a time, while the multichanneled one detects the intensity of all wavelengths simultaneously, making the emission monochromator or filter unnecessary. The most versatile fluorimeters with dual monochromators and a continuous excitation light source can record both an excitation spectrum and a fluorescence spectrum. When measuring fluorescence spectra, the wavelength of the excitation light is kept constant, preferably at a wavelength of high absorption, and the emission monochromator scans the spectrum. For measuring excitation spectra, the wavelength passing through the emission filter or monochromator is kept constant and the excitation monochromator is scanning. The excitation spectrum generally is identical to the absorption spectrum as the fluorescence intensity is proportional to the absorption. Analysis of data At low concentrations the fluorescence intensity will generally be proportional to the concentration of the fluorophore. Unlike in UV/visible spectroscopy, ‘standard’, device independent spectra are not easily attained. Several factors influence and distort the spectra, and corrections are necessary to attain ‘true’, i.e. machine-independent, spectra. The different types of distortions will here be classified as being either instrument- or sample-related. Firstly, the distortion arising from the instrument is discussed. As a start, the light source intensity and wavelength characteristics varies over time during each experiment and between each experiment. Furthermore, no lamp has a constant intensity at all wavelengths. To correct this, a beam splitter can be applied after the excitation monochromator or filter to direct a portion of the light to a reference detector. Additionally, the transmission efficiency of monochromators and filters must be taken into account. These may also change over time. The transmission efficiency of the monochromator also varies depending on wavelength. This is the reason that an optional reference detector should be placed after the excitation monochromator or filter. The percentage of the fluorescence picked up by the detector is also dependent upon the system. Furthermore, the detector quantum efficiency, that is, the percentage of photons detected, varies between different detectors, with wavelength and with time, as the detector inevitably deteriorates. Two other topics that must be considered include the optics used to direct the radiation and the means of holding or containing the sample material (called a cuvette or cell). For most UV, visible, and NIR measurements the use of precision quartz cuvettes is necessary. In both cases, it is important to select materials that have relatively little absorption in the wavelength range of interest. Quartz is ideal because it transmits from 200 nm-2500 nm; higher grade quartz can even transmit up to 3500 nm, whereas the absorption properties of other materials can mask the fluorescence from the sample. Correction of all these instrumental factors for getting a ‘standard’ spectrum is a tedious process, which is only applied in practice when it is strictly necessary. This is the case when measuring the quantum yield or when finding the wavelength with the highest emission intensity for instance. As mentioned earlier, distortions arise from the sample as well. Therefore, some aspects of the sample must be taken into account too. Firstly, photodecomposition may decrease the intensity of fluorescence over time. Scattering of light must also be taken into account. The most significant types of scattering in this context are Rayleigh and Raman scattering. Light scattered by Rayleigh scattering has the same wavelength as the incident light, whereas in Raman scattering the scattered light changes wavelength usually to longer wavelengths. Raman scattering is the result ", "canonical_url": "https://en.wikipedia.org/wiki/Fluorescence_spectroscopy", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:22.080615", "metadata": {"word_count": 89, "text_length": 17520}}
{"id": "wiki_raman_spectroscopy", "query_word": "raman spectroscopy", "title": "Raman spectroscopy", "summary": "Raman spectroscopy () (named after physicist C. V. Raman) is a spectroscopic technique typically used to determine vibrational modes of molecules, although rotational and other low-frequency modes of systems may also be observed. Raman spectroscopy is commonly used in chemistry to provide a structural fingerprint by which molecules can be identified. Raman spectroscopy relies upon inelastic scattering of photons, known as Raman scattering. A source of monochromatic light, usually from a laser in the visible, near infrared, or near ultraviolet range is used, although X-rays can also be used. The laser light interacts with molecular vibrations, phonons or other excitations in the system, resulting in the energy of the laser photons being shifted up or down. The shift in energy gives information about the vibrational modes in the system. Time-resolved spectroscopy and infrared spectroscopy typically yield similar yet complementary information. Typically, a sample is illuminated with a laser beam. Electromagnetic radiation from the illuminated spot is collected with a lens. Elastic scattered radiation at the wavelength corresponding to the laser line (Rayleigh scattering) is filtered out by either a notch filter, edge pass filter, or a band pass filter, while the rest of the collected light is dispersed onto a detector. Spontaneous Raman scattering is typically very weak. As a result, for many years the main difficulty in collecting Raman spectra was separating the weak inelastically scattered light from the intense Rayleigh scattered laser light (referred to as \"laser rejection\"). Historically, Raman spectrometers used holographic gratings and multiple dispersion stages to achieve a high degree of laser rejection. In the past, photomultipliers were the detectors of choice for dispersive Raman setups, which resulted in long acquisition times. However, modern instrumentation almost universally employs notch or edge filters for laser rejection. Dispersive single-stage spectrographs (axial transmissive (AT) or Czerny–Turner (CT) monochromators) paired with CCD detectors are most common although Fourier transform (FT) spectrometers are also common for use with NIR lasers. The name \"Raman spectroscopy\" typically refers to vibrational Raman spectroscopy using laser wavelengths which are not absorbed by the sample. There are many other variations of Raman spectroscopy including surface-enhanced Raman, resonance Raman, tip-enhanced Raman, polarized Raman, stimulated Raman, transmission Raman, spatially-offset Raman, and hyper Raman.", "text": "Raman spectroscopy () (named after physicist C. V. Raman) is a spectroscopic technique typically used to determine vibrational modes of molecules, although rotational and other low-frequency modes of systems may also be observed. Raman spectroscopy is commonly used in chemistry to provide a structural fingerprint by which molecules can be identified. Raman spectroscopy relies upon inelastic scattering of photons, known as Raman scattering. A source of monochromatic light, usually from a laser in the visible, near infrared, or near ultraviolet range is used, although X-rays can also be used. The laser light interacts with molecular vibrations, phonons or other excitations in the system, resulting in the energy of the laser photons being shifted up or down. The shift in energy gives information about the vibrational modes in the system. Time-resolved spectroscopy and infrared spectroscopy typically yield similar yet complementary information. Typically, a sample is illuminated with a laser beam. Electromagnetic radiation from the illuminated spot is collected with a lens. Elastic scattered radiation at the wavelength corresponding to the laser line (Rayleigh scattering) is filtered out by either a notch filter, edge pass filter, or a band pass filter, while the rest of the collected light is dispersed onto a detector. Spontaneous Raman scattering is typically very weak. As a result, for many years the main difficulty in collecting Raman spectra was separating the weak inelastically scattered light from the intense Rayleigh scattered laser light (referred to as \"laser rejection\"). Historically, Raman spectrometers used holographic gratings and multiple dispersion stages to achieve a high degree of laser rejection. In the past, photomultipliers were the detectors of choice for dispersive Raman setups, which resulted in long acquisition times. However, modern instrumentation almost universally employs notch or edge filters for laser rejection. Dispersive single-stage spectrographs (axial transmissive (AT) or Czerny–Turner (CT) monochromators) paired with CCD detectors are most common although Fourier transform (FT) spectrometers are also common for use with NIR lasers. The name \"Raman spectroscopy\" typically refers to vibrational Raman spectroscopy using laser wavelengths which are not absorbed by the sample. There are many other variations of Raman spectroscopy including surface-enhanced Raman, resonance Raman, tip-enhanced Raman, polarized Raman, stimulated Raman, transmission Raman, spatially-offset Raman, and hyper Raman. History Although the inelastic scattering of light was predicted by Adolf Smekal in 1923, it was not observed in practice until 1928. The Raman effect was named after one of its discoverers, the Indian scientist C. V. Raman, who observed the effect in organic liquids in 1928 together with K. S. Krishnan, and independently by Grigory Landsberg and Leonid Mandelstam in inorganic crystals. Raman won the Nobel Prize in Physics in 1930 for this discovery. The first observation of Raman spectra in gases was in 1929 by Franco Rasetti. Systematic pioneering theory of the Raman effect was developed by Czechoslovak physicist George Placzek between 1930 and 1934. The mercury arc became the principal light source, first with photographic detection and then with spectrophotometric detection. In the years following its discovery, Raman spectroscopy was used to provide the first catalog of molecular vibrational frequencies. Typically, the sample was held in a long tube and illuminated along its length with a beam of filtered monochromatic light generated by a gas discharge lamp. The photons that were scattered by the sample were collected through an optical flat at the end of the tube. To maximize the sensitivity, the sample was highly concentrated (1 M or more) and relatively large volumes (5 mL or more) were used. Theory The magnitude of the Raman effect correlates with the polarizability of the electrons in a molecule. It is a form of inelastic light scattering, where a photon excites the sample. This excitation puts the molecule into a virtual energy state for a short time before the photon is emitted. Inelastic scattering means that the energy of the emitted photon is of either lower or higher energy than the incident photon. After the scattering event, the sample is in a different rotational or vibrational state. For the total energy of the system to remain constant after the molecule moves to a new rovibronic (rotational–vibrational–electronic) state, the scattered photon shifts to a different energy, and therefore a different frequency. This energy difference is equal to that between the initial and final rovibronic states of the molecule. If the final state is higher in energy than the initial state, the scattered photon will be shifted to a lower frequency (lower energy) so that the total energy remains the same. This shift in frequency is called a Stokes shift, or downshift. If the final state is lower in energy, the scattered photon will be shifted to a higher frequency, which is called an anti-Stokes shift, or upshift. For a molecule to exhibit a Raman effect, there must be a change in its electric dipole-electric dipole polarizability with respect to the vibrational coordinate corresponding to the rovibronic state. The intensity of the Raman scattering is proportional to this polarizability change. Therefore, the Raman spectrum (scattering intensity as a function of the frequency shifts) depends on the rovibronic states of the molecule. The Raman effect is based on the interaction between the electron cloud of a sample and the external electric field of the monochromatic light, which can create an induced dipole moment within the molecule based on its polarizability. Because the laser light does not excite the molecule there can be no real transition between energy levels. The Raman effect should not be confused with emission (fluorescence or phosphorescence), where a molecule in an excited electronic state emits a photon and returns to the ground electronic state, in many cases to a vibrationally excited state on the ground electronic state potential energy surface. Raman scattering also contrasts with infrared (IR) absorption, where the energy of the absorbed photon matches the difference in energy between the initial and final rovibronic states. The dependence of Raman on the electric dipole-electric dipole polarizability derivative also differs from IR spectroscopy, which depends on the electric dipole moment derivative, the atomic polar tensor (APT). This contrasting feature allows rovibronic transitions that might not be active in IR to be analyzed using Raman spectroscopy, as exemplified by the rule of mutual exclusion in centrosymmetric molecules. Transitions which have large Raman intensities often have weak IR intensities and vice versa. If a bond is strongly polarized, a small change in its length such as that which occurs during a vibration has only a small resultant effect on polarization. Vibrations involving polar bonds (e.g. C-O, N-O, O-H) are therefore, comparatively weak Raman scatterers. Such polarized bonds, however, carry their electrical charges during the vibrational motion, (unless neutralized by symmetry factors), and this results in a larger net dipole moment change during the vibration, producing a strong IR absorption band. Conversely, relatively neutral bonds (e.g. C-C, C-H, C=C) suffer large changes in polarizability during a vibration. However, the dipole moment is not similarly affected such that while vibrations involving predominantly this type of bond are strong Raman scatterers, they are weak in the IR. A third vibrational spectroscopy technique, inelastic incoherent neutron scattering (IINS), can be used to determine the frequencies of vibrations in highly symmetric molecules that may be both IR and Raman inactive. The IINS selection rules, or allowed transitions, differ from those of IR and Raman, so the three techniques are complementary. They all give the same frequency for a given vibrational transition, but the relative intensities provide different information due to the different types of interaction between the molecule and the incoming particles, photons for IR and Raman, and neutrons for IINS. Raman shift Raman shifts are typically reported in wavenumbers, which have units of inverse length, as this value is directly related to energy. In order to convert between spectral wavelength and wavenumbers of shift in the Raman spectrum, the following formula can be used: Δ ν ~ = ( 1 λ 0 − 1 λ 1 ) , {\\displaystyle \\Delta {\\tilde {\\nu }}=\\left({\\frac {1}{\\lambda _{0}}}-{\\frac {1}{\\lambda _{1}}}\\right)\\ ,} where Δν̃ is the Raman shift expressed in wavenumber, λ0 is the excitation wavelength, and λ1 is the Raman spectrum wavelength. Most commonly, the unit chosen for expressing wavenumber in Raman spectra is inverse centimeters (cm−1). Since wavelength is often expressed in units of nanometers (nm), the formula above can scale for this unit conversion explicitly, giving Δ ν ~ ( cm − 1 ) = ( 1 λ 0 ( nm ) − 1 λ 1 ( nm ) ) × ( 10 7 nm ) ( cm ) . {\\displaystyle \\Delta {\\tilde {\\nu }}({\\text{cm}}^{-1})=\\left({\\frac {1}{\\lambda _{0}({\\text{nm}})}}-{\\frac {1}{\\lambda _{1}({\\text{nm}})}}\\right)\\times {\\frac {(10^{7}{\\text{nm}})}{({\\text{cm}})}}.} Instrumentation Modern Raman spectroscopy nearly always involves the use of lasers as excitation light sources. Because lasers were not available until more than three decades after the discovery of the effect, Raman and Krishnan used a mercury lamp and photographic plates to record spectra. Early spectra took hours or even days to acquire due to weak light sources, poor sensitivity of the detectors and the weak Raman scattering cross-sections of most materials. Various colored filters and chemical solutions were used to select certain wavelength regions for excitation and detection but the photographic spectra were still d", "canonical_url": "https://en.wikipedia.org/wiki/Raman_spectroscopy", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:23.172311", "metadata": {"word_count": 366, "text_length": 50372}}
{"id": "wiki_infrared_spectroscopy", "query_word": "infrared spectroscopy", "title": "Infrared spectroscopy", "summary": "Infrared spectroscopy (IR spectroscopy or vibrational spectroscopy) is the measurement of the interaction of infrared radiation with matter by absorption, emission, or reflection. It is used to study and identify chemical substances or functional groups in solid, liquid, or gaseous forms. It can be used to characterize new materials or identify and verify known and unknown samples. The method or technique of infrared spectroscopy is conducted with an instrument called an infrared spectrometer (or spectrophotometer) which produces an infrared spectrum. An IR spectrum can be visualized in a graph of infrared light absorbance (or transmittance) on the vertical axis vs. frequency, wavenumber or wavelength on the horizontal axis. Typical units of wavenumber used in IR spectra are reciprocal centimeters, with the symbol cm−1. Units of IR wavelength are commonly given in micrometers (formerly called \"microns\"), symbol μm, which are related to the wavenumber in a reciprocal way. A common laboratory instrument that uses this technique is a Fourier transform infrared (FTIR) spectrometer. Two-dimensional IR is also possible as discussed below. The infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14,000–4,000 cm−1 (0.7–2.5 μm wavelength) can excite overtone or combination modes of molecular vibrations. The mid-infrared, approximately 4,000–400 cm−1 (2.5–25 μm) is generally used to study the fundamental vibrations and associated rotational–vibrational structure. The far-infrared, approximately 400–10 cm−1 (25–1,000 μm) has low energy and may be used for rotational spectroscopy and low frequency vibrations. The region from 2–130 cm−1, bordering the microwave region, is considered the terahertz region and may probe intermolecular vibrations. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties.", "text": "Infrared spectroscopy (IR spectroscopy or vibrational spectroscopy) is the measurement of the interaction of infrared radiation with matter by absorption, emission, or reflection. It is used to study and identify chemical substances or functional groups in solid, liquid, or gaseous forms. It can be used to characterize new materials or identify and verify known and unknown samples. The method or technique of infrared spectroscopy is conducted with an instrument called an infrared spectrometer (or spectrophotometer) which produces an infrared spectrum. An IR spectrum can be visualized in a graph of infrared light absorbance (or transmittance) on the vertical axis vs. frequency, wavenumber or wavelength on the horizontal axis. Typical units of wavenumber used in IR spectra are reciprocal centimeters, with the symbol cm−1. Units of IR wavelength are commonly given in micrometers (formerly called \"microns\"), symbol μm, which are related to the wavenumber in a reciprocal way. A common laboratory instrument that uses this technique is a Fourier transform infrared (FTIR) spectrometer. Two-dimensional IR is also possible as discussed below. The infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14,000–4,000 cm−1 (0.7–2.5 μm wavelength) can excite overtone or combination modes of molecular vibrations. The mid-infrared, approximately 4,000–400 cm−1 (2.5–25 μm) is generally used to study the fundamental vibrations and associated rotational–vibrational structure. The far-infrared, approximately 400–10 cm−1 (25–1,000 μm) has low energy and may be used for rotational spectroscopy and low frequency vibrations. The region from 2–130 cm−1, bordering the microwave region, is considered the terahertz region and may probe intermolecular vibrations. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties. Uses and applications Infrared spectroscopy is a simple and reliable technique widely used in both organic and inorganic chemistry, in research and industry. It is used in quality control, dynamic measurement, and monitoring applications such as the long-term unattended measurement of CO2 concentrations in greenhouses and growth chambers by infrared gas analyzers. It is also used in forensic analysis in both criminal and civil cases, for example in identifying polymer degradation. It can be used in determining the blood alcohol content of a suspected drunk driver. IR spectroscopy has been used in identification of pigments in paintings and other art objects such as illuminated manuscripts. Infrared spectroscopy is also useful in measuring the degree of polymerization in polymer manufacture. Changes in the character or quantity of a particular bond are assessed by measuring at a specific frequency over time. Instruments can routinely record many spectra per second in situ, providing insights into reaction mechanism (e.g., detection of intermediates) and reaction progress. Infrared spectroscopy is utilized in the field of semiconductor microelectronics: for example, infrared spectroscopy can be applied to semiconductors like silicon, gallium arsenide, gallium nitride, zinc selenide, amorphous silicon, silicon nitride, etc. Another important application of infrared spectroscopy is in the food industry to measure the concentration of various compounds in different food products. Infrared spectroscopy is also used in gas leak detection devices such as the DP-IR and EyeCGAs. These devices detect hydrocarbon gas leaks in the transportation of natural gas and crude oil. Infrared spectroscopy is an important analysis method in the recycling process of household waste plastics, and a convenient stand-off method to sort plastic of different polymers (PET, HDPE, ...). Other developments include a miniature IR-spectrometer that's linked to a cloud based database and suitable for personal everyday use, and NIR-spectroscopic chips that can be embedded in smartphones and various gadgets. In catalysis research it is a very useful tool to characterize the catalyst, as well as to detect intermediates Infrared spectroscopy coupled with machine learning and artificial intelligence also has potential for rapid, accurate and non-invasive sensing of bacteria. The complex chemical composition of bacteria, including nucleic acids, proteins, carbohydrates and fatty acids, results in high-dimensional datasets where the essential features are effectively hidden under the total spectrum. Extraction of the essential features therefore requires advanced statistical methods such as machine learning and deep-neural networks. The potential of this technique for bacteria classification have been demonstrated for differentiation at the genus, species and serotype taxonomic levels, and it has also been shown promising for antimicrobial susceptibility testing, which is important for many clinical settings where faster susceptibility testing would decrease unnecessary blind-treatment with broad-spectrum antibiotics. The main limitation of this technique for clinical applications is the high sensitivity to technical equipment and sample preparation techniques, which makes it difficult to construct large-scale databases. Attempts in this direction have however been made by Bruker with the IR Biotyper for food microbiology. Theory Infrared spectroscopy exploits the fact that molecules absorb frequencies that are characteristic of their structure. These absorptions occur at resonant frequencies, i.e. the frequency of the absorbed radiation matches the vibrational frequency. The energies are affected by the shape of the molecular potential energy surfaces, the masses of the atoms, and the associated vibronic coupling. In particular, in the Born–Oppenheimer and harmonic approximations (i.e. when the molecular Hamiltonian corresponding to the electronic ground state can be approximated by a harmonic oscillator in the neighbourhood of the equilibrium molecular geometry), the resonant frequencies are associated with the normal modes of vibration corresponding to the molecular electronic ground state potential energy surface. Thus, it depends on both the nature of the bonds and the mass of the atoms that are involved. Using the Schrödinger equation leads to the selection rule for the vibrational quantum number in the system undergoing vibrational changes: △ v = ± 1 {\\displaystyle \\bigtriangleup v=\\pm 1} The compression and extension of a bond may be likened to the behaviour of a spring, but real molecules are hardly perfectly elastic in nature. If a bond between atoms is stretched, for instance, there comes a point at which the bond breaks and the molecule dissociates into atoms. Thus real molecules deviate from perfect harmonic motion and their molecular vibrational motion is anharmonic. An empirical expression that fits the energy curve of a diatomic molecule undergoing anharmonic extension and compression to a good approximation was derived by P.M. Morse, and is called the Morse function. Using the Schrödinger equation leads to the selection rule for the system undergoing vibrational changes : △ v = ± 1 , ± 2 , ± 3 , ⋅ ⋅ ⋅ {\\displaystyle \\bigtriangleup v=\\pm 1,\\pm 2,\\pm 3,\\cdot \\cdot \\cdot } Number of vibrational modes In order for a vibrational mode in a sample to be \"IR active\", it must be associated with changes in the molecular dipole moment. A permanent dipole is not necessary, as the rule requires only a change in dipole moment. A molecule can vibrate in many ways, and each way is called a vibrational mode. For molecules with N number of atoms, geometrically linear molecules have 3N – 5 degrees of vibrational modes, whereas nonlinear molecules have 3N – 6 degrees of vibrational modes (also called vibrational degrees of freedom). As examples linear carbon dioxide (CO2) has 3 × 3 – 5 = 4, while non-linear water (H2O), has only 3 × 3 – 6 = 3. Simple diatomic molecules have only one bond and only one vibrational band. If the molecule is symmetrical, e.g. N2, the band is not observed in the IR spectrum, but only in the Raman spectrum. Asymmetrical diatomic molecules, e.g. carbon monoxide (CO), absorb in the IR spectrum. More complex molecules have many bonds, and their vibrational spectra are correspondingly more complex, i.e. big molecules have many peaks in their IR spectra. The atoms in a CH2X2 group, commonly found in organic compounds and where X can represent any other atom, can vibrate in nine different ways. Six of these vibrations involve only the CH2 portion: two stretching modes (ν): symmetric (νs) and antisymmetric (νas); and four bending modes: scissoring (δ), rocking (ρ), wagging (ω) and twisting (τ), as shown below. Structures that do not have the two additional X groups attached have fewer modes because some modes are defined by specific relationships to those other attached groups. For example, in water, the rocking, wagging, and twisting modes do not exist because these types of motions of the H atoms represent simple rotation of the whole molecule rather than vibrations within it. In case of more complex molecules, out-of-plane (γ) vibrational modes can be also present. These figures do not represent the \"recoil\" of the C atoms, which, though necessarily present to balance the overall movements of the molecule, are much smaller than the movements of the lighter H atoms. The simplest and most important or fundamental IR bands arise from the excitations of normal modes, the simplest distortions of the molecule, from the ground state with vibrational quantum number v = 0 to the first excited state with vibrational quantum number v = 1. In some cases, overtone bands are observed. An overtone band arises from the absorption of a photon leading to a direct transition from the ground state to the second excite", "canonical_url": "https://en.wikipedia.org/wiki/Infrared_spectroscopy", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:24.058742", "metadata": {"word_count": 295, "text_length": 26949}}
{"id": "wiki_near_infrared", "query_word": "near infrared", "title": "Infrared", "summary": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.", "text": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting. Definition and relationship to the electromagnetic spectrum There is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1 mm (3 THz). Nature Sunlight, at an effective temperature of 5,780 K (5,510 °C, 9,940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 μm. On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy. Regions In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed. Visible limit Infrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700 nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050 nm from pulsed lasers can be seen by humans under certain conditions. Commonly used subdivision scheme A commonly used subdivision scheme is: NIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". CIE division scheme The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands: ISO 20473 scheme ISO 20473 specifies the following scheme: Astronomy division scheme Astronomers typically divide the infrared spectrum as follows: These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space. The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers. Sensor response division scheme A third scheme divides up the band based on the response of various detectors: Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon). Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region. Cryogenically cooled MCT detectors can cover the region of 1.0–2.5 μm. Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide, InSb and mercury cadmium telluride, HgCdTe, and partially by lead selenide, PbSe). Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers). Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon). Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available. The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage. Telecommunication bands In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors: The C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed. Heat Infrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from th", "canonical_url": "https://en.wikipedia.org/wiki/Infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:24.807060", "metadata": {"word_count": 456, "text_length": 36113}}
{"id": "wiki_thermal_infrared", "query_word": "thermal infrared", "title": "Infrared", "summary": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.", "text": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting. Definition and relationship to the electromagnetic spectrum There is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1 mm (3 THz). Nature Sunlight, at an effective temperature of 5,780 K (5,510 °C, 9,940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 μm. On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy. Regions In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed. Visible limit Infrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700 nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050 nm from pulsed lasers can be seen by humans under certain conditions. Commonly used subdivision scheme A commonly used subdivision scheme is: NIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". CIE division scheme The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands: ISO 20473 scheme ISO 20473 specifies the following scheme: Astronomy division scheme Astronomers typically divide the infrared spectrum as follows: These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space. The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers. Sensor response division scheme A third scheme divides up the band based on the response of various detectors: Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon). Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region. Cryogenically cooled MCT detectors can cover the region of 1.0–2.5 μm. Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide, InSb and mercury cadmium telluride, HgCdTe, and partially by lead selenide, PbSe). Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers). Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon). Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available. The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage. Telecommunication bands In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors: The C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed. Heat Infrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from th", "canonical_url": "https://en.wikipedia.org/wiki/Infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:26.128410", "metadata": {"word_count": 456, "text_length": 36113}}
{"id": "wiki_visible_spectrum", "query_word": "visible spectrum", "title": "Visible spectrum", "summary": "The visible spectrum is the band of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light (or simply light). The optical spectrum is sometimes considered to be the same as the visible spectrum, but some authors define the term more broadly, to include the ultraviolet and infrared parts of the electromagnetic spectrum as well, known collectively as optical radiation. A typical human eye will respond to wavelengths from about 380 to about 750 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 400–790 terahertz. These boundaries are not sharply defined and may vary per individual. Under optimal conditions, these limits of human perception can extend to 310 nm (ultraviolet) and 1100 nm (near infrared). The spectrum does not contain all the colors that the human visual system can distinguish. Unsaturated colors such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors. Visible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the Sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long-wavelength or far-infrared (LWIR or FIR) window, although other animals may perceive them.", "text": "The visible spectrum is the band of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light (or simply light). The optical spectrum is sometimes considered to be the same as the visible spectrum, but some authors define the term more broadly, to include the ultraviolet and infrared parts of the electromagnetic spectrum as well, known collectively as optical radiation. A typical human eye will respond to wavelengths from about 380 to about 750 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 400–790 terahertz. These boundaries are not sharply defined and may vary per individual. Under optimal conditions, these limits of human perception can extend to 310 nm (ultraviolet) and 1100 nm (near infrared). The spectrum does not contain all the colors that the human visual system can distinguish. Unsaturated colors such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors. Visible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the Sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long-wavelength or far-infrared (LWIR or FIR) window, although other animals may perceive them. Spectral colors Colors that can be produced by visible light of a narrow band of wavelengths (monochromatic light) are called spectral colors. The various color ranges indicated in the illustration are an approximation: The spectrum is continuous, with no clear boundaries between one color and the next. History In the 13th century, Roger Bacon theorized that rainbows were produced by a similar process to the passage of light through glass or crystal. In the 17th century, Isaac Newton discovered that prisms could disassemble and reassemble white light, and described the phenomenon in his book Opticks. He was the first to use the word spectrum (Latin for \"appearance\" or \"apparition\") in this sense in print in 1671 in describing his experiments in optics. Newton observed that, when a narrow beam of sunlight strikes the face of a glass prism at an angle, some is reflected and some of the beam passes into and through the glass, emerging as different-colored bands. Newton hypothesized light to be made up of \"corpuscles\" (particles) of different colors, with the different colors of light moving at different speeds in transparent matter, red light moving more quickly than violet in glass. The result is that red light is bent (refracted) less sharply than violet as it passes through the prism, creating a spectrum of colors. Newton originally divided the spectrum into six named colors: red, orange, yellow, green, blue, and violet. He later added indigo as the seventh color since he believed that seven was a perfect number as derived from the ancient Greek sophists, of there being a connection between the colors, the musical notes, the known objects in the Solar System, and the days of the week. The human eye is relatively insensitive to indigo's frequencies, and some people who have otherwise-good vision cannot distinguish indigo from blue and violet. For this reason, some later commentators, including Isaac Asimov, have suggested that indigo should not be regarded as a color in its own right but merely as a shade of blue or violet. Evidence indicates that what Newton meant by \"indigo\" and \"blue\" does not correspond to the modern meanings of those color words. Comparing Newton's observation of prismatic colors with a color image of the visible light spectrum shows that \"indigo\" corresponds to what is today called blue, whereas his \"blue\" corresponds to cyan. In the 18th century, Johann Wolfgang von Goethe wrote about optical spectra in his Theory of Colours. Goethe used the word spectrum (Spektrum) to designate a ghostly optical afterimage, as did Schopenhauer in On Vision and Colors. Goethe argued that the continuous spectrum was a compound phenomenon. Where Newton narrowed the beam of light to isolate the phenomenon, Goethe observed that a wider aperture produces not a spectrum but rather reddish-yellow and blue-cyan edges with white between them. The spectrum appears only when these edges are close enough to overlap. In the early 19th century, the concept of the visible spectrum became more definite, as light outside the visible range was discovered and characterized by William Herschel (infrared) and Johann Wilhelm Ritter (ultraviolet), Thomas Young, Thomas Johann Seebeck, and others. Young was the first to measure the wavelengths of different colors of light, in 1802. The connection between the visible spectrum and color vision was explored by Thomas Young and Hermann von Helmholtz in the early 19th century. Their theory of color vision correctly proposed that the eye uses three distinct receptors to perceive color. Limits to visible range The visible spectrum is limited to wavelengths that can both reach the retina and trigger visual phototransduction (excite a visual opsin). Insensitivity to UV light is generally limited by transmission through the lens. Insensitivity to IR light is limited by the spectral sensitivity functions of the visual opsins. The range is defined psychometrically by the luminous efficiency function, which accounts for all of these factors. In humans, there is a separate function for each of two visual systems, one for photopic vision, used in daylight, which is mediated by cone cells, and one for scotopic vision, used in dim light, which is mediated by rod cells. Each of these functions have different visible ranges. However, discussion on the visible range generally assumes photopic vision. Atmospheric transmission The visible range of most animals evolved to match the optical window, which is the range of light that can pass through the atmosphere. The ozone layer absorbs almost all UV light (below 315 nm). However, this only affects cosmic light (e.g. sunlight), not terrestrial light (e.g. Bioluminescence). Ocular transmission Before reaching the retina, light must first transmit through the cornea and lens. UVB light (< 315 nm) is filtered mostly by the cornea, and UVA light (315–400 nm) is filtered mostly by the lens. The lens also yellows with age, attenuating transmission most strongly at the blue part of the spectrum. This can cause xanthopsia as well as a slight truncation of the short-wave (blue) limit of the visible spectrum. Subjects with aphakia are missing a lens, so UVA light can reach the retina and excite the visual opsins; this expands the visible range and may also lead to cyanopsia. Opsin absorption Each opsin has a spectral sensitivity function, which defines how likely it is to absorb a photon of each wavelength. The luminous efficiency function is approximately the superposition of the contributing visual opsins. Variance in the position of the individual opsin spectral sensitivity functions therefore affects the luminous efficiency function and the visible range. For example, the long-wave (red) limit changes proportionally to the position of the L-opsin. The positions are defined by the peak wavelength (wavelength of highest sensitivity), so as the L-opsin peak wavelength blue shifts by 10 nm, the long-wave limit of the visible spectrum also shifts 10 nm. Large deviations of the L-opsin peak wavelength lead to a form of color blindness called protanomaly and a missing L-opsin (protanopia) shortens the visible spectrum by about 30 nm at the long-wave limit. Forms of color blindness affecting the M-opsin and S-opsin do not significantly affect the luminous efficiency function nor the limits of the visible spectrum. Different definitions Regardless of actual physical and biological variance, the definition of the limits is not standard and will change depending on the industry. For example, some industries may be concerned with practical limits, so would conservatively report 420–680 nm, while others may be concerned with psychometrics and achieving the broadest spectrum would liberally report 380–750, or even 380–800 nm. The luminous efficiency function in the NIR does not have a hard cutoff, but rather an exponential decay, such that the function's value (or vision sensitivity) at 1,050 nm is about 109 times weaker than at 700 nm; much higher intensity is therefore required to perceive 1,050 nm light than 700 nm light. Vision outside the visible spectrum Under ideal laboratory conditions, subjects may perceive infrared light up to at least 1,064 nm. While 1,050 nm NIR light can evoke red, suggesting direct absorption by the L-opsin, there are also reports that pulsed NIR lasers can evoke green, which suggests two-photon absorption may be enabling extended NIR sensitivity. Similarly, young subjects may perceive ultraviolet wavelengths down to about 310–313 nm, but detection of light below 380 nm may be due to fluorescence of the ocular media, rather than direct absorption of UV light by the opsins. As UVA light is absorbed by the ocular media (lens and cornea), it may fluoresce and be released at a lower energy (longer wavelength) that can then be absorbed by the opsins. For example, when the lens absorbs 350 nm light, the fluorescence emission spectrum is centered on 440 nm. Non-visual light detection In addition to the photopic and scotopic systems, humans have other systems for detecting light that do not contribute to the p", "canonical_url": "https://en.wikipedia.org/wiki/Visible_spectrum", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:27.060751", "metadata": {"word_count": 299, "text_length": 15239}}
{"id": "wiki_ultraviolet", "query_word": "ultraviolet", "title": "Ultraviolet", "summary": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.", "text": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see. Visibility Humans generally cannot use ultraviolet rays for vision. The lens of the human eye and surgically implanted lenses produced since 1986 block most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy. People lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet. Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds \"true\" UV vision. History and discovery \"Ultraviolet\" means \"beyond violet\" (from Latin ultra, \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light. UV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them \"(de-)oxidizing rays\" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them \"tithonic rays\"). The terms \"chemical rays\" and \"heat rays\" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established. The discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided \"unanimously\" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen. Subtypes The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348: Several solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum. Vacuum ultraviolet Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers. Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes. Extreme ultraviolet Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer satellite. Hard and soft ultraviolet Some sources use the distinction of \"hard UV\" and \"soft UV\". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with \"hard UV\" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions. Solar ultraviolet Very hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum. The atmosphere blocks about 77% of the Sun's UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth's surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB. The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photo", "canonical_url": "https://en.wikipedia.org/wiki/Ultraviolet", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:28.254295", "metadata": {"word_count": 368, "text_length": 56248}}
{"id": "wiki_multispectral_imaging", "query_word": "multispectral imaging", "title": "Multispectral imaging", "summary": "Multispectral imaging captures image data within specific wavelength ranges across the electromagnetic spectrum. The wavelengths may be separated by filters or detected with the use of instruments that are sensitive to particular wavelengths, including light from frequencies beyond the visible light range (i.e. infrared and ultraviolet). It can allow extraction of additional information the human eye fails to capture with its visible receptors for red, green and blue. It was originally developed for military target identification and reconnaissance. Early space-based imaging platforms incorporated multispectral imaging technology to map details of the Earth related to coastal boundaries, vegetation, and landforms. Multispectral imaging has also found use in document and painting analysis. Multispectral imaging measures light in a small number (typically 3 to 15) of spectral bands. Hyperspectral imaging is a special case of spectral imaging where often hundreds of contiguous spectral bands are available.", "text": "Multispectral imaging captures image data within specific wavelength ranges across the electromagnetic spectrum. The wavelengths may be separated by filters or detected with the use of instruments that are sensitive to particular wavelengths, including light from frequencies beyond the visible light range (i.e. infrared and ultraviolet). It can allow extraction of additional information the human eye fails to capture with its visible receptors for red, green and blue. It was originally developed for military target identification and reconnaissance. Early space-based imaging platforms incorporated multispectral imaging technology to map details of the Earth related to coastal boundaries, vegetation, and landforms. Multispectral imaging has also found use in document and painting analysis. Multispectral imaging measures light in a small number (typically 3 to 15) of spectral bands. Hyperspectral imaging is a special case of spectral imaging where often hundreds of contiguous spectral bands are available. Spectral band usage For different purposes, different combinations of spectral bands can be used. They are usually represented with red, green, and blue channels. Mapping of bands to colors depends on the purpose of the image and the personal preferences of the analysts. Thermal infrared is often omitted from consideration due to poor spatial resolution, except for special purposes. True-color uses only red, green, and blue channels, mapped to their respective colors. As a plain color photograph, it is good for analyzing man-made objects, and is easy to understand for beginner analysts. Green-red-infrared, where the blue channel is replaced with near infrared, is used for vegetation, which is highly reflective in near IR; it then shows as blue. This combination is often used to detect vegetation and camouflage. Blue-NIR-MIR, where the blue channel uses visible blue, green uses NIR (so vegetation stays green), and MIR is shown as red. Such images allow the water depth, vegetation coverage, soil moisture content, and the presence of fires to be seen, all in a single image. Many other combinations are in use. NIR is often shown as red, causing vegetation-covered areas to appear red. Typical spectral bands The wavelengths are approximate; exact values depend on the particular instruments (e.g. characteristics of satellite's sensors for Earth observation, characteristics of illumination and sensors for document analysis): Blue, 450–515/520 nm, is used for atmosphere and deep water imaging, and can reach depths up to 150 feet (50 m) in clear water. Green, 515/520–590/600 nm, is used for imaging vegetation and deep water structures, up to 90 feet (30 m) in clear water. Red, 600/630–680/690 nm, is used for imaging man-made objects, in water up to 30 feet (9 m) deep, soil, and vegetation. Near infrared (NIR), 750–900 nm, is used primarily for imaging vegetation. Mid-infrared (MIR), 1550–1750 nm, is used for imaging vegetation, soil moisture content, and some forest fires. Far-infrared (FIR), 2080–2350 nm, is used for imaging soil, moisture, geological features, silicates, clays, and fires. Thermal infrared, 10,400–12,500 nm, uses emitted instead of reflected radiation to image geological structures, thermal differences in water currents, fires, and for night studies. Radar and related technologies are useful for mapping terrain and for detecting various objects. Classification Unlike other aerial photographic and satellite image interpretation work, these multispectral images do not make it easy to identify directly the feature type by visual inspection. Hence the remote sensing data has to be classified first, followed by processing by various data enhancement techniques so as to help the user to understand the features that are present in the image. Such classification is a complex task which involves rigorous validation of the training samples depending on the classification algorithm used. The techniques can be grouped mainly into two types. Supervised classification techniques Unsupervised classification techniques Supervised classification makes use of training samples. Training samples are areas on the ground for which there is ground truth, that is, what is there is known. The spectral signatures of the training areas are used to search for similar signatures in the remaining pixels of the image, and we will classify accordingly. This use of training samples for classification is called supervised classification. Expert knowledge is very important in this method since the selection of the training samples and a biased selection can badly affect the accuracy of classification. Popular techniques include the maximum likelihood principle and convolutional neural network. The maximum likelihood principle calculates the probability of a pixel belonging to a class (i.e. feature) and allots the pixel to its most probable class. Newer convolutional neural network based methods account for both spatial proximity and entire spectra to determine the most likely class. In case of unsupervised classification no prior knowledge is required for classifying the features of the image. The natural clustering or grouping of the pixel values (i.e. the gray levels of the pixels) are observed. Then a threshold is defined for adopting the number of classes in the image. The finer the threshold value, the more classes there will be. However, beyond a certain limit the same class will be represented in different classes in the sense that variation in the class is represented. After forming the clusters, ground truth validation is done to identify the class the image pixel belongs to. Thus in this unsupervised classification a priori information about the classes is not required. One of the popular methods in unsupervised classification is k-means clustering. Data analysis software MicroMSI is endorsed by the NGA. Opticks is an open-source remote sensing application. Multispec is freeware multispectral analysis software. Gerbil is open source multispectral visualization and analysis software. Applications Military target tracking Multispectral imaging measures light emission and is often used in detecting or tracking military targets. In 2003, researchers at the United States Army Research Laboratory and the Federal Laboratory Collaborative Technology Alliance reported a dual band multispectral imaging focal plane array (FPA). This FPA allowed researchers to look at two infrared (IR) planes at the same time. Because mid-wave infrared (MWIR) and long wave infrared (LWIR) technologies measure radiation inherent to the object and require no external light source, they also are referred to as thermal imaging methods. The brightness of the image produced by a thermal imager depends on the objects emissivity and temperature. Every material has an infrared signature that aids in the identification of the object. These signatures are less pronounced in hyperspectral systems (which image in many more bands than multispectral systems) and when exposed to wind and, more dramatically, to rain. Sometimes the surface of the target may reflect infrared energy. This reflection may misconstrue the true reading of the objects' inherent radiation. Imaging systems that use MWIR technology function better with solar reflections on the target's surface and produce more definitive images of hot objects, such as engines, compared to LWIR technology. However, LWIR operates better in hazy environments like smoke or fog because less scattering occurs in the longer wavelengths. Researchers claim that dual-band technologies combine these advantages to provide more information from an image, particularly in the realm of target tracking. For nighttime target detection, thermal imaging outperformed single-band multispectral imaging. Dual band MWIR and LWIR technology resulted in better visualization during the nighttime than MWIR alone. Citation Citation. The US Army reports that its dual band LWIR/MWIR FPA demonstrated better visualizing of tactical vehicles than MWIR alone after tracking them through both day and night. Land mine detection By analyzing the emissivity of ground surfaces, multispectral imaging can detect the presence of underground missiles. Surface and sub-surface soil possess different physical and chemical properties that appear in spectral analysis. Disturbed soil has increased emissivity in the wavelength range of 8.5 to 9.5 micrometers while demonstrating no change in wavelengths greater than 10 micrometers. The US Army Research Laboratory's dual MWIR/LWIR FPA used \"red\" and \"blue\" detectors to search for areas with enhanced emissivity. The red detector acts as a backdrop, verifying realms of undisturbed soil areas, as it is sensitive to the 10.4 micrometer wavelength. The blue detector is sensitive to wavelengths of 9.3 micrometers. If the intensity of the blue image changes when scanning, that region is likely disturbed. The scientists reported that fusing these two images increased detection capabilities. Ballistic missile detection Intercepting an intercontinental ballistic missile (ICBM) in its boost phase requires imaging of the hard body as well as the rocket plumes. MWIR presents a strong signal from highly heated objects including rocket plumes, while LWIR produces emissions from the missile's body material. The US Army Research Laboratory reported that with their dual-band MWIR/LWIR technology, tracking of the Atlas 5 Evolved Expendable Launch Vehicles, similar in design to ICBMs, picked up both the missile body and plumage. Space-based imaging Most radiometers for remote sensing (RS) acquire multispectral images. Dividing the spectrum into many bands, multispectral is the opposite of panchromatic, which records only the total intensity of radiation falling on each pixel. Usually, Earth observation satellites have three or more radiometers. Each acquires one digital image (in remote sensing, called a 'scene') in a small spectral band. ", "canonical_url": "https://en.wikipedia.org/wiki/Multispectral_imaging", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:29.149525", "metadata": {"word_count": 143, "text_length": 13320}}
{"id": "wiki_imaging_spectrometer", "query_word": "imaging spectrometer", "title": "Imaging spectrometer", "summary": "An imaging spectrometer is an instrument used in hyperspectral imaging and imaging spectroscopy to acquire a spectrally-resolved image of an object or scene, usually to support analysis of the composition the object being imaged. The spectral data produced for a pixel is often referred to as a datacube due to the three-dimensional representation of the data. Two axes of the image correspond to vertical and horizontal distance and the third to wavelength. The principle of operation is the same as that of the simple spectrometer, but special care is taken to avoid optical aberrations for better image quality. Example imaging spectrometer types include: filtered camera, whiskbroom scanner, pushbroom scanner, integral field spectrograph (or related dimensional reformatting techniques), wedge imaging spectrometer, Fourier transform imaging spectrometer, computed tomography imaging spectrometer (CTIS), image replicating imaging spectrometer (IRIS), coded aperture snapshot spectral imager (CASSI), and image mapping spectrometer (IMS).", "text": "An imaging spectrometer is an instrument used in hyperspectral imaging and imaging spectroscopy to acquire a spectrally-resolved image of an object or scene, usually to support analysis of the composition the object being imaged. The spectral data produced for a pixel is often referred to as a datacube due to the three-dimensional representation of the data. Two axes of the image correspond to vertical and horizontal distance and the third to wavelength. The principle of operation is the same as that of the simple spectrometer, but special care is taken to avoid optical aberrations for better image quality. Example imaging spectrometer types include: filtered camera, whiskbroom scanner, pushbroom scanner, integral field spectrograph (or related dimensional reformatting techniques), wedge imaging spectrometer, Fourier transform imaging spectrometer, computed tomography imaging spectrometer (CTIS), image replicating imaging spectrometer (IRIS), coded aperture snapshot spectral imager (CASSI), and image mapping spectrometer (IMS). Background In 1704, Sir Isaac Newton demonstrated that white light could be split up into component colours. The subsequent history of spectroscopy led to precise measurements and provided the empirical foundations for atomic and molecular physics (Born & Wolf, 1999). Significant achievements in imaging spectroscopy are attributed to airborne instruments, particularly arising in the early 1980s and 1990s (Goetz et al., 1985; Vane et al., 1984). However, it was not until 1999 that the first imaging spectrometer was launched in space (the NASA Moderate-resolution Imaging Spectroradiometer, or MODIS). Terminology and definitions evolve over time. At one time, >10 spectral bands sufficed to justify the term imaging spectrometer but presently the term is seldom defined by a total minimum number of spectral bands, rather by a contiguous (or redundant) statement of spectral bands. Principle Imaging spectrometers are used specifically for the purpose of measuring the spectral content of light and electromagnetic light. The spectral data gathered is used to give the operator insight into the sources of radiation. Prism spectrometers use a classical method of dispersing radiation by means of a prism as a refracting element. The imaging spectrometer works by imaging a radiation source onto what is called a \"slit\" by means of a source imager. A collimator collimates the beam that is dispersed by a refracting prism and re-imaged onto a detection system by a re-imager. Special care is taken to produce the best possible image of the source onto the slit. The purpose of the collimator and re-imaging optics are to take the best possible image of the slit. An area-array of elements fills the detection system at this stage. The source image is reimaged, every point, as a line spectrum on what is called a detector-array column. The detector array signals supply data pertaining to spectral content, in particular, spatially resolved source points inside source area. These source points are imaged onto the slit and then re-imaged onto the detector array. Simultaneously, the system provides spectral information about the source area and its line of spatially resolved points. The line is then scanned in order to build a database of information about the spectral content. In imaging spectroscopy (also hyperspectral imaging or spectral imaging) each pixel of an image acquires many bands of light intensity data from the spectrum, instead of just the three bands of the RGB color model. More precisely, it is the simultaneous acquisition of spatially coregistered images in many spectrally contiguous bands. Some spectral images contain only a few image planes of a spectral data cube, while others are better thought of as full spectra at every location in the image. For example, solar physicists use the spectroheliograph to make images of the Sun built up by scanning the slit of a spectrograph, to study the behavior of surface features on the Sun; such a spectroheliogram may have a spectral resolution of over 100,000 ( λ / Δ λ {\\displaystyle \\lambda /\\Delta \\lambda } ) and be used to measure local motion (via the Doppler shift) and even the magnetic field (via the Zeeman splitting or Hanle effect) at each location in the image plane. The multispectral images collected by the Opportunity rover, in contrast, have only four wavelength bands and hence are only a little more than 3-color images. Unmixing Hyperspectral data is often used to determine what materials are present in a scene. Materials of interest could include roadways, vegetation, and specific targets (i.e. pollutants, hazardous materials, etc.). Trivially, each pixel of a hyperspectral image could be compared to a material database to determine the type of material making up the pixel. However, many hyperspectral imaging platforms have low resolution (>5m per pixel) causing each pixel to be a mixture of several materials. The process of unmixing one of these 'mixed' pixels is called hyperspectral image unmixing or simply hyperspectral unmixing. A solution to hyperspectral unmixing is to reverse the mixing process. Generally, two models of mixing are assumed: linear and nonlinear. Linear mixing models the ground as being flat and incident sunlight on the ground causes the materials to radiate some amount of the incident energy back to the sensor. Each pixel then, is modeled as a linear sum of all the radiated energy curves of materials making up the pixel. Therefore, each material contributes to the sensor's observation in a positive linear fashion. Additionally, a conservation of energy constraint is often observed thereby forcing the weights of the linear mixture to sum to one in addition to being positive. The model can be described mathematically as follows: p = A ∗ x {\\displaystyle p=A*x\\,} where p {\\displaystyle p} represents a pixel observed by the sensor, A {\\displaystyle A} is a matrix of material reflectance signatures (each signature is a column of the matrix), and x {\\displaystyle x} is the proportion of material present in the observed pixel. This type of model is also referred to as a simplex. With x {\\displaystyle x} satisfying the two constraints: 1. Abundance Nonnegativity Constraint (ANC) - each element of x is positive. 2. Abundance Sum-to-one Constraint (ASC) - the elements of x must sum to one. Non-linear mixing results from multiple scattering often due to non-flat surface such as buildings and vegetation. There are many algorithms to unmix hyperspectral data each with their own strengths and weaknesses. Many algorithms assume that pure pixels (pixels which contain only one materials) are present in a scene. Some algorithms to perform unmixing are listed below: Pixel Purity Index Works by projecting each pixel onto one vector from a set of random vectors spanning the reflectance space. A pixel receives a score when it represent an extremum of all the projections. Pixels with the highest scores are deemed to be spectrally pure. N-FINDR Gift Wrapping Algorithm Independent Component Analysis Endmember Extraction Algorithm - works by assuming that pure pixels occur independently than mixed pixels. Assumes pure pixels are present. Vertex Component Analysis - works on the fact that the affine transformation of a simplex is another simplex which helps to find hidden (folded) vertices of the simplex. Assumes pure pixels are present. Principal component analysis - could also be used to determine endmembers, projection on principal axes could permit endmember selection Multi endmembers spatial mixture analysis based on the SMA algorithm Spectral phasor analysis based on Fourier transformation of spectra and plotting them on a 2D plot. Non-linear unmixing algorithms also exist: support vector machines or analytical neural network. Probabilistic methods have also been attempted to unmix pixel through Monte Carlo unmixing algorithm. Once the fundamental materials of a scene are determined, it is often useful to construct an abundance map of each material which displays the fractional amount of material present at each pixel. Often linear programming is done to observed ANC and ASC. Applications Planetary observations The practical application of imaging spectrometers is they are used to observe the planet Earth from orbiting satellites. The spectrometer functions by recording all points of color on a picture, thus, the spectrometer is focused on specific parts of the Earth's surface to record data. The advantages of spectral content data include vegetation identification, physical condition analysis, mineral identification for the purpose of potential mining, and the assessment of polluted waters in oceans, coastal zones and inland waterways. Prism spectrometers are ideal for Earth observation because they measure wide spectral ranges competently. Spectrometers can be set to cover a range from 400 nm to 2,500 nm, which interests scientists who are able to observe Earth by means of aircraft and satellite. The spectral resolution of the prism spectrometer is not desirable for most scientific applications; thus, its purpose is specific to recording spectral content of areas with greater spatial variations. Venus express, orbiting Venus, had a number of imaging spectrometers covering NIR-vis-UV. Geophysical imaging One application is spectral geophysical imaging, which allows quantitative and qualitative characterization of the surface and of the atmosphere, using radiometric measurements. These measurements can then be used for unambiguous direct and indirect identification of surface materials and atmospheric trace gases, the measurement of their relative concentrations, subsequently the assignment of the proportional contribution of mixed pixel signals (e.g., the spectral unmixing problem), the derivation of their spatial distribution (mapping problem), and finally their study over time (multi-temporal analysis). The Moon Mineralogy Mapper on Chandray", "canonical_url": "https://en.wikipedia.org/wiki/Imaging_spectrometer", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:30.328937", "metadata": {"word_count": 145, "text_length": 14899}}
{"id": "wiki_pushbroom_scanner", "query_word": "pushbroom scanner", "title": "Push broom scanner", "summary": "A push broom scanner, also known as an along-track scanner, is a device for obtaining images with spectroscopic sensors. The scanners are regularly used for passive remote sensing from space, and in spectral analysis on production lines, for example with near-infrared spectroscopy used to identify contaminated food and feed. The moving scanner line in a traditional photocopier (or a scanner or facsimile machine) is also a familiar, everyday example of a push broom scanner. Push broom scanners and the whisk broom scanners variant (also known as across-track scanners) are often contrasted with staring arrays (such as in a digital camera), which image objects without scanning, and are more familiar to most people. In orbital push broom sensors, a line of sensors arranged perpendicular to the flight direction of the spacecraft is used. Different areas of the surface are imaged as the spacecraft flies forward. A push broom scanner can gather more light than a whisk broom scanner because it looks at a particular area for a longer time, like a long exposure on a camera. One drawback of push broom sensors is the varying sensitivity of the individual detectors. Another drawback is that the resolution is lower than a whisk broom scanner because the entire image is captured at once. Examples of spacecraft cameras using push broom imagers include Mars Express's High Resolution Stereo Camera, Lunar Reconnaissance Orbiter Camera NAC, Mars Global Surveyor's Mars Orbiter Camera WAC, and the Multi-angle Imaging SpectroRadiometer on board the Terra satellite.", "text": "A push broom scanner, also known as an along-track scanner, is a device for obtaining images with spectroscopic sensors. The scanners are regularly used for passive remote sensing from space, and in spectral analysis on production lines, for example with near-infrared spectroscopy used to identify contaminated food and feed. The moving scanner line in a traditional photocopier (or a scanner or facsimile machine) is also a familiar, everyday example of a push broom scanner. Push broom scanners and the whisk broom scanners variant (also known as across-track scanners) are often contrasted with staring arrays (such as in a digital camera), which image objects without scanning, and are more familiar to most people. In orbital push broom sensors, a line of sensors arranged perpendicular to the flight direction of the spacecraft is used. Different areas of the surface are imaged as the spacecraft flies forward. A push broom scanner can gather more light than a whisk broom scanner because it looks at a particular area for a longer time, like a long exposure on a camera. One drawback of push broom sensors is the varying sensitivity of the individual detectors. Another drawback is that the resolution is lower than a whisk broom scanner because the entire image is captured at once. Examples of spacecraft cameras using push broom imagers include Mars Express's High Resolution Stereo Camera, Lunar Reconnaissance Orbiter Camera NAC, Mars Global Surveyor's Mars Orbiter Camera WAC, and the Multi-angle Imaging SpectroRadiometer on board the Terra satellite. See also Time delay and integration Whisk broom scanner Rolling shutter References External links Earth Observing-1 (NASA), with animated whisk broom and push broom illustrations Airborne Pushbroom Line Scan (PDF) – overview article Linear Pushbroom Cameras (PDF) – detailed modelling theory", "canonical_url": "https://en.wikipedia.org/wiki/Push_broom_scanner", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:30.989676", "metadata": {"word_count": 247, "text_length": 1859}}
{"id": "wiki_whiskbroom_scanner", "query_word": "whiskbroom scanner", "title": "Whisk broom scanner", "summary": "A whisk broom or spotlight sensor, also known as an across-track scanner, is a technology for obtaining satellite images with optical cameras. It is used for passive remote sensing from space. In a whisk broom sensor, a mirror scans across the satellite’s path (ground track), reflecting light into a single detector which collects data one pixel at a time. The moving parts make this type of sensor expensive and more prone to wearing out, such as in the Landsat 7. Whisk broom scanners have the effect of stopping the scan, and focusing the detector on one part of the swath width. Because the detector is only focused on a subsection of the full swath at any time, it typically has a higher resolution than a push broom design for the same size of scan swath. All sensors aboard the Landsat series of satellites used the whisk broom design until Landsat 8 which used a push broom sensor.", "text": "A whisk broom or spotlight sensor, also known as an across-track scanner, is a technology for obtaining satellite images with optical cameras. It is used for passive remote sensing from space. In a whisk broom sensor, a mirror scans across the satellite’s path (ground track), reflecting light into a single detector which collects data one pixel at a time. The moving parts make this type of sensor expensive and more prone to wearing out, such as in the Landsat 7. Whisk broom scanners have the effect of stopping the scan, and focusing the detector on one part of the swath width. Because the detector is only focused on a subsection of the full swath at any time, it typically has a higher resolution than a push broom design for the same size of scan swath. All sensors aboard the Landsat series of satellites used the whisk broom design until Landsat 8 which used a push broom sensor. See also Push broom scanner References External links Earth Observing-1 (NASA), with animated whisk broom and push broom illustrations", "canonical_url": "https://en.wikipedia.org/wiki/Whisk_broom_scanner", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:31.627924", "metadata": {"word_count": 157, "text_length": 1025}}
{"id": "wiki_snapshot_hyperspectral_imaging", "query_word": "snapshot hyperspectral imaging", "title": "Snapshot hyperspectral imaging", "summary": "Snapshot hyperspectral imaging is a method for capturing hyperspectral images during a single integration time of a detector array. No scanning is involved with this method, in contrast to push broom and whisk broom scanning techniques. The lack of moving parts means that motion artifacts should be avoided. This instrument typically features detector arrays with a high number of pixels.", "text": "Snapshot hyperspectral imaging is a method for capturing hyperspectral images during a single integration time of a detector array. No scanning is involved with this method, in contrast to push broom and whisk broom scanning techniques. The lack of moving parts means that motion artifacts should be avoided. This instrument typically features detector arrays with a high number of pixels. Development Although the first known reference to a snapshot hyperspectral imaging device—the Bowen \"image slicer\"—dates from 1938, the concept was not successful until a larger amount of spatial resolution was available. With the arrival of large-format detector arrays in the late 1980s and early 1990s, a series of new snapshot hyperspectral imaging techniques were developed to take advantage of the new technology: a method which uses a fiber bundle at the image plane and reformatting the fibers in the opposite end of the bundle to a long line, viewing a scene through a 2D grating and reconstructing the multiplexed data with computed tomography mathematics, the (lenslet-based) integral field spectrograph, a modernized version of Bowen's image slicer. More recently, a number of research groups have attempted to advance the technology in order to create devices capable of commercial use. These newer devices include the HyperPixel Array imager a derivative of the integral field spectrograph, a multiaperture spectral filter approach, a compressive-sensing–based approach using a coded aperture, a microfaceted-mirror-based approach, a generalization of the Lyot filter, and a generalization of the Bayer filter approach to multispectral filtering. Slitless spectroscopy can be considered a basic snapshot hyperspectral imaging technique. Spaced point-like sources, such as a sparse field of stars, is a requirement to avoid spectrum overlap on the detector. Applications While snapshot instruments are featured prominently in the research literature, none of these instruments have seen wide adoption in commercial use (i.e. outside the professional astronomical community) due to manufacturing limitations. Thus, their primary venue continues to be astronomical telescopes. One of the main reasons for the popularity of snapshot devices in the astronomical community is that they offer large increases in the light collection capacity of a telescope when performing hyperspectral imaging. Recent applications have been in soil spectroscopy and vegetation sciences. See also Chemical imaging Computed tomography imaging spectrometer Imaging spectrometer Imaging spectroscopy Multi-spectral image Spectral imaging Video spectroscopy == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Snapshot_hyperspectral_imaging", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:32.296694", "metadata": {"word_count": 60, "text_length": 2651}}
{"id": "wiki_real-time_processing", "query_word": "real-time processing", "title": "Real-time computing", "summary": "Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\". The term \"real-time\" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock. Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given. Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load. A real-time system has been described as one which \"controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time\". The term \"real-time\" is used in process control and enterprise systems to mean \"without significant delay\". Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems (RTOSes), and real-time networks. Each of these provides essential frameworks on which to build a real-time software application. Systems used for many safety-critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes, both of which demand immediate and accurate mechanical response.", "text": "Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\". The term \"real-time\" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock. Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given. Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load. A real-time system has been described as one which \"controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time\". The term \"real-time\" is used in process control and enterprise systems to mean \"without significant delay\". Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems (RTOSes), and real-time networks. Each of these provides essential frameworks on which to build a real-time software application. Systems used for many safety-critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes, both of which demand immediate and accurate mechanical response. History The term real-time derives from its use in early simulation, where a real-world process is simulated at a rate which matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. Minicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as DOG (Digital on-screen graphic) scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data. Operating systems such as Data General's RDOS (Real-Time Disk Operating System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low-priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOS and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals. Early personal computers were sometimes used for real-time computing. The possibility of deactivating other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus bypassing all interrupting Windows code. However, several coding libraries exist that offer real-time capabilities in a high-level language on a variety of operating systems, for example, Real-time Java. Later microprocessors such as the Motorola 68000 and subsequent family members (68010, 68020, ColdFire, etc.) also became popular with manufacturers of industrial control systems. This application area is one where real-time control offers genuine advantages in terms of process performance and safety. Criteria for real-time computing A system is said to be real-time if the total correctness of an operation depends not only upon its logical correctness, but also upon the time in which it is performed. Real-time systems, as well as their deadlines, are classified by the consequence of missing a deadline: Hard – missing a deadline is a total system failure. Firm – infrequent deadline misses are tolerable, but may degrade the system's quality of service. The usefulness of a result is zero after its deadline. Soft – the usefulness of a result degrades after its deadline, thereby degrading the system's quality of service. Thus, the goal of a hard real-time system is to ensure that all deadlines are met, but for soft real-time systems the goal becomes meeting a certain subset of deadlines in order to optimize some application-specific criteria. The particular criteria optimized depend on the application, but some typical examples include maximizing the number of deadlines met, minimizing the lateness of tasks and maximizing the number of high-priority tasks meeting their deadlines. Hard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system). Some examples of hard real-time systems: A car engine control system is a hard real-time system because a delayed signal may cause engine failure or damage. Medical systems such as heart pacemakers. Even though a pacemaker's task is simple, because of the potential risk to human life, medical systems like these are typically required to undergo thorough testing and certification, which in turn requires hard real-time computing in order to offer provable guarantees that a failure is unlikely or impossible. Industrial process controllers, such as a machine on an assembly line. If the machine is delayed, the item on the assembly line could pass beyond the reach of the machine (leaving the product untouched), or the machine or the product could be damaged by activating the robot at the wrong time. If the failure is detected, both cases would lead to the assembly line stopping, which slows production. If the failure is not detected, a product with a defect could make it through production, or could cause damage in later steps of production. Hard real-time systems are typically found interacting at a low level with physical hardware, in embedded systems. Early video game systems, such as the Atari 2600 and Cinematronics vector graphics, had hard real-time requirements because of the nature of the graphics and timing hardware. Softmodems replace a hardware modem with software running on a computer's CPU. The software must run every few milliseconds to generate the next audio data to be output. If that data is late, the receiving modem will lose synchronization, causing a long interruption as synchronization is reestablished or causing the connection to be lost entirely. Many types of printers have hard real-time requirements, such as inkjets (the ink must be deposited at the correct time as the printhead crosses the page), laser printers (the laser must be activated at the right time as the beam scans across the rotating drum), and dot matrix and various types of line printers (the impact mechanism must be activated at the right time as the print mechanism comes into alignment with the desired output). A failure in any of these would cause either missing output or misaligned output. In the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). In some situations, these can guarantee hard real-time performance (for instance, if the set of tasks and their priorities is known in advance). There are other hard real-time schedulers such as rate-monotonic, which is not common in general-purpose systems, as it requires additional information in order to schedule a task: namely, a bound or worst-case estimate for how long the task must execute. Specific algorithms for scheduling such hard real-time tasks exist, like earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%. New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications. Firm real-time systems are more nebulously defined, and some classifications do not include them, distinguishing only hard and soft real-time systems. Some examples of firm real-time systems: The assembly line machine described earlier as hard real-time could instead be considered firm real-time. A missed deadline still causes an error that needs to be dealt with: there might be machinery to mark a part as bad or eject it from the assembly line, or the assembly line could be stopped so an operator can correct the problem. However, as long as these errors are infrequent, they may be tolerated. Soft real-time systems are typically used to solve issues of concurrent access and the need to keep a number of connected systems up-to-date through changing situations. Some examples of soft real-time systems: Software that maintains and updates the flight plans for commercial airliners. The flight plans must be kept reasonably current, but they can operate with the latency of a few seconds. Live audio-video systems are also usually soft real-time. A frame of audio that is played late may cause a brief audio glitch (and may cause all subsequent audio to be delayed correspondingly, causing a perception that the audio ", "canonical_url": "https://en.wikipedia.org/wiki/Real-time_computing", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:33.486323", "metadata": {"word_count": 234, "text_length": 18571}}
{"id": "wiki_onboard_processing", "query_word": "onboard processing", "title": "On-board data handling", "summary": "The on-board data handling (OBDH) subsystem of a spacecraft is the subsystem which carries and stores data between the various electronics units and the ground segment, via the telemetry, tracking and command (TT&C) subsystem. In the earlier decades of the space industry, the OBDH function was usually considered a part of the TT&C, particularly before computers became common on board. In recent years, the OBDH function has expanded, so much that it is generally considered a separate subsystem to the TT&C, which is these days concerned solely with the RF link between the ground and the spacecraft. Functions commonly performed by the OBDH are: Reception, error correction and decoding of telecommands (TCs) from the TT&C Forwarding of telecommands for execution by the target Avionics Storage of telecommands until a defined time ('time tagged' TCs) Storage of telecommands until a defined position ('position tagged' TCs) Measurement of discrete values such as voltages, temperatures, binary statuses etc. Collection of measurements made by other units and subsystems via one or more data busses, such as MIL-STD-1553 Real-time buffering of the measurements in a data pool Provision of a processing capability to achieve the aims of the mission, often using the data collected Collation and encoding of pre-defined telemetry frames Storage of telemetry frames in a mass memory Downlinking of telemetry to the ground, via the TT&C Management and distribution of time signals", "text": "The on-board data handling (OBDH) subsystem of a spacecraft is the subsystem which carries and stores data between the various electronics units and the ground segment, via the telemetry, tracking and command (TT&C) subsystem. In the earlier decades of the space industry, the OBDH function was usually considered a part of the TT&C, particularly before computers became common on board. In recent years, the OBDH function has expanded, so much that it is generally considered a separate subsystem to the TT&C, which is these days concerned solely with the RF link between the ground and the spacecraft. Functions commonly performed by the OBDH are: Reception, error correction and decoding of telecommands (TCs) from the TT&C Forwarding of telecommands for execution by the target Avionics Storage of telecommands until a defined time ('time tagged' TCs) Storage of telecommands until a defined position ('position tagged' TCs) Measurement of discrete values such as voltages, temperatures, binary statuses etc. Collection of measurements made by other units and subsystems via one or more data busses, such as MIL-STD-1553 Real-time buffering of the measurements in a data pool Provision of a processing capability to achieve the aims of the mission, often using the data collected Collation and encoding of pre-defined telemetry frames Storage of telemetry frames in a mass memory Downlinking of telemetry to the ground, via the TT&C Management and distribution of time signals Telecommand reception The OBDH receives the TCs as a synchronous PCM data stream from the TT&C Telecommand execution The desired effect of the telecommand may be just to change a value in the on-board software, or to open/close a latching relay to reconfigure or power a unit, or maybe to fire a thruster or main engine. Whichever effect is desired, the OBDH subsystem will facilitate this either by sending an electric pulse from the OBC, or by passing the command through a data bus to the unit which will eventually execute the TC. Some TCs are part of a large block of commands, used to upload updated software or data tables to fine tune the operation of the spacecraft, or to deal with anomalies. Time-tagged telecommands It is often required to delay a command's execution until a certain time. This is often because the spacecraft is not in view of the ground station, but may also be for reasons of precision. The OBC will store the TC until the required time in a queue, and then execute it. Position-tagged telecommands Similar to time-tagged commands are commands that are stored for execution until the spacecraft is at a specified position. These are most useful for Earth observation satellites, which need to start an observation over a specified point of the Earth's surface. The spacecraft, often in Sun-synchronous orbits, take a precisely repeating track over the Earth. Observations which are taken from the same position may be compared using interferometry, if they are in close enough register. The precise position required is sensed using GPS. Once a position tagged command has been executed, it may be flagged for deletion or left to execute again when the spacecraft is once again over the same point. Processing function The modern OBDH always uses an on-board computer (OBC) that is reliable, usually with redundant processors. The processing power is made available to other applications which support the spacecraft bus, such as attitude control algorithms, thermal control, failure detection isolation and recovery. If the mission itself requires only a small amount of computing power (such as a small scientific satellite) then the payload may also be controlled by the software running on the OBC, to save launch mass and the considerable expense of a dedicated payload computer. See also Spacecraft bus References External links https://ecss.nl/standard/ecss-e-st-50-04c-space-data-links-telecommand-protocols-synchronization-and-channel-coding/", "canonical_url": "https://en.wikipedia.org/wiki/On-board_data_handling", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:34.106064", "metadata": {"word_count": 229, "text_length": 3964}}
{"id": "wiki_data_compression", "query_word": "data compression", "title": "Data compression", "summary": "In information theory, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder. The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding: encoding is done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal. Data compression algorithms present a space–time complexity trade-off between the bytes needed to store or transmit information, and the computational resources needed to perform the encoding and decoding. The design of data compression schemes involves balancing the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources or time required to compress and decompress the data.", "text": "In information theory, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder. The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding: encoding is done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal. Data compression algorithms present a space–time complexity trade-off between the bytes needed to store or transmit information, and the computational resources needed to perform the encoding and decoding. The design of data compression schemes involves balancing the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources or time required to compress and decompress the data. Lossless Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding \"red pixel, red pixel, ...\" the data may be encoded as \"279 red pixels\". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy. The Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. In the mid-1980s, following work by Terry Welch, the Lempel–Ziv–Welch (LZW) algorithm rapidly became the method of choice for most general-purpose compression systems. LZW is used in GIF images, programs such as PKZIP, and hardware devices such as modems. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded. Grammar-based codes like this can compress highly repetitive input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Other practical grammar compression algorithms include Sequitur and Re-Pair. The strongest modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows–Wheeler transform can also be viewed as an indirect form of statistical modelling. In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression compared to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was in an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.263, H.264/MPEG-4 AVC and HEVC for video coding. Archive software typically has the ability to adjust the \"dictionary size\", where a larger size demands more random-access memory during compression and decompression, but compresses stronger, especially on repeating patterns in files' content. Lossy In the late 1980s, digital images became more common, and standards for lossless image compression emerged. In the early 1990s, lossy compression methods began to be widely used. In these schemes, some loss of information is accepted as dropping nonessential detail can save storage space. There is a corresponding trade-off between preserving information and reducing size. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. A number of popular compression formats exploit these perceptual differences, including psychoacoustics for sound, and psychovisuals for images and video. Most forms of lossy compression are based on transform coding, especially the discrete cosine transform (DCT). It was first proposed in 1972 by Nasir Ahmed, who then developed a working algorithm with T. Natarajan and K. R. Rao in 1973, before introducing it in January 1974. DCT is the most widely used lossy compression method, and is used in multimedia formats for images (such as JPEG and HEIF), video (such as MPEG, AVC and HEVC) and audio (such as MP3, AAC and Vorbis). Lossy image compression is used in digital cameras, to increase storage capacities. Similarly, DVDs, Blu-ray and streaming video use lossy video coding formats. Lossy compression is extensively used in video. In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding is distinguished as a separate discipline from general-purpose audio compression. Speech coding is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players. Lossy compression can cause generation loss. Theory The theoretical basis for compression is provided by information theory and, more specifically, Shannon's source coding theorem; domain-specific theories include algorithmic information theory for lossless compression and rate–distortion theory for lossy compression. These areas of study were essentially created by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Other topics associated with compression include coding theory and statistical inference. Machine learning There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\". An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM. According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form. Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC. Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression. In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression. Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.", "canonical_url": "https://en.wikipedia.org/wiki/Data_compression", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:35.067518", "metadata": {"word_count": 216, "text_length": 33314}}
{"id": "wiki_lossless_compression", "query_word": "lossless compression", "title": "Lossless compression", "summary": "Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data with no loss of information. Lossless compression is possible because most real-world data exhibits statistical redundancy. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though usually with greatly improved compression rates (and therefore reduced media sizes). By operation of the pigeonhole principle, no lossless compression algorithm can shrink the size of all possible data: Some data will get longer by at least one symbol or bit. Compression algorithms are usually effective for human- and machine-readable documents and cannot shrink the size of random data that contain no redundancy. Different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain. Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by MP3 encoders and other lossy audio encoders). Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Common examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.", "text": "Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data with no loss of information. Lossless compression is possible because most real-world data exhibits statistical redundancy. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though usually with greatly improved compression rates (and therefore reduced media sizes). By operation of the pigeonhole principle, no lossless compression algorithm can shrink the size of all possible data: Some data will get longer by at least one symbol or bit. Compression algorithms are usually effective for human- and machine-readable documents and cannot shrink the size of random data that contain no redundancy. Different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain. Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by MP3 encoders and other lossy audio encoders). Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Common examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary. Techniques Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that \"probable\" (i.e. frequently encountered) data will produce shorter output than \"improbable\" data. The primary encoding algorithms used to produce bit sequences are Huffman coding (also used by the deflate algorithm) and arithmetic coding. Arithmetic coding achieves compression rates close to the best possible for a particular statistical model, which is given by the information entropy, whereas Huffman compression is simpler and faster but produces poor results for models that deal with symbol probabilities close to 1. There are two primary ways of constructing statistical models: in a static model, the data is analyzed and a model is constructed, then this model is stored with the compressed data. This approach is simple and modular, but has the disadvantage that the model itself can be expensive to store, and also that it forces using a single model for all data being compressed, and so performs poorly on files that contain heterogeneous data. Adaptive models dynamically update the model as the data is compressed. Both the encoder and decoder begin with a trivial model, yielding poor compression of initial data, but as they learn more about the data, performance improves. Most popular types of compression used in practice now use adaptive coders. Lossless compression methods may be categorized according to the type of data they are designed to compress. While, in principle, any general-purpose lossless compression algorithm (general-purpose meaning that they can accept any bitstring) can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. Many of the lossless compression techniques used for text also work reasonably well for indexed-color images. Multimedia These techniques take advantage of the specific characteristics of images such as the common phenomenon of contiguous 2-D areas of similar tones. Every pixel but the first is replaced by the difference to its left neighbor. This leads to small values having a much higher probability than large values. This is often also applied to sound files, and can compress files that contain mostly low frequencies and low volumes. For images, this step can be repeated by taking the difference to the top pixel, and then in videos, the difference to the pixel in the next frame can be taken. The adaptive encoding uses the probabilities from the previous sample in sound encoding, from the left and upper pixel in image encoding, and additionally from the previous frame in video encoding. In the wavelet transformation, the probabilities are also passed through the hierarchy. Historical legal issues Many of these methods are implemented in open-source and proprietary tools, particularly LZW and its variants. Some algorithms are patented in the United States and other countries and their legal usage requires licensing by the patent holder. Because of patents on certain kinds of LZW compression, and in particular licensing practices by patent holder Unisys that many developers considered abusive, some open source proponents encouraged people to avoid using the Graphics Interchange Format (GIF) for compressing still image files in favor of Portable Network Graphics (PNG), which combines the LZ77-based deflate algorithm with a selection of domain-specific prediction filters. However, the patents on LZW expired on June 20, 2003. Many of the lossless compression techniques used for text also work reasonably well for indexed images, but there are other techniques that do not work for typical text that are useful for some images (particularly simple bitmaps), and other techniques that take advantage of the specific characteristics of images (such as the common phenomenon of contiguous 2-D areas of similar tones, and the fact that color images usually have a preponderance of a limited range of colors out of those representable in the color space). As mentioned previously, lossless sound compression is a somewhat specialized area. Lossless sound compression algorithms can take advantage of the repeating patterns shown by the wave-like nature of the data‍—‍essentially using autoregressive models to predict the \"next\" value and encoding the (possibly small) difference between the expected value and the actual data. If the difference between the predicted and the actual data (called the error) tends to be small, then certain difference values (like 0, +1, −1 etc. on sample values) become very frequent, which can be exploited by encoding them in few output bits. It is sometimes beneficial to compress only the differences between two versions of a file (or, in video compression, of successive images within a sequence). This is called delta encoding (from the Greek letter Δ, which in mathematics, denotes a difference), but the term is typically only used if both versions are meaningful outside compression and decompression. For example, while the process of compressing the error in the above-mentioned lossless audio compression scheme could be described as delta encoding from the approximated sound wave to the original sound wave, the approximated version of the sound wave is not meaningful in any other context. Methods No lossless compression algorithm can efficiently compress all possible data (see § Limitations for more on this). For this reason, many different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain. Some of the most common lossless compression algorithms are listed below. General purpose ANS – Entropy encoding, used by LZFSE and Zstandard Arithmetic coding – Entropy encoding Burrows–Wheeler transform reversible transform for making textual data more compressible, used by bzip2 Huffman coding – Entropy encoding, pairs well with other algorithms Lempel-Ziv compression (LZ77 and LZ78) – Dictionary-based algorithm that forms the basis for many other algorithms Deflate – Combines LZ77 compression with Huffman coding, used by ZIP, gzip, and PNG images Lempel–Ziv–Markov chain algorithm (LZMA) – Very high compression ratio, used by 7zip and xz Lempel–Ziv–Storer–Szymanski (LZSS) – Used by WinRAR in tandem with Huffman coding Lempel–Ziv–Welch (LZW) – Used by GIF images and Unix's compress utility Prediction by partial matching (PPM) – Optimized for compressing plain text Run-length encoding (RLE) – Simple scheme that provides good compression of data containing many runs of the same value Audio Adaptive Transform Acoustic Coding (ATRAC) Apple Lossless (ALAC – Apple Lossless Audio Codec) Audio Lossless Coding (also known as MPEG-4 ALS) Direct Stream Transfer (DST) Dolby TrueHD DTS-HD Master Audio Free Lossless Audio Codec (FLAC) Meridian Lossless Packing (MLP) Monkey's Audio (Monkey's Audio APE) MPEG-4 SLS (also known as HD-AAC) OptimFROG Original Sound Quality (OSQ) RealPlayer (RealAudio Lossless) Shorten (SHN) TTA (True Audio Lossless) WavPack (WavPack lossless) Windows Media Audio 9 Lossless (WMA Lossless) Raster graphics Lossless only encoding BMP PNG – Portable Network Graphics GIF – Graphics Interchange Format Lossy and Lossless encoding options AVIF – AV1 Image File Format FLIF – Free Lossless Image Format HEIF – High Efficiency Image File Format, using HEVC ILBM – (RLE compression of Amiga IFF images) JBIG2 – compression of B&W images JPEG 2000 – (via Le Gall–Tabatabai 5/3 reversible integer wavelet transform) JPEG-LS JPEG XL JPEG XR – formerly WMPhoto and HD Photo LDCT – Discrete Cosine Transform PCX – PiCture eXchange QOI – Quite OK Image Format TGA – Truevision TGA TIFF – Tag Image File Format WebP 3D Graphics OpenCTM – Lossless compression of 3D triangle meshes Video See list of lossless video codecs", "canonical_url": "https://en.wikipedia.org/wiki/Lossless_compression", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:35.957158", "metadata": {"word_count": 302, "text_length": 22287}}
{"id": "wiki_lossy_compression", "query_word": "lossy compression", "title": "Lossy compression", "summary": "In information technology, lossy compression or irreversible compression is the class of data compression methods that uses inexact approximations and partial data discarding to represent the content. These techniques are used to reduce data size for storing, handling, and transmitting content. Higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression (reversible data compression) which does not degrade the data. The amount of data reduction possible using lossy compression is much higher than using lossless techniques. Well-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication or to reduce transmission times or storage needs). The most widely used lossy compression algorithm is the discrete cosine transform (DCT), first published by Nasir Ahmed, T. Natarajan and K. R. Rao in 1974. Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. It can be advantageous to make a master lossless file which can then be used to produce additional copies from. This allows one to avoid basing new compressed copies on a lossy source file, which would yield additional artifacts and further unnecessary information loss.", "text": "In information technology, lossy compression or irreversible compression is the class of data compression methods that uses inexact approximations and partial data discarding to represent the content. These techniques are used to reduce data size for storing, handling, and transmitting content. Higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression (reversible data compression) which does not degrade the data. The amount of data reduction possible using lossy compression is much higher than using lossless techniques. Well-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication or to reduce transmission times or storage needs). The most widely used lossy compression algorithm is the discrete cosine transform (DCT), first published by Nasir Ahmed, T. Natarajan and K. R. Rao in 1974. Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. It can be advantageous to make a master lossless file which can then be used to produce additional copies from. This allows one to avoid basing new compressed copies on a lossy source file, which would yield additional artifacts and further unnecessary information loss. Types It is possible to compress many types of digital data in a way that reduces the size of a computer file needed to store it, or the bandwidth needed to transmit it, with no loss of the full information contained in the original file. A picture, for example, is converted to a digital file by considering it to be an array of dots and specifying the color and brightness of each dot. If the picture contains an area of the same color, it can be compressed without loss by saying \"200 red dots\" instead of \"red dot, red dot, ...(197 more times)..., red dot.\" The original data contains a certain amount of information, and there is a lower bound to the size of a file that can still carry all the information. Basic information theory says that there is an absolute limit in reducing the size of this data. When data is compressed, its entropy increases, and it cannot increase indefinitely. For example, a compressed ZIP file is smaller than its original, but repeatedly compressing the same file will not reduce the size to nothing. Most compression algorithms can recognize when further compression would be pointless and would in fact increase the size of the data. In many cases, files or data streams contain more information than is needed. For example, a picture may have more detail than the eye can distinguish when reproduced at the largest size intended; likewise, an audio file does not need a lot of fine detail during a very loud passage. Developing lossy compression techniques as closely matched to human perception as possible is a complex task. Sometimes the ideal is a file that provides exactly the same perception as the original, with as much digital information as possible removed; other times, perceptible loss of quality is considered a valid tradeoff. The terms \"irreversible\" and \"reversible\" are preferred over \"lossy\" and \"lossless\" respectively for some applications, such as medical image compression, to circumvent the negative implications of \"loss\". The type and amount of loss can affect the utility of the images. Artifacts or undesirable effects of compression may be clearly discernible yet the result still useful for the intended purpose. Or lossy compressed images may be 'visually lossless', or in the case of medical images, so-called diagnostically acceptable irreversible compression (DAIC) may have been applied. Transform coding Some forms of lossy compression can be thought of as an application of transform coding, which is a type of data compression used for digital images, digital audio signals, and digital video. The transformation is typically used to enable better (more targeted) quantization. Knowledge of the application is used to choose information to discard, thereby lowering its bandwidth. The remaining information can then be compressed via a variety of methods. When the output is decoded, the result may not be identical to the original input, but is expected to be close enough for the purpose of the application. The most common form of lossy compression is a transform coding method, the discrete cosine transform (DCT), which was first published by Nasir Ahmed, T. Natarajan and K. R. Rao in 1974. DCT is the most widely used form of lossy compression, for popular image compression formats (such as JPEG), video coding standards (such as MPEG and H.264/AVC) and audio compression formats (such as MP3 and AAC). In the case of audio data, a popular form of transform coding is perceptual coding, which transforms the raw data to a domain that more accurately reflects the information content. For example, rather than expressing a sound file as the amplitude levels over time, one may express it as the frequency spectrum over time, which corresponds more accurately to human audio perception. While data reduction (compression, be it lossy or lossless) is a main goal of transform coding, it also allows other goals: one may represent data more accurately for the original amount of space – for example, in principle, if one starts with an analog or high-resolution digital master, an MP3 file of a given size should provide a better representation than a raw uncompressed audio in WAV or AIFF file of the same size. This is because uncompressed audio can only reduce file size by lowering bit rate or depth, whereas compressing audio can reduce size while maintaining bit rate and depth. This compression becomes a selective loss of the least significant data, rather than losing data across the board. Further, a transform coding may provide a better domain for manipulating or otherwise editing the data – for example, equalization of audio is most naturally expressed in the frequency domain (boost the bass, for instance) rather than in the raw time domain. From this point of view, perceptual encoding is not essentially about discarding data, but rather about a better representation of data. Another use is for backward compatibility and graceful degradation: in color television, encoding color via a luminance-chrominance transform domain (such as YUV) means that black-and-white sets display the luminance, while ignoring the color information. Another example is chroma subsampling: the use of color spaces such as YIQ, used in NTSC, allow one to reduce the resolution on the components to accord with human perception – humans have highest resolution for black-and-white (luma), lower resolution for mid-spectrum colors like yellow and green, and lowest for red and blues – thus NTSC displays approximately 350 pixels of luma per scanline, 150 pixels of yellow vs. green, and 50 pixels of blue vs. red, which are proportional to human sensitivity to each component. Information loss Lossy compression formats suffer from generation loss: repeatedly compressing and decompressing the file will cause it to progressively lose quality. This is in contrast with lossless data compression, where data will not be lost via the use of such a procedure. Information-theoretical foundations for lossy data compression are provided by rate-distortion theory. Much like the use of probability in optimal coding theory, rate-distortion theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment. There are two basic lossy compression schemes: In lossy transform codecs, samples of picture or sound are taken, chopped into small segments, transformed into a new basis space, and quantized. The resulting quantized values are then entropy coded. In lossy predictive codecs, previous and/or subsequent decoded data is used to predict the current sound sample or image frame. The error between the predicted data and the real data, together with any extra information needed to reproduce the prediction, is then quantized and coded. In some systems the two techniques are combined, with transform codecs being used to compress the error signals generated by the predictive stage. Comparison The advantage of lossy methods over lossless methods is that in some cases a lossy method can produce a much smaller compressed file than any lossless method, while still meeting the requirements of the application. Lossy methods are most often used for compressing sound, images or videos. This is because these types of data are intended for human interpretation where the mind can easily \"fill in the blanks\" or see past very minor errors or inconsistencies – ideally lossy compression is transparent (imperceptible), which can be verified via an ABX test. Data files using lossy compression are smaller in size and thus cost less to store and to transmit over the Internet, a crucial consideration for streaming video services such as Netflix and streaming audio services such as Spotify. Transparency When a user acquires a lossily compressed file, (for example, to reduce download time) the retrieved file can be quite different from the original at the bit level while being indistinguishable to the human ear or eye for most practical purposes. Many compression methods focus on the idiosyncrasies of human physiology, taking into account, for instance, that the human eye can see only certain wavelengths of light. The psychoacoustic model describes how sound can be highly compressed without degrading perceived quality. Flaws caused by lossy compression that are noticeable to the human eye or ear are known as compression artifacts. Compressio", "canonical_url": "https://en.wikipedia.org/wiki/Lossy_compression", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:36.656006", "metadata": {"word_count": 238, "text_length": 18842}}
{"id": "wiki_electromagnetic_spectrum", "query_word": "electromagnetic spectrum", "title": "Electromagnetic spectrum", "summary": "The electromagnetic spectrum is the full range of electromagnetic radiation, organized by frequency or wavelength. The spectrum is divided into separate bands, with different names for the electromagnetic waves within each band. From low to high frequency these are: radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays. The electromagnetic waves in each of these bands have different characteristics, such as how they are produced, how they interact with matter, and their practical applications. Radio waves, at the low-frequency end of the spectrum, have the lowest photon energy and the longest wavelengths—thousands of kilometers, or more. They can be emitted and received by antennas, and pass through the atmosphere, foliage, and most building materials. Gamma rays, at the high-frequency end of the spectrum, have the highest photon energies and the shortest wavelengths—much smaller than an atomic nucleus. Gamma rays, X-rays, and extreme ultraviolet rays are called ionizing radiation because their high photon energy is able to ionize atoms, causing chemical reactions. Longer-wavelength radiation such as visible light is nonionizing; the photons do not have sufficient energy to ionize atoms. Throughout most of the electromagnetic spectrum, spectroscopy can be used to separate waves of different frequencies, so that the intensity of the radiation can be measured as a function of frequency or wavelength. Spectroscopy is used to study the interactions of electromagnetic waves with matter.", "text": "The electromagnetic spectrum is the full range of electromagnetic radiation, organized by frequency or wavelength. The spectrum is divided into separate bands, with different names for the electromagnetic waves within each band. From low to high frequency these are: radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays. The electromagnetic waves in each of these bands have different characteristics, such as how they are produced, how they interact with matter, and their practical applications. Radio waves, at the low-frequency end of the spectrum, have the lowest photon energy and the longest wavelengths—thousands of kilometers, or more. They can be emitted and received by antennas, and pass through the atmosphere, foliage, and most building materials. Gamma rays, at the high-frequency end of the spectrum, have the highest photon energies and the shortest wavelengths—much smaller than an atomic nucleus. Gamma rays, X-rays, and extreme ultraviolet rays are called ionizing radiation because their high photon energy is able to ionize atoms, causing chemical reactions. Longer-wavelength radiation such as visible light is nonionizing; the photons do not have sufficient energy to ionize atoms. Throughout most of the electromagnetic spectrum, spectroscopy can be used to separate waves of different frequencies, so that the intensity of the radiation can be measured as a function of frequency or wavelength. Spectroscopy is used to study the interactions of electromagnetic waves with matter. History and discovery Humans have always been aware of visible light and radiant heat but for most of history it was not known that these phenomena were connected or were representatives of a more extensive principle. The ancient Greeks recognized that light traveled in straight lines and studied some of its properties, including reflection and refraction. Light was intensively studied from the beginning of the 17th century leading to the invention of important instruments like the telescope and microscope. Isaac Newton was the first to use the term spectrum for the range of colours that white light could be split into with a prism. Starting in 1666, Newton showed that these colours were intrinsic to light and could be recombined into white light. A debate arose over whether light had a wave nature or a particle nature with René Descartes, Robert Hooke and Christiaan Huygens favouring a wave description and Newton favouring a particle description. Huygens in particular had a well developed theory from which he was able to derive the laws of reflection and refraction. Around 1801, Thomas Young measured the wavelength of a light beam with his two-slit experiment thus conclusively demonstrating that light was a wave. In 1800, William Herschel discovered infrared radiation. He was studying the temperature of different colours by moving a thermometer through light split by a prism. He noticed that the highest temperature was beyond red. He theorized that this temperature change was due to \"calorific rays\", a type of light ray that could not be seen. The next year, Johann Ritter, working at the other end of the spectrum, noticed what he called \"chemical rays\" (invisible light rays that induced certain chemical reactions). These behaved similarly to visible violet light rays, but were beyond them in the spectrum. They were later renamed ultraviolet radiation. The study of electromagnetism began in 1820 when Hans Christian Ørsted discovered that electric currents produce magnetic fields (Oersted's law). Light was first linked to electromagnetism in 1845, when Michael Faraday noticed that the polarization of light traveling through a transparent material responded to a magnetic field (see Faraday effect). During the 1860s, James Clerk Maxwell developed four partial differential equations (Maxwell's equations) for the electromagnetic field. Two of these equations predicted the possibility and behavior of waves in the field. Analyzing the speed of these theoretical waves, Maxwell realized that they must travel at a speed that was about the known speed of light. This startling coincidence in value led Maxwell to make the inference that light itself is a type of electromagnetic wave. Maxwell's equations predicted an infinite range of frequencies of electromagnetic waves, all traveling at the speed of light. This was the first indication of the existence of the entire electromagnetic spectrum. Maxwell's predicted waves included waves at very low frequencies compared to infrared, which in theory might be created by oscillating charges in an ordinary electrical circuit of a certain type. Attempting to prove Maxwell's equations and detect such low frequency electromagnetic radiation, in 1886, the physicist Heinrich Hertz built an apparatus to generate and detect what are now called radio waves. Hertz found the waves and was able to infer (by measuring their wavelength and multiplying it by their frequency) that they traveled at the speed of light. Hertz also demonstrated that the new radiation could be both reflected and refracted by various dielectric media, in the same manner as light. For example, Hertz was able to focus the waves using a lens made of tree resin. In a later experiment, Hertz similarly produced and measured the properties of microwaves. These new types of waves paved the way for inventions such as the wireless telegraph and the radio. In 1895, Wilhelm Röntgen noticed a new type of radiation emitted during an experiment with an evacuated tube subjected to a high voltage. He called this radiation \"x-rays\" and found that they were able to travel through parts of the human body but were reflected or stopped by denser matter such as bones. Before long, many uses were found for this radiography. The last portion of the electromagnetic spectrum was filled in with the discovery of gamma rays. In 1900, Paul Villard was studying the radioactive emissions of radium when he identified a new type of radiation that he at first thought consisted of particles similar to known alpha and beta particles, but with the power of being far more penetrating than either. However, in 1910, British physicist William Henry Bragg demonstrated that gamma rays are electromagnetic radiation, not particles, and in 1914, Ernest Rutherford (who had named them gamma rays in 1903 when he realized that they were fundamentally different from charged alpha and beta particles) and Edward Andrade measured their wavelengths, and found that gamma rays were similar to X-rays, but with shorter wavelengths. The wave-particle debate was rekindled in 1901 when Max Planck discovered that light is absorbed only in discrete \"quanta\", now called photons, implying that light has a particle nature. This idea was made explicit by Albert Einstein in 1905, but never accepted by Planck and many other contemporaries. The modern position of science is that electromagnetic radiation has both a wave and a particle nature, the wave-particle duality. The contradictions arising from this position are still being debated by scientists and philosophers. Range Electromagnetic waves are typically described by any of the following three physical properties: the frequency f, wavelength λ, or photon energy E. Frequencies observed in astronomy range from 2.4×1023 Hz (1 GeV gamma rays) down to the local plasma frequency of the ionized interstellar medium (~1 kHz). Wavelength is inversely proportional to the wave frequency, so gamma rays have very short wavelengths that are fractions of the size of atoms, whereas wavelengths on the opposite end of the spectrum can be indefinitely long. Photon energy is directly proportional to the wave frequency, so gamma ray photons have the highest energy (around a billion electron volts), while radio wave photons have very low energy (around a femtoelectronvolt). These relations are illustrated by the following equations: f = c λ , f = E h , E = h c λ , {\\displaystyle f={\\frac {c}{\\lambda }},\\quad f={\\frac {E}{h}},\\quad E={\\frac {hc}{\\lambda }},} where: c is the speed of light in vacuum h is the Planck constant. Whenever electromagnetic waves travel in a medium with matter, their wavelength is decreased. Wavelengths of electromagnetic radiation, whatever medium they are traveling through, are usually quoted in terms of the vacuum wavelength, although this is not always explicitly stated. Generally, electromagnetic radiation is classified by wavelength into radio wave, microwave, infrared, visible light, ultraviolet, X-rays and gamma rays. The behavior of EM radiation depends on its wavelength. When EM radiation interacts with single atoms and molecules, its behavior also depends on the amount of energy per quantum (photon) it carries. Spectroscopy can detect a much wider region of the EM spectrum than the visible wavelength range of 400 nm to 700 nm in a vacuum. A common laboratory spectroscope can detect wavelengths from 2 nm to 2500 nm. Detailed information about the physical properties of objects, gases, or even stars can be obtained from this type of device. Spectroscopes are widely used in astrophysics. For example, many hydrogen atoms emit a radio wave photon that has a wavelength of 21.12 cm. Also, frequencies of 30 Hz and below can be produced by and are important in the study of certain stellar nebulae and frequencies as high as 2.9×1027 Hz have been detected from astrophysical sources. Regions The types of electromagnetic radiation are broadly classified into the following classes (regions, bands or types): Gamma radiation X-ray radiation Ultraviolet radiation Visible light (light that humans can see) Infrared radiation Microwave radiation Radio waves This classification goes in the increasing order of wavelength, which is characteristic of the type of radiation. There are no precisely defined boundaries between the bands of the electromagnetic spectrum; rather they fade into each other like the bands in a rainbow. Radiation o", "canonical_url": "https://en.wikipedia.org/wiki/Electromagnetic_spectrum", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:38.120737", "metadata": {"word_count": 226, "text_length": 25121}}
{"id": "wiki_spectral_range", "query_word": "spectral range", "title": "Electromagnetic spectrum", "summary": "The electromagnetic spectrum is the full range of electromagnetic radiation, organized by frequency or wavelength. The spectrum is divided into separate bands, with different names for the electromagnetic waves within each band. From low to high frequency these are: radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays. The electromagnetic waves in each of these bands have different characteristics, such as how they are produced, how they interact with matter, and their practical applications. Radio waves, at the low-frequency end of the spectrum, have the lowest photon energy and the longest wavelengths—thousands of kilometers, or more. They can be emitted and received by antennas, and pass through the atmosphere, foliage, and most building materials. Gamma rays, at the high-frequency end of the spectrum, have the highest photon energies and the shortest wavelengths—much smaller than an atomic nucleus. Gamma rays, X-rays, and extreme ultraviolet rays are called ionizing radiation because their high photon energy is able to ionize atoms, causing chemical reactions. Longer-wavelength radiation such as visible light is nonionizing; the photons do not have sufficient energy to ionize atoms. Throughout most of the electromagnetic spectrum, spectroscopy can be used to separate waves of different frequencies, so that the intensity of the radiation can be measured as a function of frequency or wavelength. Spectroscopy is used to study the interactions of electromagnetic waves with matter.", "text": "The electromagnetic spectrum is the full range of electromagnetic radiation, organized by frequency or wavelength. The spectrum is divided into separate bands, with different names for the electromagnetic waves within each band. From low to high frequency these are: radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays. The electromagnetic waves in each of these bands have different characteristics, such as how they are produced, how they interact with matter, and their practical applications. Radio waves, at the low-frequency end of the spectrum, have the lowest photon energy and the longest wavelengths—thousands of kilometers, or more. They can be emitted and received by antennas, and pass through the atmosphere, foliage, and most building materials. Gamma rays, at the high-frequency end of the spectrum, have the highest photon energies and the shortest wavelengths—much smaller than an atomic nucleus. Gamma rays, X-rays, and extreme ultraviolet rays are called ionizing radiation because their high photon energy is able to ionize atoms, causing chemical reactions. Longer-wavelength radiation such as visible light is nonionizing; the photons do not have sufficient energy to ionize atoms. Throughout most of the electromagnetic spectrum, spectroscopy can be used to separate waves of different frequencies, so that the intensity of the radiation can be measured as a function of frequency or wavelength. Spectroscopy is used to study the interactions of electromagnetic waves with matter. History and discovery Humans have always been aware of visible light and radiant heat but for most of history it was not known that these phenomena were connected or were representatives of a more extensive principle. The ancient Greeks recognized that light traveled in straight lines and studied some of its properties, including reflection and refraction. Light was intensively studied from the beginning of the 17th century leading to the invention of important instruments like the telescope and microscope. Isaac Newton was the first to use the term spectrum for the range of colours that white light could be split into with a prism. Starting in 1666, Newton showed that these colours were intrinsic to light and could be recombined into white light. A debate arose over whether light had a wave nature or a particle nature with René Descartes, Robert Hooke and Christiaan Huygens favouring a wave description and Newton favouring a particle description. Huygens in particular had a well developed theory from which he was able to derive the laws of reflection and refraction. Around 1801, Thomas Young measured the wavelength of a light beam with his two-slit experiment thus conclusively demonstrating that light was a wave. In 1800, William Herschel discovered infrared radiation. He was studying the temperature of different colours by moving a thermometer through light split by a prism. He noticed that the highest temperature was beyond red. He theorized that this temperature change was due to \"calorific rays\", a type of light ray that could not be seen. The next year, Johann Ritter, working at the other end of the spectrum, noticed what he called \"chemical rays\" (invisible light rays that induced certain chemical reactions). These behaved similarly to visible violet light rays, but were beyond them in the spectrum. They were later renamed ultraviolet radiation. The study of electromagnetism began in 1820 when Hans Christian Ørsted discovered that electric currents produce magnetic fields (Oersted's law). Light was first linked to electromagnetism in 1845, when Michael Faraday noticed that the polarization of light traveling through a transparent material responded to a magnetic field (see Faraday effect). During the 1860s, James Clerk Maxwell developed four partial differential equations (Maxwell's equations) for the electromagnetic field. Two of these equations predicted the possibility and behavior of waves in the field. Analyzing the speed of these theoretical waves, Maxwell realized that they must travel at a speed that was about the known speed of light. This startling coincidence in value led Maxwell to make the inference that light itself is a type of electromagnetic wave. Maxwell's equations predicted an infinite range of frequencies of electromagnetic waves, all traveling at the speed of light. This was the first indication of the existence of the entire electromagnetic spectrum. Maxwell's predicted waves included waves at very low frequencies compared to infrared, which in theory might be created by oscillating charges in an ordinary electrical circuit of a certain type. Attempting to prove Maxwell's equations and detect such low frequency electromagnetic radiation, in 1886, the physicist Heinrich Hertz built an apparatus to generate and detect what are now called radio waves. Hertz found the waves and was able to infer (by measuring their wavelength and multiplying it by their frequency) that they traveled at the speed of light. Hertz also demonstrated that the new radiation could be both reflected and refracted by various dielectric media, in the same manner as light. For example, Hertz was able to focus the waves using a lens made of tree resin. In a later experiment, Hertz similarly produced and measured the properties of microwaves. These new types of waves paved the way for inventions such as the wireless telegraph and the radio. In 1895, Wilhelm Röntgen noticed a new type of radiation emitted during an experiment with an evacuated tube subjected to a high voltage. He called this radiation \"x-rays\" and found that they were able to travel through parts of the human body but were reflected or stopped by denser matter such as bones. Before long, many uses were found for this radiography. The last portion of the electromagnetic spectrum was filled in with the discovery of gamma rays. In 1900, Paul Villard was studying the radioactive emissions of radium when he identified a new type of radiation that he at first thought consisted of particles similar to known alpha and beta particles, but with the power of being far more penetrating than either. However, in 1910, British physicist William Henry Bragg demonstrated that gamma rays are electromagnetic radiation, not particles, and in 1914, Ernest Rutherford (who had named them gamma rays in 1903 when he realized that they were fundamentally different from charged alpha and beta particles) and Edward Andrade measured their wavelengths, and found that gamma rays were similar to X-rays, but with shorter wavelengths. The wave-particle debate was rekindled in 1901 when Max Planck discovered that light is absorbed only in discrete \"quanta\", now called photons, implying that light has a particle nature. This idea was made explicit by Albert Einstein in 1905, but never accepted by Planck and many other contemporaries. The modern position of science is that electromagnetic radiation has both a wave and a particle nature, the wave-particle duality. The contradictions arising from this position are still being debated by scientists and philosophers. Range Electromagnetic waves are typically described by any of the following three physical properties: the frequency f, wavelength λ, or photon energy E. Frequencies observed in astronomy range from 2.4×1023 Hz (1 GeV gamma rays) down to the local plasma frequency of the ionized interstellar medium (~1 kHz). Wavelength is inversely proportional to the wave frequency, so gamma rays have very short wavelengths that are fractions of the size of atoms, whereas wavelengths on the opposite end of the spectrum can be indefinitely long. Photon energy is directly proportional to the wave frequency, so gamma ray photons have the highest energy (around a billion electron volts), while radio wave photons have very low energy (around a femtoelectronvolt). These relations are illustrated by the following equations: f = c λ , f = E h , E = h c λ , {\\displaystyle f={\\frac {c}{\\lambda }},\\quad f={\\frac {E}{h}},\\quad E={\\frac {hc}{\\lambda }},} where: c is the speed of light in vacuum h is the Planck constant. Whenever electromagnetic waves travel in a medium with matter, their wavelength is decreased. Wavelengths of electromagnetic radiation, whatever medium they are traveling through, are usually quoted in terms of the vacuum wavelength, although this is not always explicitly stated. Generally, electromagnetic radiation is classified by wavelength into radio wave, microwave, infrared, visible light, ultraviolet, X-rays and gamma rays. The behavior of EM radiation depends on its wavelength. When EM radiation interacts with single atoms and molecules, its behavior also depends on the amount of energy per quantum (photon) it carries. Spectroscopy can detect a much wider region of the EM spectrum than the visible wavelength range of 400 nm to 700 nm in a vacuum. A common laboratory spectroscope can detect wavelengths from 2 nm to 2500 nm. Detailed information about the physical properties of objects, gases, or even stars can be obtained from this type of device. Spectroscopes are widely used in astrophysics. For example, many hydrogen atoms emit a radio wave photon that has a wavelength of 21.12 cm. Also, frequencies of 30 Hz and below can be produced by and are important in the study of certain stellar nebulae and frequencies as high as 2.9×1027 Hz have been detected from astrophysical sources. Regions The types of electromagnetic radiation are broadly classified into the following classes (regions, bands or types): Gamma radiation X-ray radiation Ultraviolet radiation Visible light (light that humans can see) Infrared radiation Microwave radiation Radio waves This classification goes in the increasing order of wavelength, which is characteristic of the type of radiation. There are no precisely defined boundaries between the bands of the electromagnetic spectrum; rather they fade into each other like the bands in a rainbow. Radiation o", "canonical_url": "https://en.wikipedia.org/wiki/Electromagnetic_spectrum", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:39.115248", "metadata": {"word_count": 226, "text_length": 25121}}
{"id": "wiki_visible_light", "query_word": "visible light", "title": "Light", "summary": "Light, visible light, or visible radiation is electromagnetic radiation that can be perceived by the human eye. Visible light spans the visible spectrum and is usually defined as having wavelengths in the range of 400–700 nanometres (nm), corresponding to frequencies of 750–420 terahertz. The visible band sits adjacent to the infrared (with longer wavelengths and lower frequencies) and the ultraviolet (with shorter wavelengths and higher frequencies), called collectively optical radiation. In physics, the term \"light\" may refer more broadly to electromagnetic radiation of any wavelength, whether visible or not. In this sense, gamma rays, X-rays, microwaves and radio waves are also light. The primary properties of light are intensity, propagation direction, frequency or wavelength spectrum, and polarization. Its speed in vacuum, 299792458 m/s, is one of the fundamental constants of nature. All electromagnetic radiation exhibits some properties of both particles and waves. Single, massless elementary particles, or quanta, of light called photons can be detected with specialized equipment; phenomena like interference are described by waves. Most everyday interactions with light can be understood using geometrical optics; quantum optics, is an important research area in modern physics. The main source of natural light on Earth is the Sun. Historically, another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight.", "text": "Light, visible light, or visible radiation is electromagnetic radiation that can be perceived by the human eye. Visible light spans the visible spectrum and is usually defined as having wavelengths in the range of 400–700 nanometres (nm), corresponding to frequencies of 750–420 terahertz. The visible band sits adjacent to the infrared (with longer wavelengths and lower frequencies) and the ultraviolet (with shorter wavelengths and higher frequencies), called collectively optical radiation. In physics, the term \"light\" may refer more broadly to electromagnetic radiation of any wavelength, whether visible or not. In this sense, gamma rays, X-rays, microwaves and radio waves are also light. The primary properties of light are intensity, propagation direction, frequency or wavelength spectrum, and polarization. Its speed in vacuum, 299792458 m/s, is one of the fundamental constants of nature. All electromagnetic radiation exhibits some properties of both particles and waves. Single, massless elementary particles, or quanta, of light called photons can be detected with specialized equipment; phenomena like interference are described by waves. Most everyday interactions with light can be understood using geometrical optics; quantum optics, is an important research area in modern physics. The main source of natural light on Earth is the Sun. Historically, another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight. Electromagnetic spectrum and visible light Generally, electromagnetic radiation (EMR) is classified by wavelength into radio waves, microwaves, infrared, the visible spectrum that we perceive as light, ultraviolet, X-rays and gamma rays. The designation \"radiation\" excludes static electric, magnetic and near fields. The behavior of EMR depends on its wavelength. Higher frequencies have shorter wavelengths and lower frequencies have longer wavelengths. When EMR interacts with single atoms and molecules, its behavior depends on the amount of energy per quantum it carries. EMR in the visible light region consists of quanta (called photons) that are at the lower end of the energies that are capable of causing electronic excitation within molecules, which leads to changes in the bonding or chemistry of the molecule. At the lower end of the visible light spectrum, EMR becomes invisible to humans (infrared) because its photons no longer have enough individual energy to cause a lasting molecular change (a change in conformation) in the visual molecule retinal in the human retina, which change triggers the sensation of vision. There exist animals that are sensitive to various types of infrared, but not by means of quantum-absorption. Infrared sensing in snakes depends on a kind of natural thermal imaging, in which tiny packets of cellular water are raised in temperature by the infrared radiation. EMR in this range causes molecular vibration and heating effects, which is how these animals detect it. Above the frequency range of visible light, ultraviolet light becomes invisible to humans, mostly because it is absorbed by the cornea with wavelengths shorter than 360 nm and the internal lens at wavelengths shorter than 400 nm. Furthermore, the rods and cones located in the retina of the human eye cannot detect the very short (shorter than 360 nm) ultraviolet wavelengths and are in fact damaged by ultraviolet. Many animals with eyes that do not require lenses (such as insects and shrimp) are able to detect ultraviolet, by quantum photon-absorption mechanisms, in much the same chemical way that humans detect visible light. Various sources define visible light as narrowly as 420–680 nm to as broadly as 380–800 nm. Under ideal laboratory conditions, people can see infrared up to at least 1,050 nm; children and young adults may perceive ultraviolet wavelengths down to about 310–313 nm. Plant growth is also affected by the colour spectrum of light, a process known as photomorphogenesis. Speed of light The speed of light in vacuum is defined to be exactly 299792458 m/s (approximately 186,282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum. Different physicists have attempted to measure the speed of light throughout history. Galileo attempted to measure the speed of light in the seventeenth century. An early experiment to measure the speed of light was conducted by Ole Rømer, a Danish physicist, in 1676. Using a telescope, Rømer observed the motions of Jupiter and one of its moons, Io. Noting discrepancies in the apparent period of Io's orbit, he calculated that light takes about 22 minutes to traverse the diameter of Earth's orbit. However, its size was not known at that time. If Rømer had known the diameter of the Earth's orbit, he would have calculated a speed of 227000000 m/s. Another more accurate measurement of the speed of light was performed in Europe by Hippolyte Fizeau in 1849. Fizeau directed a beam of light at a mirror several kilometers away. A rotating cog wheel was placed in the path of the light beam as it traveled from the source, to the mirror and then returned to its origin. Fizeau found that at a certain rate of rotation, the beam would pass through one gap in the wheel on the way out and the next gap on the way back. Knowing the distance to the mirror, the number of teeth on the wheel and the rate of rotation, Fizeau was able to calculate the speed of light as 313000000 m/s. Léon Foucault carried out an experiment which used rotating mirrors to obtain a value of 298000000 m/s in 1862. Albert A. Michelson conducted experiments on the speed of light from 1877 until his death in 1931. He refined Foucault's methods in 1926 using improved rotating mirrors to measure the time it took light to make a round trip from Mount Wilson to Mount San Antonio in California. The precise measurements yielded a speed of 299796000 m/s. The effective velocity of light in various transparent substances containing ordinary matter, is less than in vacuum. For example, the speed of light in water is about 3/4 of that in vacuum. Two independent teams of physicists were said to bring light to a \"complete standstill\" by passing it through a Bose–Einstein condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Massachusetts and the other at the Harvard–Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being \"stopped\" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrary later time, as stimulated by a second laser pulse. During the time it had \"stopped\", it had ceased to be light. Optics The study of light and the interaction of light and matter is termed optics. Optics has different forms appropriate to different circumstances. Geometrical optics, appropriate for understanding things like eyes, lenses, cameras, fiber optics, and mirrors, works well when the wavelength of light is small in comparison to the objects it interacts with. Physical optics incorporates wave properties and is needed understand diffraction and interference. Quantum optics applies when studying individual photons interacting with matter. Surface scattering A transparent object allows light to transmit or pass through. Conversely, an opaque object does not allow light to transmit through and instead reflecting or absorbing the light it receives. Most objects do not reflect or transmit light specularly and to some degree scatters the incoming light, which is called glossiness. Surface scattering is caused by the surface roughness of the reflecting surfaces, and internal scattering is caused by the difference of refractive index between the particles and medium inside the object. Like transparent objects, translucent objects allow light to transmit through, but translucent objects also scatter certain wavelength of light via internal scattering. Refraction Refraction is the bending of light rays when passing through a surface between one transparent material and another. It is described by Snell's Law: n 1 sin ⁡ θ 1 = n 2 sin ⁡ θ 2 {\\displaystyle n_{1}\\sin \\theta _{1}=n_{2}\\sin \\theta _{2}} where θ1 is the angle between the ray and the surface normal in the first medium, θ2 is the angle between the ray and the surface normal in the second medium and n1 and n2 are the indices of refraction, n = 1 in a vacuum and n > 1 in a transparent substance. When a beam of light crosses the boundary between a vacuum and another medium, or between two different media, the wavelength of the light changes, but the frequency remains constant. If the beam of light is not orthogonal (or rather normal) to the boundary, the change in wavelength results in a change in the direction of the beam. This change of direction is known as refraction. The refractive quality of lenses is frequently used to manipulate light in order to change the apparent size of images. Magnifying glasses, spectacles, contact lenses, microscopes and refracting telescopes are all examples of this manipulation. Light sources There are many sources of light. A body at a given temperature emits a characteristic spectrum of black-body radiation. A simple thermal source is sunlight, the radiation emitted by the chromosphere of the Sun at around 6,000 K (5,730 °C; 10,340 °F). Solar radiation peaks in the visible region of the electromagnetic spectrum when plotted in wavelength units, and roughly 44% of the radiation that reaches the ground is visible. Another example is incandescent light bulbs, which emit only around 10% of their energy as visible light and the remainder as infrared. A common thermal light", "canonical_url": "https://en.wikipedia.org/wiki/Light", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:40.231562", "metadata": {"word_count": 230, "text_length": 32088}}
{"id": "wiki_visible_wavelength", "query_word": "visible wavelength", "title": "Visible spectrum", "summary": "The visible spectrum is the band of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light (or simply light). The optical spectrum is sometimes considered to be the same as the visible spectrum, but some authors define the term more broadly, to include the ultraviolet and infrared parts of the electromagnetic spectrum as well, known collectively as optical radiation. A typical human eye will respond to wavelengths from about 380 to about 750 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 400–790 terahertz. These boundaries are not sharply defined and may vary per individual. Under optimal conditions, these limits of human perception can extend to 310 nm (ultraviolet) and 1100 nm (near infrared). The spectrum does not contain all the colors that the human visual system can distinguish. Unsaturated colors such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors. Visible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the Sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long-wavelength or far-infrared (LWIR or FIR) window, although other animals may perceive them.", "text": "The visible spectrum is the band of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light (or simply light). The optical spectrum is sometimes considered to be the same as the visible spectrum, but some authors define the term more broadly, to include the ultraviolet and infrared parts of the electromagnetic spectrum as well, known collectively as optical radiation. A typical human eye will respond to wavelengths from about 380 to about 750 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 400–790 terahertz. These boundaries are not sharply defined and may vary per individual. Under optimal conditions, these limits of human perception can extend to 310 nm (ultraviolet) and 1100 nm (near infrared). The spectrum does not contain all the colors that the human visual system can distinguish. Unsaturated colors such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors. Visible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the Sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long-wavelength or far-infrared (LWIR or FIR) window, although other animals may perceive them. Spectral colors Colors that can be produced by visible light of a narrow band of wavelengths (monochromatic light) are called spectral colors. The various color ranges indicated in the illustration are an approximation: The spectrum is continuous, with no clear boundaries between one color and the next. History In the 13th century, Roger Bacon theorized that rainbows were produced by a similar process to the passage of light through glass or crystal. In the 17th century, Isaac Newton discovered that prisms could disassemble and reassemble white light, and described the phenomenon in his book Opticks. He was the first to use the word spectrum (Latin for \"appearance\" or \"apparition\") in this sense in print in 1671 in describing his experiments in optics. Newton observed that, when a narrow beam of sunlight strikes the face of a glass prism at an angle, some is reflected and some of the beam passes into and through the glass, emerging as different-colored bands. Newton hypothesized light to be made up of \"corpuscles\" (particles) of different colors, with the different colors of light moving at different speeds in transparent matter, red light moving more quickly than violet in glass. The result is that red light is bent (refracted) less sharply than violet as it passes through the prism, creating a spectrum of colors. Newton originally divided the spectrum into six named colors: red, orange, yellow, green, blue, and violet. He later added indigo as the seventh color since he believed that seven was a perfect number as derived from the ancient Greek sophists, of there being a connection between the colors, the musical notes, the known objects in the Solar System, and the days of the week. The human eye is relatively insensitive to indigo's frequencies, and some people who have otherwise-good vision cannot distinguish indigo from blue and violet. For this reason, some later commentators, including Isaac Asimov, have suggested that indigo should not be regarded as a color in its own right but merely as a shade of blue or violet. Evidence indicates that what Newton meant by \"indigo\" and \"blue\" does not correspond to the modern meanings of those color words. Comparing Newton's observation of prismatic colors with a color image of the visible light spectrum shows that \"indigo\" corresponds to what is today called blue, whereas his \"blue\" corresponds to cyan. In the 18th century, Johann Wolfgang von Goethe wrote about optical spectra in his Theory of Colours. Goethe used the word spectrum (Spektrum) to designate a ghostly optical afterimage, as did Schopenhauer in On Vision and Colors. Goethe argued that the continuous spectrum was a compound phenomenon. Where Newton narrowed the beam of light to isolate the phenomenon, Goethe observed that a wider aperture produces not a spectrum but rather reddish-yellow and blue-cyan edges with white between them. The spectrum appears only when these edges are close enough to overlap. In the early 19th century, the concept of the visible spectrum became more definite, as light outside the visible range was discovered and characterized by William Herschel (infrared) and Johann Wilhelm Ritter (ultraviolet), Thomas Young, Thomas Johann Seebeck, and others. Young was the first to measure the wavelengths of different colors of light, in 1802. The connection between the visible spectrum and color vision was explored by Thomas Young and Hermann von Helmholtz in the early 19th century. Their theory of color vision correctly proposed that the eye uses three distinct receptors to perceive color. Limits to visible range The visible spectrum is limited to wavelengths that can both reach the retina and trigger visual phototransduction (excite a visual opsin). Insensitivity to UV light is generally limited by transmission through the lens. Insensitivity to IR light is limited by the spectral sensitivity functions of the visual opsins. The range is defined psychometrically by the luminous efficiency function, which accounts for all of these factors. In humans, there is a separate function for each of two visual systems, one for photopic vision, used in daylight, which is mediated by cone cells, and one for scotopic vision, used in dim light, which is mediated by rod cells. Each of these functions have different visible ranges. However, discussion on the visible range generally assumes photopic vision. Atmospheric transmission The visible range of most animals evolved to match the optical window, which is the range of light that can pass through the atmosphere. The ozone layer absorbs almost all UV light (below 315 nm). However, this only affects cosmic light (e.g. sunlight), not terrestrial light (e.g. Bioluminescence). Ocular transmission Before reaching the retina, light must first transmit through the cornea and lens. UVB light (< 315 nm) is filtered mostly by the cornea, and UVA light (315–400 nm) is filtered mostly by the lens. The lens also yellows with age, attenuating transmission most strongly at the blue part of the spectrum. This can cause xanthopsia as well as a slight truncation of the short-wave (blue) limit of the visible spectrum. Subjects with aphakia are missing a lens, so UVA light can reach the retina and excite the visual opsins; this expands the visible range and may also lead to cyanopsia. Opsin absorption Each opsin has a spectral sensitivity function, which defines how likely it is to absorb a photon of each wavelength. The luminous efficiency function is approximately the superposition of the contributing visual opsins. Variance in the position of the individual opsin spectral sensitivity functions therefore affects the luminous efficiency function and the visible range. For example, the long-wave (red) limit changes proportionally to the position of the L-opsin. The positions are defined by the peak wavelength (wavelength of highest sensitivity), so as the L-opsin peak wavelength blue shifts by 10 nm, the long-wave limit of the visible spectrum also shifts 10 nm. Large deviations of the L-opsin peak wavelength lead to a form of color blindness called protanomaly and a missing L-opsin (protanopia) shortens the visible spectrum by about 30 nm at the long-wave limit. Forms of color blindness affecting the M-opsin and S-opsin do not significantly affect the luminous efficiency function nor the limits of the visible spectrum. Different definitions Regardless of actual physical and biological variance, the definition of the limits is not standard and will change depending on the industry. For example, some industries may be concerned with practical limits, so would conservatively report 420–680 nm, while others may be concerned with psychometrics and achieving the broadest spectrum would liberally report 380–750, or even 380–800 nm. The luminous efficiency function in the NIR does not have a hard cutoff, but rather an exponential decay, such that the function's value (or vision sensitivity) at 1,050 nm is about 109 times weaker than at 700 nm; much higher intensity is therefore required to perceive 1,050 nm light than 700 nm light. Vision outside the visible spectrum Under ideal laboratory conditions, subjects may perceive infrared light up to at least 1,064 nm. While 1,050 nm NIR light can evoke red, suggesting direct absorption by the L-opsin, there are also reports that pulsed NIR lasers can evoke green, which suggests two-photon absorption may be enabling extended NIR sensitivity. Similarly, young subjects may perceive ultraviolet wavelengths down to about 310–313 nm, but detection of light below 380 nm may be due to fluorescence of the ocular media, rather than direct absorption of UV light by the opsins. As UVA light is absorbed by the ocular media (lens and cornea), it may fluoresce and be released at a lower energy (longer wavelength) that can then be absorbed by the opsins. For example, when the lens absorbs 350 nm light, the fluorescence emission spectrum is centered on 440 nm. Non-visual light detection In addition to the photopic and scotopic systems, humans have other systems for detecting light that do not contribute to the p", "canonical_url": "https://en.wikipedia.org/wiki/Visible_spectrum", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:41.080196", "metadata": {"word_count": 299, "text_length": 15239}}
{"id": "wiki_ultraviolet_spectrum", "query_word": "ultraviolet spectrum", "title": "Ultraviolet", "summary": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.", "text": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see. Visibility Humans generally cannot use ultraviolet rays for vision. The lens of the human eye and surgically implanted lenses produced since 1986 block most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy. People lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet. Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds \"true\" UV vision. History and discovery \"Ultraviolet\" means \"beyond violet\" (from Latin ultra, \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light. UV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them \"(de-)oxidizing rays\" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them \"tithonic rays\"). The terms \"chemical rays\" and \"heat rays\" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established. The discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided \"unanimously\" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen. Subtypes The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348: Several solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum. Vacuum ultraviolet Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers. Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes. Extreme ultraviolet Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer satellite. Hard and soft ultraviolet Some sources use the distinction of \"hard UV\" and \"soft UV\". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with \"hard UV\" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions. Solar ultraviolet Very hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum. The atmosphere blocks about 77% of the Sun's UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth's surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB. The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photo", "canonical_url": "https://en.wikipedia.org/wiki/Ultraviolet", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:41.771317", "metadata": {"word_count": 368, "text_length": 56248}}
{"id": "wiki_ultraviolet_radiation", "query_word": "ultraviolet radiation", "title": "Ultraviolet", "summary": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.", "text": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see. Visibility Humans generally cannot use ultraviolet rays for vision. The lens of the human eye and surgically implanted lenses produced since 1986 block most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy. People lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet. Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds \"true\" UV vision. History and discovery \"Ultraviolet\" means \"beyond violet\" (from Latin ultra, \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light. UV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them \"(de-)oxidizing rays\" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them \"tithonic rays\"). The terms \"chemical rays\" and \"heat rays\" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established. The discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided \"unanimously\" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen. Subtypes The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348: Several solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum. Vacuum ultraviolet Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers. Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes. Extreme ultraviolet Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer satellite. Hard and soft ultraviolet Some sources use the distinction of \"hard UV\" and \"soft UV\". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with \"hard UV\" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions. Solar ultraviolet Very hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum. The atmosphere blocks about 77% of the Sun's UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth's surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB. The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photo", "canonical_url": "https://en.wikipedia.org/wiki/Ultraviolet", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:42.944652", "metadata": {"word_count": 368, "text_length": 56248}}
{"id": "wiki_UV-A", "query_word": "UV-A", "title": "Ultraviolet", "summary": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.", "text": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see. Visibility Humans generally cannot use ultraviolet rays for vision. The lens of the human eye and surgically implanted lenses produced since 1986 block most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy. People lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet. Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds \"true\" UV vision. History and discovery \"Ultraviolet\" means \"beyond violet\" (from Latin ultra, \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light. UV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them \"(de-)oxidizing rays\" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them \"tithonic rays\"). The terms \"chemical rays\" and \"heat rays\" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established. The discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided \"unanimously\" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen. Subtypes The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348: Several solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum. Vacuum ultraviolet Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers. Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes. Extreme ultraviolet Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer satellite. Hard and soft ultraviolet Some sources use the distinction of \"hard UV\" and \"soft UV\". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with \"hard UV\" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions. Solar ultraviolet Very hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum. The atmosphere blocks about 77% of the Sun's UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth's surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB. The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photo", "canonical_url": "https://en.wikipedia.org/wiki/Ultraviolet", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:43.632581", "metadata": {"word_count": 368, "text_length": 56248}}
{"id": "wiki_UV-B", "query_word": "UV-B", "title": "Ultraviolet", "summary": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.", "text": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see. Visibility Humans generally cannot use ultraviolet rays for vision. The lens of the human eye and surgically implanted lenses produced since 1986 block most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy. People lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet. Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds \"true\" UV vision. History and discovery \"Ultraviolet\" means \"beyond violet\" (from Latin ultra, \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light. UV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them \"(de-)oxidizing rays\" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them \"tithonic rays\"). The terms \"chemical rays\" and \"heat rays\" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established. The discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided \"unanimously\" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen. Subtypes The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348: Several solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum. Vacuum ultraviolet Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers. Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes. Extreme ultraviolet Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer satellite. Hard and soft ultraviolet Some sources use the distinction of \"hard UV\" and \"soft UV\". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with \"hard UV\" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions. Solar ultraviolet Very hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum. The atmosphere blocks about 77% of the Sun's UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth's surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB. The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photo", "canonical_url": "https://en.wikipedia.org/wiki/Ultraviolet", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:44.705584", "metadata": {"word_count": 368, "text_length": 56248}}
{"id": "wiki_UV-C", "query_word": "UV-C", "title": "Ultraviolet", "summary": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.", "text": "Ultraviolet radiation or UV is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. The photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature. Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact. For humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life. The lower wavelength limit of the visible spectrum is conventionally taken as 400 nm. Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range. Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see. Visibility Humans generally cannot use ultraviolet rays for vision. The lens of the human eye and surgically implanted lenses produced since 1986 block most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy. People lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet. Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds \"true\" UV vision. History and discovery \"Ultraviolet\" means \"beyond violet\" (from Latin ultra, \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light. UV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them \"(de-)oxidizing rays\" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them \"tithonic rays\"). The terms \"chemical rays\" and \"heat rays\" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established. The discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided \"unanimously\" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen. Subtypes The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348: Several solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum. Vacuum ultraviolet Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers. Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes. Extreme ultraviolet Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer satellite. Hard and soft ultraviolet Some sources use the distinction of \"hard UV\" and \"soft UV\". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with \"hard UV\" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions. Solar ultraviolet Very hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum. The atmosphere blocks about 77% of the Sun's UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth's surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB. The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photo", "canonical_url": "https://en.wikipedia.org/wiki/Ultraviolet", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:45.796342", "metadata": {"word_count": 368, "text_length": 56248}}
{"id": "wiki_near_infrared_spectrum", "query_word": "near infrared spectrum", "title": "Near-infrared spectroscopy", "summary": "Near-infrared spectroscopy (NIRS) is a spectroscopic method that uses the near-infrared region of the electromagnetic spectrum (from 780 nm to 2500 nm). Typical applications include medical and physiological diagnostics and research including blood sugar, pulse oximetry, functional neuroimaging, sports medicine, elite sports training, ergonomics, rehabilitation, neonatal research, brain computer interface, urology (bladder contraction), and neurology (neurovascular coupling). There are also applications in other areas as well such as pharmaceutical, food and agrochemical quality control, atmospheric chemistry, combustion propagation.", "text": "Near-infrared spectroscopy (NIRS) is a spectroscopic method that uses the near-infrared region of the electromagnetic spectrum (from 780 nm to 2500 nm). Typical applications include medical and physiological diagnostics and research including blood sugar, pulse oximetry, functional neuroimaging, sports medicine, elite sports training, ergonomics, rehabilitation, neonatal research, brain computer interface, urology (bladder contraction), and neurology (neurovascular coupling). There are also applications in other areas as well such as pharmaceutical, food and agrochemical quality control, atmospheric chemistry, combustion propagation. Theory Near-infrared spectroscopy is based on molecular overtone and combination vibrations. Overtones and combinations exhibit lower intensity compared to the fundamental, as a result, the molar absorptivity in the near-IR region is typically quite small. (NIR absorption bands are typically 10–100 times weaker than the corresponding fundamental mid-IR absorption band.) The lower absorption allows NIR radiation to penetrate much further into a sample than mid infrared radiation. Near-infrared spectroscopy is, therefore, not a particularly sensitive technique, but it can be very useful in probing bulk material with little to no sample preparation. The molecular overtone and combination bands seen in the near-IR are typically very broad, leading to complex spectra; it can be difficult to assign specific features to specific chemical components. Multivariate (multiple variables) calibration techniques (e.g., principal components analysis, partial least squares, or artificial neural networks) are often employed to extract the desired chemical information. Careful development of a set of calibration samples and application of multivariate calibration techniques is essential for near-infrared analytical methods. History The discovery of near-infrared energy is ascribed to William Herschel in the 19th century, but the first industrial application began in the 1950s. In the first applications, NIRS was used only as an add-on unit to other optical devices that used other wavelengths such as ultraviolet (UV), visible (Vis), or mid-infrared (MIR) spectrometers. In the 1980s, a single-unit, stand-alone NIRS system was made available. In the 1980s, Karl Norris (while working at the USDA Instrumentation Research Laboratory, Beltsville, USA) pioneered the use NIR spectroscopy for quality assessments of agricultural products. Since then, use has expanded from food and agricultural to chemical, polymer, and petroleum industries; pharmaceutical industry; biomedical sciences; and environmental analysis. With the introduction of light-fiber optics in the mid-1980s and the monochromator-detector developments in the early 1990s, NIRS became a more powerful tool for scientific research. The method has been used in a number of fields of science including physics, physiology, or medicine. It is only in the last few decades that NIRS began to be used as a medical tool for monitoring patients, with the first clinical application of so-called fNIRS in 1994. Instrumentation Instrumentation for near-IR (NIR) spectroscopy is similar to instruments for the UV-visible and mid-IR ranges. There is a source, a detector, and a dispersive element (such as a prism, or, more commonly, a diffraction grating) to allow the intensity at different wavelengths to be recorded. Fourier transform NIR instruments using an interferometer are also common, especially for wavelengths above ~1000 nm. Depending on the sample, the spectrum can be measured in either reflection or transmission. Common incandescent or quartz halogen light bulbs are most often used as broadband sources of near-infrared radiation for analytical applications. Light-emitting diodes (LEDs) can also be used. For high precision spectroscopy, wavelength-scanned lasers and frequency combs have recently become powerful sources, albeit with sometimes longer acquisition timescales. When lasers are used, a single detector without any dispersive elements might be sufficient. The type of detector used depends primarily on the range of wavelengths to be measured. Silicon-based CCDs are suitable for the shorter end of the NIR range, but are not sufficiently sensitive over most of the range (over 1000 nm). InGaAs and PbS devices are more suitable and have higher quantum efficiency for wavelengths above 1100 nm. It is possible to combine silicon-based and InGaAs detectors in the same instrument. Such instruments can record both UV-visible and NIR spectra 'simultaneously'. Instruments intended for chemical imaging in the NIR may use a 2D array detector with an acousto-optic tunable filter. Multiple images may be recorded sequentially at different narrow wavelength bands. Many commercial instruments for UV/vis spectroscopy are capable of recording spectra in the NIR range (to perhaps ~900 nm). In the same way, the range of some mid-IR instruments may extend into the NIR. In these instruments, the detector used for the NIR wavelengths is often the same detector used for the instrument's \"main\" range of interest. NIRS as an analytical technique The use of NIR as an analytical technique did not come from extending the use of mid-IR into the near-IR range, but developed independently. A striking way this was exhibited is that, while mid-IR spectroscopists use wavenumbers (cm−1) when displaying spectra, NIR spectroscopists used wavelength (nm), as is used in ultraviolet–visible spectroscopy. Early practitioners of IR spectroscopy, who depended on assignment of absorption bands to specific bond types, were frustrated by the complexity of the bonding regions being measured. However, as a quantitative tool, the lower molar absorption levels in the bonding region tended to keep absorption maxima \"on-scale\", enabling quantitative work with little sample preparation. Techniques applied to extract the quantitative information from these complex spectra were unfamiliar to analytical chemists, and the technique was viewed with suspicion in academia. Generally, quantitative NIR analysis is accomplished by selecting a group of calibration samples, for which the concentration of the analyte of interest has been determined by a reference method, and finding a correlation between various spectral features and those concentrations using a chemometric tool. The calibration is then validated by using it to predict the analyte values for samples in a validation set, whose values have been determined by the reference method but have not been included in the calibration. A validated calibration is then used to predict the values of samples. The complexity of the spectra are overcome by the use of multivariate calibration. The two tools most often used a multi-wavelength linear regression and partial least squares. Applications Typical applications of NIR spectroscopy include the analysis of food products, pharmaceuticals, combustion products, and a major branch of astronomical spectroscopy. Astronomical spectroscopy Near-infrared spectroscopy is used in astronomy for studying the atmospheres of cool stars where molecules can form. The vibrational and rotational signatures of molecules such as titanium oxide, cyanide, and carbon monoxide can be seen in this wavelength range and can give a clue towards the star's spectral type. It is also used for studying molecules in other astronomical contexts, such as in molecular clouds where new stars are formed. The astronomical phenomenon known as reddening means that near-infrared wavelengths are less affected by dust in the interstellar medium, such that regions inaccessible by optical spectroscopy can be studied in the near-infrared. Since dust and gas are strongly associated, these dusty regions are exactly those where infrared spectroscopy is most useful. The near-infrared spectra of very young stars provide important information about their ages and masses, which is important for understanding star formation in general. Astronomical spectrographs have also been developed for the detection of exoplanets using the Doppler shift of the parent star due to the radial velocity of the planet around the star. Agriculture Near-infrared spectroscopy is widely applied in agriculture for determining the quality of forages, grains, and grain products, oilseeds, coffee, tea, spices, fruits, vegetables, sugarcane, beverages, fats, and oils, dairy products, eggs, meat, and other agricultural products. It is widely used to quantify the composition of agricultural products because it meets the criteria of being accurate, reliable, rapid, non-destructive, and inexpensive. Abeni and Bergoglio 2001 apply NIRS to chicken breeding as the assay method for characteristics of fat composition. Remote monitoring Techniques have been developed for NIR spectroscopic imaging. Hyperspectral imaging has been applied for a wide range of uses, including the remote investigation of plants and soils. Data can be collected from instruments on airplanes, satellites or unmanned aerial systems to assess ground cover and soil chemistry. Remote monitoring or remote sensing from the NIR spectroscopic region can also be used to study the atmosphere. For example, measurements of atmospheric gases are made from NIR spectra measured by the OCO-2, GOSAT, and the TCCON. Materials science Techniques have been developed for NIR spectroscopy of microscopic sample areas for film thickness measurements, research into the optical characteristics of nanoparticles and optical coatings for the telecommunications industry. Medical uses The application of NIRS in medicine centres on its ability to provide information about the oxygen saturation of haemoglobin within the microcirculation. Broadly speaking, it can be used to assess oxygenation and microvascular function in the brain (cerebral NIRS) or in the peripheral tissues (peripheral NIRS). Cerebral NIRS When a specific area of the brain is activated, the locali", "canonical_url": "https://en.wikipedia.org/wiki/Near-infrared_spectroscopy", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:46.448822", "metadata": {"word_count": 78, "text_length": 16422}}
{"id": "wiki_NIR", "query_word": "NIR", "title": "NIR", "summary": "NIR or Nir may refer to:", "text": "NIR or Nir may refer to: Science and technology Near-infrared, a region within the infrared part of the electromagnetic radiation spectrum Near-infrared spectroscopy, a spectroscopic method that uses the near-infrared region (from 780 nm to 2500 nm). National Identity Register, a former UK database National Internet registry, which coordinates IP address and other resource allocation NIR, proposed variation of the SECAM colour television system in the Soviet Union Numéro d'inscription au répertoire national d'identification des personnes and numéro d'inscription au répertoire, national identity numbers; see INSEE code Places Nir, Iran (disambiguation), several places in Iran Negros Island Region, one of the 18 regions of the Philippines Nigeria, UNDP country code Northern Ireland (FIFA country code: NIR, ISO 3166 code: GB-NIR), a part of the United Kingdom Ness Islands Railway, a miniature railway in Scotland Nainpur Junction railway station (station code: NIR), Madhya Pradesh, India Other uses Nir (name), a Hebrew given name and surname Northern Ireland Railways, the railway operator in Northern Ireland Rate of natural increase, a demographic metric of population increase Nationaal Instituut voor de Radio-Omroep, Dutch name of Belgium's National Institute of Radio Broadcasting (1930-1960) See also NIRS (disambiguation) All pages with titles containing NIR", "canonical_url": "https://en.wikipedia.org/wiki/NIR", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:47.108137", "metadata": {"word_count": 6, "text_length": 1378}}
{"id": "wiki_VNIR", "query_word": "VNIR", "title": "VNIR", "summary": "The visible and near-infrared (VNIR) portion of the electromagnetic spectrum has wavelengths between approximately 400 and 1100 nanometers (nm). It combines the full visible spectrum with an adjacent portion of the infrared spectrum up to the water absorption band between 1400 and 1500 nm. Some definitions also include the short-wavelength infrared band from 1400 nm up to the water absorption band at 2500 nm. VNIR multi-spectral image cameras have wide applications in remote sensing and imaging spectroscopy. Hyperspectral Imaging Satellite carried two payloads, among which one was working on the spectral range of VNIR.", "text": "The visible and near-infrared (VNIR) portion of the electromagnetic spectrum has wavelengths between approximately 400 and 1100 nanometers (nm). It combines the full visible spectrum with an adjacent portion of the infrared spectrum up to the water absorption band between 1400 and 1500 nm. Some definitions also include the short-wavelength infrared band from 1400 nm up to the water absorption band at 2500 nm. VNIR multi-spectral image cameras have wide applications in remote sensing and imaging spectroscopy. Hyperspectral Imaging Satellite carried two payloads, among which one was working on the spectral range of VNIR. See also Advanced Spaceborne Thermal Emission and Reflection Radiometer Airborne Real-time Cueing Hyperspectral Enhanced Reconnaissance Mars Reconnaissance Orbiter Near infrared spectroscopy == References ==", "canonical_url": "https://en.wikipedia.org/wiki/VNIR", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:47.764223", "metadata": {"word_count": 94, "text_length": 834}}
{"id": "wiki_SWIR", "query_word": "SWIR", "title": "SWIR", "summary": "SWIR may refer to: Short-wavelength infrared, a region of the infrared light spectrum Sierra Wireless, a multinational communication company Southwest Indian Ridge, a mid-ocean ridge between Africa and Antarctica", "text": "SWIR may refer to: Short-wavelength infrared, a region of the infrared light spectrum Sierra Wireless, a multinational communication company Southwest Indian Ridge, a mid-ocean ridge between Africa and Antarctica See also Svir (disambiguation)", "canonical_url": "https://en.wikipedia.org/wiki/SWIR", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:49.025729", "metadata": {"word_count": 29, "text_length": 243}}
{"id": "wiki_MWIR", "query_word": "MWIR", "title": "Infrared", "summary": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.", "text": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting. Definition and relationship to the electromagnetic spectrum There is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1 mm (3 THz). Nature Sunlight, at an effective temperature of 5,780 K (5,510 °C, 9,940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 μm. On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy. Regions In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed. Visible limit Infrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700 nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050 nm from pulsed lasers can be seen by humans under certain conditions. Commonly used subdivision scheme A commonly used subdivision scheme is: NIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". CIE division scheme The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands: ISO 20473 scheme ISO 20473 specifies the following scheme: Astronomy division scheme Astronomers typically divide the infrared spectrum as follows: These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space. The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers. Sensor response division scheme A third scheme divides up the band based on the response of various detectors: Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon). Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region. Cryogenically cooled MCT detectors can cover the region of 1.0–2.5 μm. Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide, InSb and mercury cadmium telluride, HgCdTe, and partially by lead selenide, PbSe). Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers). Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon). Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available. The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage. Telecommunication bands In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors: The C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed. Heat Infrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from th", "canonical_url": "https://en.wikipedia.org/wiki/Infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:50.217417", "metadata": {"word_count": 456, "text_length": 36113}}
{"id": "wiki_LWIR", "query_word": "LWIR", "title": "Infrared", "summary": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.", "text": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting. Definition and relationship to the electromagnetic spectrum There is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1 mm (3 THz). Nature Sunlight, at an effective temperature of 5,780 K (5,510 °C, 9,940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 μm. On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy. Regions In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed. Visible limit Infrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700 nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050 nm from pulsed lasers can be seen by humans under certain conditions. Commonly used subdivision scheme A commonly used subdivision scheme is: NIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". CIE division scheme The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands: ISO 20473 scheme ISO 20473 specifies the following scheme: Astronomy division scheme Astronomers typically divide the infrared spectrum as follows: These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space. The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers. Sensor response division scheme A third scheme divides up the band based on the response of various detectors: Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon). Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region. Cryogenically cooled MCT detectors can cover the region of 1.0–2.5 μm. Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide, InSb and mercury cadmium telluride, HgCdTe, and partially by lead selenide, PbSe). Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers). Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon). Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available. The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage. Telecommunication bands In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors: The C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed. Heat Infrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from th", "canonical_url": "https://en.wikipedia.org/wiki/Infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:51.413413", "metadata": {"word_count": 456, "text_length": 36113}}
{"id": "wiki_TIR", "query_word": "TIR", "title": "TIR", "summary": "Tir, tir or TIR may refer to: The modern Persian name for the Zoroastrian god Tishtrya Tir (month), of the Iranian calendar Tir (god), of ancient Armenia Tabar, Iran, a village in North Khorasan Province Old English spelling of the Norse god Týr Tir (demon), son of Iblis Tir, a 2010 album by Cerys Matthews", "text": "Tir, tir or TIR may refer to: The modern Persian name for the Zoroastrian god Tishtrya Tir (month), of the Iranian calendar Tir (god), of ancient Armenia Tabar, Iran, a village in North Khorasan Province Old English spelling of the Norse god Týr Tir (demon), son of Iblis Tir, a 2010 album by Cerys Matthews Abbreviations Total internal reflection Tigrinya language, ISO 639 code Tirupati Airport, IATA code Translocated intimin receptor, used by E. coli TIR Convention, (Transports Internationaux Routiers, International Road Transport) Toll-Interleukin receptor Total indicator reading in metrology Other Tir McDohl, in the video game Suikoden INS Tir, various ships of the Indian navy See also Tira (disambiguation) Tiran (disambiguation) Tir Planitia, a large basin on Mercury Tír na nÓg, an Otherworld in Irish mythology", "canonical_url": "https://en.wikipedia.org/wiki/TIR", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:52.384684", "metadata": {"word_count": 55, "text_length": 825}}
{"id": "wiki_far_infrared", "query_word": "far infrared", "title": "Far infrared", "summary": "Far infrared (FIR) or long wave refers to a specific range within the infrared spectrum of electromagnetic radiation. It encompasses radiation with wavelengths ranging from 15 μm (micrometers) to 1 mm, which corresponds to a frequency range of approximately 20 THz to 300 GHz. This places far infrared radiation within the CIE IR-B and IR-C bands. The longer wavelengths of the FIR spectrum overlap with a range known as terahertz radiation. Different sources may use different boundaries to define the far infrared range. For instance, astronomers often define it as wavelengths between 25 μm and 350 μm. Infrared photons possess significantly lower energy than photons in the visible light spectrum, with tens to hundreds of times less energy.", "text": "Far infrared (FIR) or long wave refers to a specific range within the infrared spectrum of electromagnetic radiation. It encompasses radiation with wavelengths ranging from 15 μm (micrometers) to 1 mm, which corresponds to a frequency range of approximately 20 THz to 300 GHz. This places far infrared radiation within the CIE IR-B and IR-C bands. The longer wavelengths of the FIR spectrum overlap with a range known as terahertz radiation. Different sources may use different boundaries to define the far infrared range. For instance, astronomers often define it as wavelengths between 25 μm and 350 μm. Infrared photons possess significantly lower energy than photons in the visible light spectrum, with tens to hundreds of times less energy. Applications Astronomy Objects within a temperature range of approximately 5 K to 340 K emit radiation in the far infrared range as a result of black-body radiation, in accordance with Wien's displacement law. This characteristic is utilized in the observation of interstellar gases, which are frequently associated with the formation of new stars. The brightness observed in far infrared images of the center of the Milky Way galaxy arises from the high density of stars in that region, which heats the surrounding dust and induces radiation emission in the far infrared spectrum. Excluding the center of the Milky Way galaxy, the galaxy M82 is the most prominent far-infrared object in the sky, with its central region emitting amounts of far infrared light equivalent to the combined emissions of all the stars in the Milky Way. As of 29 May 2012, the source responsible for heating the dust at the center of M82 remains unknown. Human body detection Certain human proximity sensors utilize passive infrared sensing within the far infrared wavelength range to detect the presence of stationary and moving human bodies. Heating Infrared heating (IR) is a method of heating an area through more efficient results than gas or electric convection heating. Studies show IR heats faster, more uniformly, and more efficiently than a traditional system. Increasingly, IR heating is utilised as part of scheme designs to achieve spot, zonal and smart heating within occupation zones within a building. Though multiple applications of long wave or FIR heating exist, a common representation comprises radiant panel heaters. Radiant panel heaters typically contain a grid of resistance wire or ribbons which are sandwiched between a thin plate of electrical insulation on an emitting die and thermal insulation on the back side. Owing to their size and flexibility, infrared panel heaters can be fitted on walls and ceilings for added-space saving benefits. Electric FIR panel heaters are shown to have up to 98.5% efficiency from supply to production of heat with satisfactory thermal comfort, thermostatic control, and with low initial investment. Therapeutic modality Researchers have observed that among all forms of radiant heat, only far-infrared radiation transfers energy solely in the form of heat that can be sensed by the human body. They have found that this type of radiant heat can penetrate the skin up to a depth of approximately 1.5 inches (3.8 cm). In the field of biomedicine, experiments have been conducted using fabrics woven with FIR-emitting ceramics embedded in their fibers. These studies have indicated a potential delay in the onset of fatigue induced by muscle contractions in participants. The researchers have suggested that the emission of far-infrared radiation by these ceramics (referred to as cFIR) could facilitate cellular repair. Certain heating pads have been marketed to provide \"far infrared\" therapy, which is claimed to offer deeper penetration. However, the infrared radiation emitted by an object is determined by its temperature. Therefore, all heating pads emit the same type of infrared radiation if they are at the same temperature. Higher temperatures will result in greater infrared radiation, but caution must be exercised to avoid burns. References == External links ==", "canonical_url": "https://en.wikipedia.org/wiki/Far_infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:53.739932", "metadata": {"word_count": 118, "text_length": 4061}}
{"id": "wiki_FIR", "query_word": "FIR", "title": "FIR", "summary": "A fir is a type of evergreen coniferous tree. Fir, FIR or F.I.R. may also refer to:", "text": "A fir is a type of evergreen coniferous tree. Fir, FIR or F.I.R. may also refer to: Entertainment F.I.R., a Taiwanese pop music group F.I.R. (album) F. I. R. (1999 film), an Indian Malayalam-language action film F.I.R No. 339/07/06, a 2021 Indian Bengali-language crime thriller film FIR (2022 film), an Indian Tamil-language action thriller film F.I.R. (TV series), an Indian sitcom Falling in Reverse, an American rock band Science and technology Far infrared Finite impulse response, a type of filter in signal processing Free ideal ring Flight information region FiR 1, the first nuclear reactor in Finland Research Institute for Operations Management (German: Forschungsinstitut für Rationalisierung), in Germany Other uses Fir, an alternative spelling of Fier, a city in Albania Firan language (ISO 639-3 language code: fir) First information report, a police document in some countries Flats Industrial Railroad, a former US company Flight information region, Specified region of airspace International Federation of Resistance Fighters – Association of Anti-Fascists (French: FIR Fédération Internationale des Résistantes – Association des Antifascistes) Italian Rugby Federation (Italian: Federazione Italiana Rugby) USCGC Fir, 2 US Coast Guard ships See also FIRS (disambiguation) The Firs (disambiguation)", "canonical_url": "https://en.wikipedia.org/wiki/FIR", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:54.403376", "metadata": {"word_count": 17, "text_length": 1316}}
{"id": "wiki_infrared_spectrum", "query_word": "infrared spectrum", "title": "Infrared", "summary": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.", "text": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting. Definition and relationship to the electromagnetic spectrum There is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1 mm (3 THz). Nature Sunlight, at an effective temperature of 5,780 K (5,510 °C, 9,940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 μm. On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy. Regions In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed. Visible limit Infrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700 nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050 nm from pulsed lasers can be seen by humans under certain conditions. Commonly used subdivision scheme A commonly used subdivision scheme is: NIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". CIE division scheme The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands: ISO 20473 scheme ISO 20473 specifies the following scheme: Astronomy division scheme Astronomers typically divide the infrared spectrum as follows: These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space. The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers. Sensor response division scheme A third scheme divides up the band based on the response of various detectors: Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon). Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region. Cryogenically cooled MCT detectors can cover the region of 1.0–2.5 μm. Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide, InSb and mercury cadmium telluride, HgCdTe, and partially by lead selenide, PbSe). Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers). Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon). Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available. The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage. Telecommunication bands In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors: The C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed. Heat Infrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from th", "canonical_url": "https://en.wikipedia.org/wiki/Infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:55.276384", "metadata": {"word_count": 456, "text_length": 36113}}
{"id": "wiki_infrared_radiation", "query_word": "infrared radiation", "title": "Infrared", "summary": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.", "text": "Infrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around 780 nm (380 THz) to 1 mm (300 GHz). IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30–100 μm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon. It was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate. Infrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range. Infrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting. Definition and relationship to the electromagnetic spectrum There is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1 mm (3 THz). Nature Sunlight, at an effective temperature of 5,780 K (5,510 °C, 9,940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 μm. On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy. Regions In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed. Visible limit Infrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700 nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050 nm from pulsed lasers can be seen by humans under certain conditions. Commonly used subdivision scheme A commonly used subdivision scheme is: NIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". CIE division scheme The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands: ISO 20473 scheme ISO 20473 specifies the following scheme: Astronomy division scheme Astronomers typically divide the infrared spectrum as follows: These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space. The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers. Sensor response division scheme A third scheme divides up the band based on the response of various detectors: Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon). Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region. Cryogenically cooled MCT detectors can cover the region of 1.0–2.5 μm. Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide, InSb and mercury cadmium telluride, HgCdTe, and partially by lead selenide, PbSe). Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers). Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon). Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available. The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage. Telecommunication bands In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors: The C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed. Heat Infrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from th", "canonical_url": "https://en.wikipedia.org/wiki/Infrared", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:56.141906", "metadata": {"word_count": 456, "text_length": 36113}}
{"id": "wiki_microwave", "query_word": "microwave", "title": "Microwave", "summary": "Microwave is a form of electromagnetic radiation with wavelengths shorter than other radio waves but longer than infrared waves. Its wavelength ranges from about one meter to one millimeter, corresponding to frequencies between 300 MHz and 300 GHz, broadly construed. A more common definition in radio-frequency engineering is the range between 1 and 100 GHz (wavelengths between 30 cm and 3 mm), or between 1 and 3000 GHz (30 cm and 0.1 mm). In all cases, microwaves include the entire super high frequency (SHF) band (3 to 30 GHz, or 10 to 1 cm) at minimum. The boundaries between far infrared, terahertz radiation, microwaves, and ultra-high-frequency (UHF) are fairly arbitrary and differ between different fields of study. The prefix micro- in microwave indicates that microwaves are small (having shorter wavelengths), compared to the radio waves used in prior radio technology. Frequencies in the microwave range are often referred to by their IEEE radar band designations: S, C, X, Ku, K, or Ka band, or by similar NATO or EU designations. Microwaves travel by line-of-sight; unlike lower frequency radio waves, they do not diffract around hills, follow the Earth's surface as ground waves, or reflect from the ionosphere, so terrestrial microwave communication links are limited by the visual horizon to about 40 miles (64 km). At the high end of the band, they are absorbed by gases in the atmosphere, limiting practical communication distances to around a kilometer. Microwaves are widely used in modern technology, for example in point-to-point communication links, wireless networks, microwave radio relay networks, radar, satellite and spacecraft communication, medical diathermy and cancer treatment, remote sensing, radio astronomy, particle accelerators, spectroscopy, industrial heating, collision avoidance systems, garage door openers and keyless entry systems, and for cooking food in microwave ovens.", "text": "Microwave is a form of electromagnetic radiation with wavelengths shorter than other radio waves but longer than infrared waves. Its wavelength ranges from about one meter to one millimeter, corresponding to frequencies between 300 MHz and 300 GHz, broadly construed. A more common definition in radio-frequency engineering is the range between 1 and 100 GHz (wavelengths between 30 cm and 3 mm), or between 1 and 3000 GHz (30 cm and 0.1 mm). In all cases, microwaves include the entire super high frequency (SHF) band (3 to 30 GHz, or 10 to 1 cm) at minimum. The boundaries between far infrared, terahertz radiation, microwaves, and ultra-high-frequency (UHF) are fairly arbitrary and differ between different fields of study. The prefix micro- in microwave indicates that microwaves are small (having shorter wavelengths), compared to the radio waves used in prior radio technology. Frequencies in the microwave range are often referred to by their IEEE radar band designations: S, C, X, Ku, K, or Ka band, or by similar NATO or EU designations. Microwaves travel by line-of-sight; unlike lower frequency radio waves, they do not diffract around hills, follow the Earth's surface as ground waves, or reflect from the ionosphere, so terrestrial microwave communication links are limited by the visual horizon to about 40 miles (64 km). At the high end of the band, they are absorbed by gases in the atmosphere, limiting practical communication distances to around a kilometer. Microwaves are widely used in modern technology, for example in point-to-point communication links, wireless networks, microwave radio relay networks, radar, satellite and spacecraft communication, medical diathermy and cancer treatment, remote sensing, radio astronomy, particle accelerators, spectroscopy, industrial heating, collision avoidance systems, garage door openers and keyless entry systems, and for cooking food in microwave ovens. Electromagnetic spectrum Microwaves occupy a place in the electromagnetic spectrum with frequency above ordinary radio waves, and below infrared light: In descriptions of the electromagnetic spectrum, some sources classify microwaves as radio waves, a subset of the radio wave band, while others classify microwaves and radio waves as distinct types of radiation. This is an arbitrary distinction. Frequency bands Bands of frequencies in the microwave spectrum are designated by letters. Unfortunately, there are several incompatible band designation systems, and even within a system the frequency ranges corresponding to some of the letters vary somewhat between different application fields. The letter system had its origin in World War 2 in a top-secret U.S. classification of bands used in radar sets; this is the origin of the oldest letter system, the IEEE radar bands. One set of microwave frequency bands designations by the Radio Society of Great Britain (RSGB), is tabulated below: Other definitions exist. The term P band is sometimes used for UHF frequencies below the L band but is now obsolete per IEEE Std 521. When radars were first developed at K band during World War 2, it was not known that there was a nearby absorption band (due to water vapor and oxygen in the atmosphere). To avoid this problem, the original K band was split into a lower band, Ku, and upper band, Ka. Propagation Microwaves travel solely by line-of-sight paths; unlike lower frequency radio waves, they do not travel as ground waves which follow the contour of the Earth, or reflect off the ionosphere (skywaves). Although at the low end of the band, they can pass through building walls enough for useful reception, usually rights of way cleared to the first Fresnel zone are required. Therefore, on the surface of the Earth, microwave communication links are limited by the visual horizon to about 30–40 miles (48–64 km). Microwaves are absorbed by moisture in the atmosphere, and the attenuation increases with frequency, becoming a significant factor (rain fade) at the high end of the band. Beginning at about 40 GHz, atmospheric gases also begin to absorb microwaves, so above this frequency microwave transmission is limited to a few kilometers. A spectral band structure causes absorption peaks at specific frequencies (see graph at right). Above 100 GHz, the absorption of electromagnetic radiation by Earth's atmosphere is so effective that it is in effect opaque, until the atmosphere becomes transparent again in the so-called infrared and optical window frequency ranges. Troposcatter In a microwave beam directed at an angle into the sky, a small amount of the power will be randomly scattered as the beam passes through the troposphere. A sensitive receiver beyond the horizon with a high gain antenna focused on that area of the troposphere can pick up the signal. This technique has been used at frequencies between 0.45 and 5 GHz in tropospheric scatter (troposcatter) communication systems to communicate beyond the horizon, at distances up to 300 km. Antennas The short wavelengths of microwaves allow omnidirectional antennas for portable devices to be made very small, from 1 to 20 centimeters long, so microwave frequencies are widely used for wireless devices such as cell phones, cordless phones, and wireless LANs (Wi-Fi) access for laptops, and Bluetooth earphones. Antennas used include short whip antennas, rubber ducky antennas, sleeve dipoles, patch antennas, and increasingly the printed circuit inverted F antenna (PIFA) used in cell phones. Their short wavelength also allows narrow beams of microwaves to be produced by conveniently small high gain antennas from a half meter to 5 meters in diameter. Therefore, beams of microwaves are used for point-to-point communication links, and for radar. An advantage of narrow beams is that they do not interfere with nearby equipment using the same frequency, allowing frequency reuse by nearby transmitters. Parabolic (\"dish\") antennas are the most widely used directive antennas at microwave frequencies, but horn antennas, slot antennas and lens antennas are also used. Flat microstrip antennas are being increasingly used in consumer devices. Another directive antenna practical at microwave frequencies is the phased array, a computer-controlled array of antennas that produces a beam that can be electronically steered in different directions. At microwave frequencies, the transmission lines which are used to carry lower frequency radio waves to and from antennas, such as coaxial cable and parallel wire lines, have excessive power losses, so when low attenuation is required, microwaves are carried by metal pipes called waveguides. Due to the high cost and maintenance requirements of waveguide runs, in many microwave antennas the output stage of the transmitter or the RF front end of the receiver is located at the antenna. Design and analysis The term microwave also has a more technical meaning in electromagnetics and circuit theory. Apparatus and techniques may be described qualitatively as \"microwave\" when the wavelengths of signals are roughly the same as the dimensions of the circuit, so that lumped-element circuit theory is inaccurate, and instead distributed circuit elements and transmission-line theory are more useful methods for design and analysis. As a consequence, practical microwave circuits tend not to use the discrete resistors, capacitors, and inductors used with lower-frequency radio waves. Open-wire and coaxial transmission lines used at lower frequencies are replaced by waveguides and stripline, and lumped-element tuned circuits are replaced by cavity resonators or resonant stubs. In turn, at even higher frequencies, where the wavelength of the electromagnetic waves becomes small in comparison to the size of the structures used to process them, microwave techniques become inadequate, and the methods of optics are used. Sources High-power microwave sources use specialized vacuum tubes to generate microwaves. These devices operate on different principles from low-frequency vacuum tubes, using the ballistic motion of electrons in a vacuum under the influence of controlling electric or magnetic fields, and include the magnetron (used in microwave ovens), klystron, traveling-wave tube (TWT), crossed-field amplifier, and gyrotron. These devices work in the density modulated mode, rather than the current modulated mode. This means that they work on the basis of clumps of electrons flying ballistically through them, rather than using a continuous stream of electrons. Low-power microwave sources use solid-state devices such as the field-effect transistor (at least at lower frequencies), tunnel diodes, Gunn diodes, and IMPATT diodes. Low-power sources are available as benchtop instruments, rackmount instruments, embeddable modules and in card-level formats. A maser is a solid-state device that amplifies microwaves using similar principles to the laser, which amplifies higher-frequency light waves. All warm objects emit low level microwave black-body radiation, depending on their temperature, so in meteorology and remote sensing, microwave radiometers are used to measure the temperature of objects or terrain. The sun and other astronomical radio sources such as Cassiopeia A emit low level microwave radiation which carries information about their makeup, which is studied by radio astronomers using receivers called radio telescopes. The cosmic microwave background radiation (CMBR), for example, is a weak microwave noise filling empty space which is a major source of information on cosmology's Big Bang theory of the origin of the Universe. Applications Microwave technology is extensively used for point-to-point telecommunications (i.e., non-broadcast uses). Microwaves are especially suitable for this use since they are more easily focused into narrower beams than radio waves, allowing frequency reuse; their comparatively higher frequencies allow broad bandwidth and high data transmission rates, and antenna sizes are smaller ", "canonical_url": "https://en.wikipedia.org/wiki/Microwave", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:57.069047", "metadata": {"word_count": 292, "text_length": 37308}}
{"id": "wiki_microwave_spectrum", "query_word": "microwave spectrum", "title": "Microwave", "summary": "Microwave is a form of electromagnetic radiation with wavelengths shorter than other radio waves but longer than infrared waves. Its wavelength ranges from about one meter to one millimeter, corresponding to frequencies between 300 MHz and 300 GHz, broadly construed. A more common definition in radio-frequency engineering is the range between 1 and 100 GHz (wavelengths between 30 cm and 3 mm), or between 1 and 3000 GHz (30 cm and 0.1 mm). In all cases, microwaves include the entire super high frequency (SHF) band (3 to 30 GHz, or 10 to 1 cm) at minimum. The boundaries between far infrared, terahertz radiation, microwaves, and ultra-high-frequency (UHF) are fairly arbitrary and differ between different fields of study. The prefix micro- in microwave indicates that microwaves are small (having shorter wavelengths), compared to the radio waves used in prior radio technology. Frequencies in the microwave range are often referred to by their IEEE radar band designations: S, C, X, Ku, K, or Ka band, or by similar NATO or EU designations. Microwaves travel by line-of-sight; unlike lower frequency radio waves, they do not diffract around hills, follow the Earth's surface as ground waves, or reflect from the ionosphere, so terrestrial microwave communication links are limited by the visual horizon to about 40 miles (64 km). At the high end of the band, they are absorbed by gases in the atmosphere, limiting practical communication distances to around a kilometer. Microwaves are widely used in modern technology, for example in point-to-point communication links, wireless networks, microwave radio relay networks, radar, satellite and spacecraft communication, medical diathermy and cancer treatment, remote sensing, radio astronomy, particle accelerators, spectroscopy, industrial heating, collision avoidance systems, garage door openers and keyless entry systems, and for cooking food in microwave ovens.", "text": "Microwave is a form of electromagnetic radiation with wavelengths shorter than other radio waves but longer than infrared waves. Its wavelength ranges from about one meter to one millimeter, corresponding to frequencies between 300 MHz and 300 GHz, broadly construed. A more common definition in radio-frequency engineering is the range between 1 and 100 GHz (wavelengths between 30 cm and 3 mm), or between 1 and 3000 GHz (30 cm and 0.1 mm). In all cases, microwaves include the entire super high frequency (SHF) band (3 to 30 GHz, or 10 to 1 cm) at minimum. The boundaries between far infrared, terahertz radiation, microwaves, and ultra-high-frequency (UHF) are fairly arbitrary and differ between different fields of study. The prefix micro- in microwave indicates that microwaves are small (having shorter wavelengths), compared to the radio waves used in prior radio technology. Frequencies in the microwave range are often referred to by their IEEE radar band designations: S, C, X, Ku, K, or Ka band, or by similar NATO or EU designations. Microwaves travel by line-of-sight; unlike lower frequency radio waves, they do not diffract around hills, follow the Earth's surface as ground waves, or reflect from the ionosphere, so terrestrial microwave communication links are limited by the visual horizon to about 40 miles (64 km). At the high end of the band, they are absorbed by gases in the atmosphere, limiting practical communication distances to around a kilometer. Microwaves are widely used in modern technology, for example in point-to-point communication links, wireless networks, microwave radio relay networks, radar, satellite and spacecraft communication, medical diathermy and cancer treatment, remote sensing, radio astronomy, particle accelerators, spectroscopy, industrial heating, collision avoidance systems, garage door openers and keyless entry systems, and for cooking food in microwave ovens. Electromagnetic spectrum Microwaves occupy a place in the electromagnetic spectrum with frequency above ordinary radio waves, and below infrared light: In descriptions of the electromagnetic spectrum, some sources classify microwaves as radio waves, a subset of the radio wave band, while others classify microwaves and radio waves as distinct types of radiation. This is an arbitrary distinction. Frequency bands Bands of frequencies in the microwave spectrum are designated by letters. Unfortunately, there are several incompatible band designation systems, and even within a system the frequency ranges corresponding to some of the letters vary somewhat between different application fields. The letter system had its origin in World War 2 in a top-secret U.S. classification of bands used in radar sets; this is the origin of the oldest letter system, the IEEE radar bands. One set of microwave frequency bands designations by the Radio Society of Great Britain (RSGB), is tabulated below: Other definitions exist. The term P band is sometimes used for UHF frequencies below the L band but is now obsolete per IEEE Std 521. When radars were first developed at K band during World War 2, it was not known that there was a nearby absorption band (due to water vapor and oxygen in the atmosphere). To avoid this problem, the original K band was split into a lower band, Ku, and upper band, Ka. Propagation Microwaves travel solely by line-of-sight paths; unlike lower frequency radio waves, they do not travel as ground waves which follow the contour of the Earth, or reflect off the ionosphere (skywaves). Although at the low end of the band, they can pass through building walls enough for useful reception, usually rights of way cleared to the first Fresnel zone are required. Therefore, on the surface of the Earth, microwave communication links are limited by the visual horizon to about 30–40 miles (48–64 km). Microwaves are absorbed by moisture in the atmosphere, and the attenuation increases with frequency, becoming a significant factor (rain fade) at the high end of the band. Beginning at about 40 GHz, atmospheric gases also begin to absorb microwaves, so above this frequency microwave transmission is limited to a few kilometers. A spectral band structure causes absorption peaks at specific frequencies (see graph at right). Above 100 GHz, the absorption of electromagnetic radiation by Earth's atmosphere is so effective that it is in effect opaque, until the atmosphere becomes transparent again in the so-called infrared and optical window frequency ranges. Troposcatter In a microwave beam directed at an angle into the sky, a small amount of the power will be randomly scattered as the beam passes through the troposphere. A sensitive receiver beyond the horizon with a high gain antenna focused on that area of the troposphere can pick up the signal. This technique has been used at frequencies between 0.45 and 5 GHz in tropospheric scatter (troposcatter) communication systems to communicate beyond the horizon, at distances up to 300 km. Antennas The short wavelengths of microwaves allow omnidirectional antennas for portable devices to be made very small, from 1 to 20 centimeters long, so microwave frequencies are widely used for wireless devices such as cell phones, cordless phones, and wireless LANs (Wi-Fi) access for laptops, and Bluetooth earphones. Antennas used include short whip antennas, rubber ducky antennas, sleeve dipoles, patch antennas, and increasingly the printed circuit inverted F antenna (PIFA) used in cell phones. Their short wavelength also allows narrow beams of microwaves to be produced by conveniently small high gain antennas from a half meter to 5 meters in diameter. Therefore, beams of microwaves are used for point-to-point communication links, and for radar. An advantage of narrow beams is that they do not interfere with nearby equipment using the same frequency, allowing frequency reuse by nearby transmitters. Parabolic (\"dish\") antennas are the most widely used directive antennas at microwave frequencies, but horn antennas, slot antennas and lens antennas are also used. Flat microstrip antennas are being increasingly used in consumer devices. Another directive antenna practical at microwave frequencies is the phased array, a computer-controlled array of antennas that produces a beam that can be electronically steered in different directions. At microwave frequencies, the transmission lines which are used to carry lower frequency radio waves to and from antennas, such as coaxial cable and parallel wire lines, have excessive power losses, so when low attenuation is required, microwaves are carried by metal pipes called waveguides. Due to the high cost and maintenance requirements of waveguide runs, in many microwave antennas the output stage of the transmitter or the RF front end of the receiver is located at the antenna. Design and analysis The term microwave also has a more technical meaning in electromagnetics and circuit theory. Apparatus and techniques may be described qualitatively as \"microwave\" when the wavelengths of signals are roughly the same as the dimensions of the circuit, so that lumped-element circuit theory is inaccurate, and instead distributed circuit elements and transmission-line theory are more useful methods for design and analysis. As a consequence, practical microwave circuits tend not to use the discrete resistors, capacitors, and inductors used with lower-frequency radio waves. Open-wire and coaxial transmission lines used at lower frequencies are replaced by waveguides and stripline, and lumped-element tuned circuits are replaced by cavity resonators or resonant stubs. In turn, at even higher frequencies, where the wavelength of the electromagnetic waves becomes small in comparison to the size of the structures used to process them, microwave techniques become inadequate, and the methods of optics are used. Sources High-power microwave sources use specialized vacuum tubes to generate microwaves. These devices operate on different principles from low-frequency vacuum tubes, using the ballistic motion of electrons in a vacuum under the influence of controlling electric or magnetic fields, and include the magnetron (used in microwave ovens), klystron, traveling-wave tube (TWT), crossed-field amplifier, and gyrotron. These devices work in the density modulated mode, rather than the current modulated mode. This means that they work on the basis of clumps of electrons flying ballistically through them, rather than using a continuous stream of electrons. Low-power microwave sources use solid-state devices such as the field-effect transistor (at least at lower frequencies), tunnel diodes, Gunn diodes, and IMPATT diodes. Low-power sources are available as benchtop instruments, rackmount instruments, embeddable modules and in card-level formats. A maser is a solid-state device that amplifies microwaves using similar principles to the laser, which amplifies higher-frequency light waves. All warm objects emit low level microwave black-body radiation, depending on their temperature, so in meteorology and remote sensing, microwave radiometers are used to measure the temperature of objects or terrain. The sun and other astronomical radio sources such as Cassiopeia A emit low level microwave radiation which carries information about their makeup, which is studied by radio astronomers using receivers called radio telescopes. The cosmic microwave background radiation (CMBR), for example, is a weak microwave noise filling empty space which is a major source of information on cosmology's Big Bang theory of the origin of the Universe. Applications Microwave technology is extensively used for point-to-point telecommunications (i.e., non-broadcast uses). Microwaves are especially suitable for this use since they are more easily focused into narrower beams than radio waves, allowing frequency reuse; their comparatively higher frequencies allow broad bandwidth and high data transmission rates, and antenna sizes are smaller ", "canonical_url": "https://en.wikipedia.org/wiki/Microwave", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:57.998407", "metadata": {"word_count": 292, "text_length": 37308}}
{"id": "wiki_radio_waves", "query_word": "radio waves", "title": "Radio wave", "summary": "Radio waves (formerly called Hertzian waves) are a type of electromagnetic radiation with the lowest frequencies and the longest wavelengths in the electromagnetic spectrum, typically with frequencies below 300 gigahertz (GHz) and wavelengths greater than 1 millimeter (3⁄64 inch), about the diameter of a grain of rice. Radio waves with frequencies above about 1 GHz and wavelengths shorter than 30 centimeters are called microwaves. Like all electromagnetic waves, radio waves in vacuum travel at the speed of light, and in the Earth's atmosphere at a slightly lower speed. Radio waves are generated by charged particles undergoing acceleration, such as time-varying electric currents. Naturally occurring radio waves are emitted by lightning and astronomical objects, and are part of the blackbody radiation emitted by all warm objects. Radio waves are generated artificially by an electronic device called a transmitter, which is connected to an antenna, which radiates the waves. They are received by another antenna connected to a radio receiver, which processes the received signal. Radio waves are very commonly used in modern technology for fixed and mobile radio communication, broadcasting, radar and radio navigation systems, communications satellites, wireless computer networks, and many other applications. Different frequencies of radio waves have different propagation characteristics in the Earth's atmosphere; long waves can diffract around obstacles like mountains and follow the contour of the Earth (ground waves), shorter waves can reflect off the ionosphere and return to Earth beyond the horizon (skywaves), while much shorter wavelengths bend or diffract very little and travel on a line of sight, so their propagation distances are limited to the visual horizon. To prevent interference between different users, the artificial generation and use of radio waves is strictly regulated by law, coordinated by an international body called the International Telecommunication Union (ITU), which defines radio waves as \"electromagnetic waves of frequencies arbitrarily lower than 3000 GHz, propagated in space without artificial guide\". The radio spectrum is divided into a number of radio bands on the basis of frequency, allocated to different uses. Higher-frequency, shorter-wavelength radio waves are called microwaves.", "text": "Radio waves (formerly called Hertzian waves) are a type of electromagnetic radiation with the lowest frequencies and the longest wavelengths in the electromagnetic spectrum, typically with frequencies below 300 gigahertz (GHz) and wavelengths greater than 1 millimeter (3⁄64 inch), about the diameter of a grain of rice. Radio waves with frequencies above about 1 GHz and wavelengths shorter than 30 centimeters are called microwaves. Like all electromagnetic waves, radio waves in vacuum travel at the speed of light, and in the Earth's atmosphere at a slightly lower speed. Radio waves are generated by charged particles undergoing acceleration, such as time-varying electric currents. Naturally occurring radio waves are emitted by lightning and astronomical objects, and are part of the blackbody radiation emitted by all warm objects. Radio waves are generated artificially by an electronic device called a transmitter, which is connected to an antenna, which radiates the waves. They are received by another antenna connected to a radio receiver, which processes the received signal. Radio waves are very commonly used in modern technology for fixed and mobile radio communication, broadcasting, radar and radio navigation systems, communications satellites, wireless computer networks, and many other applications. Different frequencies of radio waves have different propagation characteristics in the Earth's atmosphere; long waves can diffract around obstacles like mountains and follow the contour of the Earth (ground waves), shorter waves can reflect off the ionosphere and return to Earth beyond the horizon (skywaves), while much shorter wavelengths bend or diffract very little and travel on a line of sight, so their propagation distances are limited to the visual horizon. To prevent interference between different users, the artificial generation and use of radio waves is strictly regulated by law, coordinated by an international body called the International Telecommunication Union (ITU), which defines radio waves as \"electromagnetic waves of frequencies arbitrarily lower than 3000 GHz, propagated in space without artificial guide\". The radio spectrum is divided into a number of radio bands on the basis of frequency, allocated to different uses. Higher-frequency, shorter-wavelength radio waves are called microwaves. Discovery and exploitation Radio waves were first predicted by the theory of electromagnetism that was proposed in 1867 by Scottish mathematical physicist James Clerk Maxwell. His mathematical theory, now called Maxwell's equations, predicted that a coupled electric and magnetic field could travel through space as an \"electromagnetic wave\". Maxwell proposed that light consisted of electromagnetic waves of very short wavelength. In 1887, German physicist Heinrich Hertz demonstrated the reality of Maxwell's electromagnetic waves by experimentally generating electromagnetic waves lower in frequency than light, radio waves, in his laboratory, showing that they exhibited the same wave properties as light: standing waves, refraction, diffraction, and polarization. Italian inventor Guglielmo Marconi developed the first practical radio transmitters and receivers around 1894–1895. He received the 1909 Nobel Prize in Physics for his radio work. Radio communication began to be used commercially around 1900. The modern term \"radio wave\" replaced the original name \"Hertzian wave\" around 1912. Generation and reception Radio waves are radiated by charged particles when they are accelerated. Natural sources of radio waves include radio noise produced by lightning and other natural processes in the Earth's atmosphere, and astronomical radio sources in space such as the Sun, galaxies and nebulas. All warm objects radiate high frequency radio waves (microwaves) as part of their black body radiation. Radio waves are produced artificially by time-varying electric currents, consisting of electrons flowing back and forth in a specially shaped metal conductor called an antenna. An electronic device called a radio transmitter applies oscillating electric current to the antenna, and the antenna radiates the power as radio waves. Radio waves are received by another antenna attached to a radio receiver. When radio waves strike the receiving antenna they push the electrons in the metal back and forth, creating tiny oscillating currents which are detected by the receiver. From quantum mechanics, like other electromagnetic radiation such as light, radio waves can alternatively be regarded as streams of uncharged elementary particles called photons. In an antenna transmitting radio waves, the electrons in the antenna emit the energy in discrete packets called radio photons, while in a receiving antenna the electrons absorb the energy as radio photons. An antenna is a coherent emitter of photons, like a laser, so the radio photons are all in phase. However, from Planck's relation E = h ν {\\displaystyle E=h\\nu } , the energy of individual radio photons is extremely small, from 10−22 to 10−30 joules. So the antenna of even a very low power transmitter emits an enormous number of photons every second. Therefore, except for certain molecular electron transition processes such as atoms in a maser emitting microwave photons, radio wave emission and absorption is usually regarded as a continuous classical process, governed by Maxwell's equations. Properties Radio waves in vacuum travel at the speed of light c {\\displaystyle c} . When passing through a material medium, they are slowed depending on the medium's permeability and permittivity. Air is tenuous enough that in the Earth's atmosphere radio waves travel at very nearly the speed of light. The wavelength λ {\\displaystyle \\lambda } is the distance from one peak (crest) of the wave's electric field to the next, and is inversely proportional to the frequency f {\\displaystyle f} of the wave. The relation of frequency and wavelength in a radio wave traveling in vacuum or air is λ = c f , {\\displaystyle \\lambda ={\\frac {\\;c\\;}{f}}~,} where c ≈ 2.9979 × 10 8 m/s . {\\displaystyle c\\approx 2.9979\\times 10^{8}{\\text{ m/s}}~.} Equivalently, c {\\displaystyle c} , the distance that a radio wave travels in vacuum in one second, is 299,792,458 meters (983,571,056 ft), which is the wavelength of a 1 hertz radio signal. A 1 megahertz radio wave (mid-AM band) has a wavelength of 299.79 meters (983.6 ft). Polarization Like other electromagnetic waves, a radio wave has a property called polarization, which is defined as the direction of the wave's oscillating electric field perpendicular to the direction of motion. A plane-polarized radio wave has an electric field that oscillates in a plane perpendicular to the direction of motion. In a horizontally polarized radio wave the electric field oscillates in a horizontal direction. In a vertically polarized wave the electric field oscillates in a vertical direction. In a circularly polarized wave the electric field at any point rotates about the direction of travel, once per cycle. A right circularly polarized wave rotates in a right-hand sense about the direction of travel, while a left circularly polarized wave rotates in the opposite sense. The wave's magnetic field is perpendicular to the electric field, and the electric and magnetic field are oriented in a right-hand sense with respect to the direction of radiation. An antenna emits polarized radio waves, with the polarization determined by the direction of the metal antenna elements. For example, a dipole antenna consists of two collinear metal rods. If the rods are horizontal, it radiates horizontally polarized radio waves, while if the rods are vertical, it radiates vertically polarized waves. An antenna receiving the radio waves must have the same polarization as the transmitting antenna, or it will suffer a severe loss of reception. Many natural sources of radio waves, such as the sun, stars and blackbody radiation from warm objects, emit unpolarized waves, consisting of incoherent short wave trains in an equal mixture of polarization states. The polarization of radio waves is determined by a quantum mechanical property of the photons called their spin. A photon can have one of two possible values of spin; it can spin in a right-hand sense about its direction of motion, or in a left-hand sense. Right circularly polarized radio waves consist of photons spinning in a right hand sense. Left circularly polarized radio waves consist of photons spinning in a left hand sense. Plane polarized radio waves consist of photons in a quantum superposition of right and left hand spin states. The electric field consists of a superposition of right and left rotating fields, resulting in a plane oscillation. Propagation characteristics Radio waves are more widely used for communication than other electromagnetic waves mainly because of their desirable propagation properties, stemming from their large wavelength. Radio waves have the ability to pass through the atmosphere in any weather, foliage, and through most building materials. By diffraction, longer wavelengths can bend around obstructions, and unlike other electromagnetic waves they tend to be scattered rather than absorbed by objects larger than their wavelength. The study of radio propagation, how radio waves move in free space and over the surface of the Earth, is vitally important in the design of practical radio systems. Radio waves passing through different environments experience reflection, refraction, polarization, diffraction, and absorption. Different frequencies experience different combinations of these phenomena in the Earth's atmosphere, making certain radio bands more useful for specific purposes than others. Practical radio systems mainly use three different techniques of radio propagation to communicate: Line of sight: This refers to radio waves that travel in a straight line from the transmitting antenna to the receiving antenna. It does not necessaril", "canonical_url": "https://en.wikipedia.org/wiki/Radio_wave", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:58.913511", "metadata": {"word_count": 341, "text_length": 21316}}
{"id": "wiki_radio_frequency", "query_word": "radio frequency", "title": "Radio frequency", "summary": "Radio frequency (RF) is the oscillation rate of an alternating electric current or voltage or of a magnetic, electric or electromagnetic field or mechanical system in the frequency range from around 20 kHz to around 300 GHz. This is roughly between the upper limit of audio frequencies that humans can hear (though these are not electromagnetic) and the lower limit of infrared frequencies, and also encompasses the microwave range. These are the frequencies at which energy from an oscillating current can radiate off a conductor into space as radio waves, so they are used in radio technology, among other uses. Different sources specify different upper and lower bounds for the frequency range.", "text": "Radio frequency (RF) is the oscillation rate of an alternating electric current or voltage or of a magnetic, electric or electromagnetic field or mechanical system in the frequency range from around 20 kHz to around 300 GHz. This is roughly between the upper limit of audio frequencies that humans can hear (though these are not electromagnetic) and the lower limit of infrared frequencies, and also encompasses the microwave range. These are the frequencies at which energy from an oscillating current can radiate off a conductor into space as radio waves, so they are used in radio technology, among other uses. Different sources specify different upper and lower bounds for the frequency range. Electric current Electric currents that oscillate at radio frequencies (RF currents) have special properties not shared by direct current or lower audio frequency alternating current, such as the 50 or 60 Hz current used in electrical power distribution. Energy from RF currents in conductors can radiate into space as electromagnetic waves (radio waves). This is the basis of radio technology. RF current does not penetrate deeply into electrical conductors but tends to flow along their surfaces; this is known as the skin effect. RF currents applied to the body often do not cause the painful sensation and muscular contraction of electric shock that lower frequency currents produce. This is because the current changes direction too quickly to trigger depolarization of nerve membranes. However, this does not mean RF currents are harmless; they can cause internal injury as well as serious superficial burns called RF burns. RF current can ionize air, creating a conductive path through it. This property is exploited by \"high frequency\" units used in electric arc welding, which use currents at higher frequencies than power distribution uses. Another property is the ability to appear to flow through paths that contain insulating material, like the dielectric insulator of a capacitor. This is because capacitive reactance in a circuit decreases with increasing frequency. In contrast, RF current can be blocked by a coil of wire, or even a single turn or bend in a wire. This is because the inductive reactance of a circuit increases with increasing frequency. When conducted by an ordinary electric cable, RF current has a tendency to reflect from discontinuities in the cable, such as connectors, and travel back down the cable toward the source, causing a condition called standing waves. RF current may be carried efficiently over transmission lines such as coaxial cables. Frequency bands The radio spectrum of frequencies is divided into bands with conventional names designated by the International Telecommunication Union (ITU): Frequencies of 1 GHz and above are conventionally called microwave, while frequencies of 30 GHz and above are designated millimeter wave. More detailed band designations are given by the standard IEEE letter- band frequency designations and the EU/NATO frequency designations. Applications Radio has many practical applications, which include broadcasting, voice communication, data communication, radar, radiolocation, medical treatments, and remote control. Measurement Test apparatus for radio frequencies can include standard instruments at the lower end of the range, but at higher frequencies, the test equipment becomes more specialized. Mechanical oscillations While RF usually refers to electrical oscillations, mechanical RF systems are not uncommon: see mechanical filter and RF MEMS. See also References External links Analog, RF and EMC Considerations in Printed Wiring Board (PWB) Design Definition of frequency bands (VLF, ELF ... etc.) IK1QFK Home Page (vlf.it) Radio, light, and sound waves, conversion between wavelength and frequency Archived 2012-03-11 at the Wayback Machine RF Terms Glossary Archived 2008-08-20 at the Wayback Machine", "canonical_url": "https://en.wikipedia.org/wiki/Radio_frequency", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:11:59.621721", "metadata": {"word_count": 112, "text_length": 3901}}
{"id": "wiki_RF", "query_word": "RF", "title": "Radio frequency", "summary": "Radio frequency (RF) is the oscillation rate of an alternating electric current or voltage or of a magnetic, electric or electromagnetic field or mechanical system in the frequency range from around 20 kHz to around 300 GHz. This is roughly between the upper limit of audio frequencies that humans can hear (though these are not electromagnetic) and the lower limit of infrared frequencies, and also encompasses the microwave range. These are the frequencies at which energy from an oscillating current can radiate off a conductor into space as radio waves, so they are used in radio technology, among other uses. Different sources specify different upper and lower bounds for the frequency range.", "text": "Radio frequency (RF) is the oscillation rate of an alternating electric current or voltage or of a magnetic, electric or electromagnetic field or mechanical system in the frequency range from around 20 kHz to around 300 GHz. This is roughly between the upper limit of audio frequencies that humans can hear (though these are not electromagnetic) and the lower limit of infrared frequencies, and also encompasses the microwave range. These are the frequencies at which energy from an oscillating current can radiate off a conductor into space as radio waves, so they are used in radio technology, among other uses. Different sources specify different upper and lower bounds for the frequency range. Electric current Electric currents that oscillate at radio frequencies (RF currents) have special properties not shared by direct current or lower audio frequency alternating current, such as the 50 or 60 Hz current used in electrical power distribution. Energy from RF currents in conductors can radiate into space as electromagnetic waves (radio waves). This is the basis of radio technology. RF current does not penetrate deeply into electrical conductors but tends to flow along their surfaces; this is known as the skin effect. RF currents applied to the body often do not cause the painful sensation and muscular contraction of electric shock that lower frequency currents produce. This is because the current changes direction too quickly to trigger depolarization of nerve membranes. However, this does not mean RF currents are harmless; they can cause internal injury as well as serious superficial burns called RF burns. RF current can ionize air, creating a conductive path through it. This property is exploited by \"high frequency\" units used in electric arc welding, which use currents at higher frequencies than power distribution uses. Another property is the ability to appear to flow through paths that contain insulating material, like the dielectric insulator of a capacitor. This is because capacitive reactance in a circuit decreases with increasing frequency. In contrast, RF current can be blocked by a coil of wire, or even a single turn or bend in a wire. This is because the inductive reactance of a circuit increases with increasing frequency. When conducted by an ordinary electric cable, RF current has a tendency to reflect from discontinuities in the cable, such as connectors, and travel back down the cable toward the source, causing a condition called standing waves. RF current may be carried efficiently over transmission lines such as coaxial cables. Frequency bands The radio spectrum of frequencies is divided into bands with conventional names designated by the International Telecommunication Union (ITU): Frequencies of 1 GHz and above are conventionally called microwave, while frequencies of 30 GHz and above are designated millimeter wave. More detailed band designations are given by the standard IEEE letter- band frequency designations and the EU/NATO frequency designations. Applications Radio has many practical applications, which include broadcasting, voice communication, data communication, radar, radiolocation, medical treatments, and remote control. Measurement Test apparatus for radio frequencies can include standard instruments at the lower end of the range, but at higher frequencies, the test equipment becomes more specialized. Mechanical oscillations While RF usually refers to electrical oscillations, mechanical RF systems are not uncommon: see mechanical filter and RF MEMS. See also References External links Analog, RF and EMC Considerations in Printed Wiring Board (PWB) Design Definition of frequency bands (VLF, ELF ... etc.) IK1QFK Home Page (vlf.it) Radio, light, and sound waves, conversion between wavelength and frequency Archived 2012-03-11 at the Wayback Machine RF Terms Glossary Archived 2008-08-20 at the Wayback Machine", "canonical_url": "https://en.wikipedia.org/wiki/Radio_frequency", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:00.261023", "metadata": {"word_count": 112, "text_length": 3901}}
{"id": "wiki_X-ray", "query_word": "X-ray", "title": "X-ray", "summary": "An X-ray is a form of high-energy electromagnetic radiation with a wavelength shorter than those of ultraviolet rays and longer than those of gamma rays. Roughly, X-rays have a wavelength ranging from 10 nanometers to 10 picometers, corresponding to frequencies in the range of 30 petahertz to 30 exahertz (3×1016 Hz to 3×1019 Hz) and photon energies in the range of 100 eV to 100 keV, respectively. X-rays were discovered in 1895 by the German scientist Wilhelm Conrad Röntgen, who named it X-radiation to signify an unknown type of radiation. X-rays can penetrate many solid substances such as construction materials and living tissue, so X-ray radiography is widely used in medical diagnostics (e.g., checking for broken bones) and materials science (e.g., identification of some chemical elements and detecting weak points in construction materials). However X-rays are ionizing radiation and exposure can be hazardous to health, causing DNA damage, cancer and, at higher intensities, burns and radiation sickness. Their generation and use is strictly controlled by public health authorities.", "text": "An X-ray is a form of high-energy electromagnetic radiation with a wavelength shorter than those of ultraviolet rays and longer than those of gamma rays. Roughly, X-rays have a wavelength ranging from 10 nanometers to 10 picometers, corresponding to frequencies in the range of 30 petahertz to 30 exahertz (3×1016 Hz to 3×1019 Hz) and photon energies in the range of 100 eV to 100 keV, respectively. X-rays were discovered in 1895 by the German scientist Wilhelm Conrad Röntgen, who named it X-radiation to signify an unknown type of radiation. X-rays can penetrate many solid substances such as construction materials and living tissue, so X-ray radiography is widely used in medical diagnostics (e.g., checking for broken bones) and materials science (e.g., identification of some chemical elements and detecting weak points in construction materials). However X-rays are ionizing radiation and exposure can be hazardous to health, causing DNA damage, cancer and, at higher intensities, burns and radiation sickness. Their generation and use is strictly controlled by public health authorities. History Pre-Röntgen observations and research X-rays were originally noticed in science as a type of unidentified radiation emanating from discharge tubes by experimenters investigating cathode rays produced by such tubes, which are energetic electron beams that were first observed in 1869. Early researchers noticed effects that were attributable to them in many of the early Crookes tubes (invented around 1875). Crookes tubes created free electrons by ionization of the residual air in the tube by a high DC voltage of anywhere between a few kilovolts and 100 kV. This voltage accelerated the electrons coming from the cathode to a high enough velocity that they created X-rays when they struck the anode or the glass wall of the tube. The earliest experimenter thought to have (unknowingly) produced X-rays was William Morgan. In 1785, he presented a paper to the Royal Society of London describing the effects of passing electrical currents through a partially evacuated glass tube, producing a glow created by X-rays. This work was further explored by Humphry Davy and his assistant Michael Faraday. Starting in 1888, Philipp Lenard conducted experiments to see whether cathode rays could pass out of the Crookes tube into the air. He built a Crookes tube with a \"window\" at the end made of thin aluminium, facing the cathode so the cathode rays would strike it (later called a \"Lenard tube\"). He found that something came through, that would expose photographic plates and cause fluorescence. He measured the penetrating power of these rays through various materials. It has been suggested that at least some of these \"Lenard rays\" were actually X-rays. Helmholtz formulated mathematical equations for X-rays. He postulated a dispersion theory before Röntgen made his discovery and announcement. He based it on the electromagnetic theory of light. However, he did not work with actual X-rays. In early 1890 photographer William Jennings and associate professor of the University of Pennsylvania Arthur W. Goodspeed were making photographs of coins with electric sparks. On 22 February after the end of their experiments two coins were left on a stack of photographic plates before Goodspeed demonstrated to Jennings the operation of Crookes tubes. While developing the plates, Jennings noticed disks of unknown origin on some of the plates, but nobody could explain them, and they moved on. Only in 1896 they realized that they accidentally made an X-ray photograph (they didn't claim a discovery). Also in 1890, Roentgen's assistant Ludwig Zehnder noticed a flash of light from a fluorescent screen immediately before the covered tube he was switching on punctured. When Stanford University physics professor Fernando Sanford conducted his \"electric photography\" experiments in 1891–1893 by photographing coins in the light of electric sparks, like Jennings and Goodspeed, he may have unknowingly generated and detected X-rays. His letter of 6 January 1893 to the Physical Review was duly published and an article entitled Without Lens or Light, Photographs Taken With Plate and Object in Darkness appeared in the San Francisco Examiner. In 1894, Nikola Tesla noticed damaged film in his lab that seemed to be associated with Crookes tube experiments and began investigating this invisible, radiant energy. After Röntgen identified the X-ray, Tesla began making X-ray images of his own using high voltages and tubes of his own design, as well as Crookes tubes. Discovery by Röntgen On 8 November 1895, German physics professor Wilhelm Röntgen discovered X-rays while experimenting with Lenard tubes and Crookes tubes and began studying them. He wrote an initial report \"On a new kind of ray: A preliminary communication\" and on 28 December 1895, submitted it to Würzburg's Physical-Medical Society journal. This was the first paper written on X-rays. Röntgen referred to the radiation as \"X\", to indicate that it was an unknown type of radiation. Some early texts refer to them as Chi-rays, having interpreted \"X\" as the uppercase Greek letter Chi, Χ. There are conflicting accounts of his discovery because Röntgen had his lab notes burned after his death, but this is a likely reconstruction by his biographers: Röntgen was investigating cathode rays from a Crookes tube which he had wrapped in black cardboard so that the visible light from the tube would not interfere, using a fluorescent screen painted with barium platinocyanide. He noticed a faint green glow from the screen, about 1 meter (3.3 ft) away. Röntgen realized some invisible rays coming from the tube were passing through the cardboard to make the screen glow. He found they could also pass through books and papers on his desk. Röntgen threw himself into investigating these unknown rays systematically. Two months after his initial discovery, he published his paper. Röntgen discovered their medical use when he made a picture of his wife's hand on a photographic plate formed due to X-rays. The photograph of his wife's hand was the first photograph of a human body part using X-rays. When she saw the picture, she said \"I have seen my death.\" The discovery of X-rays generated significant interest. Röntgen's biographer Otto Glasser estimated that, in 1896 alone, as many as 49 essays and 1044 articles about the new rays were published. This was probably a conservative estimate, if one considers that nearly every paper around the world extensively reported about the new discovery, with a magazine such as Science dedicating as many as 23 articles to it in that year alone. Sensationalist reactions to the new discovery included publications linking the new kind of rays to occult and paranormal theories, such as telepathy. The name X-rays stuck, although (over Röntgen's great objections) many of his colleagues suggested calling them Röntgen rays. They are still referred to as such in many languages, including German, Hungarian, Ukrainian, Danish, Polish, Czech, Bulgarian, Swedish, Finnish, Portuguese, Estonian, Slovak, Slovenian, Turkish, Russian, Latvian, Lithuanian, Albanian, Japanese, Dutch, Georgian, Hebrew, Icelandic, and Norwegian. Röntgen received the inaugural Nobel Prize in Physics for his discovery. Advances in radiology Röntgen immediately noticed X-rays could have medical applications. Along with his 28 December Physical-Medical Society submission, he sent a letter to physicians he knew around Europe (1 January 1896). News (and the creation of \"shadowgrams\") spread rapidly with Scottish electrical engineer Alan Archibald Campbell-Swinton being the first after Röntgen to create an X-ray photograph (of a hand). Through February, there were 46 experimenters taking up the technique in North America alone. The first use of X-rays under clinical conditions was by John Hall-Edwards in Birmingham, England on 11 January 1896, when he radiographed a needle stuck in the hand of an associate. On 14 February 1896, Hall-Edwards was also the first to use X-rays in a surgical operation. In early 1896, several weeks after Röntgen's discovery, Ivan Romanovich Tarkhanov irradiated frogs and insects with X-rays, concluding that the rays \"not only photograph, but also affect the living function\". At around the same time, the zoological illustrator James Green began to use X-rays to examine fragile specimens. George Albert Boulenger first mentioned this work in a paper he delivered before the Zoological Society of London in May 1896. The book Sciagraphs of British Batrachians and Reptiles (sciagraph is an obsolete name for an X-ray photograph), by Green and James H. Gardiner, with a foreword by Boulenger, was published in 1897. The first medical X-ray made in the United States was obtained using a discharge tube of Ivan Puluj's design. In January 1896, on reading of Röntgen's discovery, Frank Austin of Dartmouth College tested all of the discharge tubes in the physics laboratory and found that only the Puluj tube produced X-rays. This was a result of Puluj's inclusion of an oblique \"target\" of mica, used for holding samples of fluorescent material, within the tube. On 3 February 1896, Gilman Frost, professor of medicine at the college, and his brother Edwin Frost, professor of physics, exposed the wrist of Eddie McCarthy, whom Gilman had treated some weeks earlier for a fracture, to the X-rays and collected the resulting image of the broken bone on gelatin photographic plates obtained from Howard Langill, a local photographer also interested in Röntgen's work. Many experimenters, including Röntgen himself in his original experiments, came up with methods to view X-ray images \"live\" using some form of luminescent screen. Röntgen used a screen coated with barium platinocyanide. On 5 February 1896, live imaging devices were developed by both Italian scientist Enrico Salvioni (his \"cryptoscope\") and William Francis Magie of Princeton University (his \"Skiascope\"), bot", "canonical_url": "https://en.wikipedia.org/wiki/X-ray", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:01.420181", "metadata": {"word_count": 168, "text_length": 51712}}
{"id": "wiki_X-ray_spectrum", "query_word": "X-ray spectrum", "title": "X-ray spectroscopy", "summary": "X-ray spectroscopy is a general term for several spectroscopic techniques for characterization of materials by using x-ray radiation.", "text": "X-ray spectroscopy is a general term for several spectroscopic techniques for characterization of materials by using x-ray radiation. Characteristic X-ray spectroscopy When an electron from the inner shell of an atom is excited by the energy of a photon, it moves to a higher energy level. When it returns to the low energy level, the energy it previously gained by excitation is emitted as a photon of one of the wavelengths uniquely characteristic of the element. Analysis of the X-ray emission spectrum produces qualitative results about the elemental composition of the specimen. Comparison of the specimen's spectrum with the spectra of samples of known composition produces quantitative results (after some mathematical corrections for absorption, fluorescence and atomic number). Atoms can be excited by a high-energy beam of charged particles such as electrons (in an electron microscope for example), protons (see PIXE) or a beam of X-rays (see X-ray fluorescence, or XRF or also recently in transmission XRT). These methods enable elements from the entire periodic table to be analysed, with the exception of H, He and Li. In electron microscopy an electron beam excites X-rays; there are two main techniques for analysis of spectra of characteristic X-ray radiation: energy-dispersive X-ray spectroscopy (EDS) and wavelength dispersive X-ray spectroscopy (WDS). In X-ray transmission (XRT), the equivalent atomic composition (Zeff) is captured based on photoelectric and Compton effects. Energy-dispersive X-ray spectroscopy In an energy-dispersive X-ray spectrometer, a semiconductor detector measures energy of incoming photons. To maintain detector integrity and resolution it should be cooled with liquid nitrogen or by Peltier cooling. EDS is widely employed in electron microscopes (where imaging rather than spectroscopy is a main task) and in cheaper and/or portable XRF units. Wavelength-dispersive X-ray spectroscopy In a wavelength-dispersive X-ray spectrometer, a single crystal diffracts the photons according to Bragg's law, which are then collected by a detector. By moving the diffraction crystal and detector relative to each other, a wide region of the spectrum can be observed. To observe a large spectral range, three of four different single crystals may be needed. In contrast to EDS, WDS is a method of sequential spectrum acquisition. While WDS is slower than EDS and more sensitive to the positioning of the sample in the spectrometer, it has superior spectral resolution and sensitivity. WDS is widely used in microprobes (where X-ray microanalysis is the main task) and in XRF; it is widely used in the field of X-ray diffraction to calculate various data such as interplanar spacing and wavelength of the incident X-ray using Bragg's law. X-ray emission spectroscopy The father-and-son scientific team of William Lawrence Bragg and William Henry Bragg, who were 1915 Nobel Prize Winners, were the original pioneers in developing X-ray emission spectroscopy. An example of a spectrometer developed by William Henry Bragg, which was used by both father and son to investigate the structure of crystals, can be seen at the Science Museum, London. Jointly they measured the X-ray wavelengths of many elements to high precision, using high-energy electrons as excitation source. The cathode-ray tube or an x-ray tube was the method used to pass electrons through a crystal of numerous elements. They also painstakingly produced numerous diamond-ruled glass diffraction gratings for their spectrometers. The law of diffraction of a crystal is called Bragg's law in their honor. Intense and wavelength-tunable X-rays are now typically generated with synchrotrons. In a material, the X-rays may suffer an energy loss compared to the incoming beam. This energy loss of the re-emerging beam reflects an internal excitation of the atomic system, an X-ray analogue to the well-known Raman spectroscopy that is widely used in the optical region. In the X-ray region there is sufficient energy to probe changes in the electronic state (transitions between orbitals; this is in contrast with the optical region, where the energy emitted or absorbed is often due to changes in the state of the rotational or vibrational degrees of freedom of the system's atoms and groups of atoms). For instance, in the ultra soft X-ray region (below about 1 keV), crystal field excitations give rise to the energy loss. The photon-in-photon-out process may be thought of as a scattering event. When the x-ray energy corresponds to the binding energy of a core-level electron, this scattering process is resonantly enhanced by many orders of magnitude. This type of X-ray emission spectroscopy is often referred to as resonant inelastic X-ray scattering (RIXS). Due to the wide separation of orbital energies of the core levels, it is possible to select a certain atom of interest. The small spatial extent of core level orbitals forces the RIXS process to reflect the electronic structure in close vicinity of the chosen atom. Thus, RIXS experiments give valuable information about the local electronic structure of complex systems, and theoretical calculations are relatively simple to perform. Instrumentation There exist several efficient designs for analyzing an X-ray emission spectrum in the ultra soft X-ray region. The figure of merit for such instruments is the spectral throughput, i.e. the product of detected intensity and spectral resolving power. Usually, it is possible to change these parameters within a certain range while keeping their product constant. Grating spectrometers Usually X-ray diffraction in spectrometers is achieved on crystals, but in Grating spectrometers, the X-rays emerging from a sample must pass a source-defining slit, then optical elements (mirrors and/or gratings) disperse them by diffraction according to their wavelength and, finally, a detector is placed at their focal points. Spherical grating mounts Henry Augustus Rowland (1848–1901) devised an instrument that allowed the use of a single optical element that combines diffraction and focusing: a spherical grating. Reflectivity of X-rays is low, regardless of the used material and therefore, grazing incidence upon the grating is necessary. X-ray beams impinging on a smooth surface at a few degrees glancing angle of incidence undergo external total reflection which is taken advantage of to enhance the instrumental efficiency substantially. R denotes the radius of curvature of a spherical grating. The Rowland circle is an imaginary circle with a radius of R/2, positioned so that it is tangent to the center of the spherical grating surface. When both the entrance slit and the detector lie on this Rowland circle, light incident on the grating is both diffracted and focused by the same spherical grating. Each wavelength is diffracted at a specific angle according to Bragg's law, and the Rowland geometry ensures the diffracted beams come into focus along the circle. This setup allows for high spectral energy resolution without the need for additional focusing optics. Plane grating mounts Similar to optical spectrometers, a plane grating spectrometer first needs optics that turns the divergent rays emitted by the x-ray source into a parallel beam. This may be achieved by using a parabolic mirror. The parallel rays emerging from this mirror strike a plane grating (with constant groove distance) at the same angle and are diffracted according to their wavelength. A second parabolic mirror then collects the diffracted rays at a certain angle and creates an image on a detector. A spectrum within a certain wavelength range can be recorded simultaneously by using a two-dimensional position-sensitive detector such as a microchannel photomultiplier plate or an X-ray sensitive CCD chip (film plates are also possible to use). Interferometers Instead of using the concept of multiple beam interference that gratings produce, the two rays may simply interfere. By recording the intensity of two such co-linearly at some fixed point and changing their relative phase one obtains an intensity spectrum as a function of path length difference. One can show that this is equivalent to a Fourier transformed spectrum as a function of frequency. The highest recordable frequency of such a spectrum is dependent on the minimum step size chosen in the scan and the frequency resolution (i.e. how well a certain wave can be defined in terms of its frequency) depends on the maximum path length difference achieved. The latter feature allows a much more compact design for achieving high resolution than for a grating spectrometer because x-ray wavelengths are small compared to attainable path length differences. Early history of X-ray spectroscopy in the U.S. Philips Gloeilampen Fabrieken, headquartered in Eindhoven in the Netherlands, got its start as a manufacturer of light bulbs, but quickly evolved until it is now one of the leading manufacturers of electrical apparatus, electronics, and related products including X-ray equipment. It also has had one of the world's largest R&D labs. In 1940, the Netherlands was overrun by Hitler’s Germany. The company was able to transfer a substantial sum of money to a company that it set up as an R&D laboratory in an estate in Irvington on the Hudson in NY. As an extension to their work on light bulbs, the Dutch company had developed a line of X-ray tubes for medical applications that were powered by transformers. These X-ray tubes could also be used in scientific X-ray instrumentations, but there was very little commercial demand for the latter. As a result, management decided to try to develop this market and they set up development groups in their research labs in both Holland and the United States. They hired Dr. Ira Duffendack, a professor at University of Michigan and a world expert on infrared research to head the lab and to hire a staff. In 1951 he hired Dr. David Miller as Assistant Director of Research. Dr. M", "canonical_url": "https://en.wikipedia.org/wiki/X-ray_spectroscopy", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:02.265084", "metadata": {"word_count": 18, "text_length": 16078}}
{"id": "wiki_gamma_ray", "query_word": "gamma ray", "title": "Gamma ray", "summary": "A gamma ray, also known as gamma radiation (symbol γ), is a penetrating form of electromagnetic radiation arising from high-energy interactions like the radioactive decay of atomic nuclei or astronomical events like solar flares. It consists of the shortest wavelength electromagnetic waves, typically shorter than those of X-rays. With frequencies above 30 exahertz (3×1019 Hz) and wavelengths less than 10 picometers (1×10−11 m), gamma ray photons have the highest photon energy of any form of electromagnetic radiation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900 while studying radiation emitted by radium. In 1903, Ernest Rutherford named this radiation gamma rays based on their relatively strong penetration of matter; in 1900, he had already named two less penetrating types of decay radiation (discovered by Henri Becquerel) alpha rays and beta rays in ascending order of penetrating power. Gamma rays from radioactive decay are in the energy range from a few kiloelectronvolts (keV) to approximately 8 megaelectronvolts (MeV), corresponding to the typical energy levels in nuclei with reasonably long lifetimes. The energy spectrum of gamma rays can be used to identify the decaying radionuclides using gamma spectroscopy. Very-high-energy gamma rays in the 100–1000 teraelectronvolt (TeV) range have been observed from astronomical sources such as the Cygnus X-3 microquasar. Natural sources of gamma rays originating on Earth are mostly a result of radioactive decay and secondary radiation from atmospheric interactions with cosmic ray particles. However, there are other rare natural sources, such as terrestrial gamma-ray flashes, which produce gamma rays from electron action upon the nucleus. Notable artificial sources of gamma rays include fission, such as that which occurs in nuclear reactors, and high energy physics experiments, such as neutral pion decay and nuclear fusion. The energy ranges of gamma rays and X-rays overlap in the electromagnetic spectrum, so the terminology for these electromagnetic waves varies between scientific disciplines. In some fields of physics, they are distinguished by their origin: gamma rays are created by nuclear decay while X-rays originate outside the nucleus. In astrophysics, gamma rays are conventionally defined as having photon energies above 100 keV and are the subject of gamma-ray astronomy, while radiation below 100 keV is classified as X-rays and is the subject of X-ray astronomy. Gamma rays are ionizing radiation and are thus hazardous to life. They can cause DNA mutations, cancer and tumors, and at high doses burns and radiation sickness. Due to their high penetration power, they can damage bone marrow and internal organs. Unlike alpha and beta rays, they easily pass through the body and thus pose a formidable radiation protection challenge, requiring shielding made from dense materials such as lead or concrete. On Earth, the magnetosphere protects life from most types of lethal cosmic radiation other than gamma rays.", "text": "A gamma ray, also known as gamma radiation (symbol γ), is a penetrating form of electromagnetic radiation arising from high-energy interactions like the radioactive decay of atomic nuclei or astronomical events like solar flares. It consists of the shortest wavelength electromagnetic waves, typically shorter than those of X-rays. With frequencies above 30 exahertz (3×1019 Hz) and wavelengths less than 10 picometers (1×10−11 m), gamma ray photons have the highest photon energy of any form of electromagnetic radiation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900 while studying radiation emitted by radium. In 1903, Ernest Rutherford named this radiation gamma rays based on their relatively strong penetration of matter; in 1900, he had already named two less penetrating types of decay radiation (discovered by Henri Becquerel) alpha rays and beta rays in ascending order of penetrating power. Gamma rays from radioactive decay are in the energy range from a few kiloelectronvolts (keV) to approximately 8 megaelectronvolts (MeV), corresponding to the typical energy levels in nuclei with reasonably long lifetimes. The energy spectrum of gamma rays can be used to identify the decaying radionuclides using gamma spectroscopy. Very-high-energy gamma rays in the 100–1000 teraelectronvolt (TeV) range have been observed from astronomical sources such as the Cygnus X-3 microquasar. Natural sources of gamma rays originating on Earth are mostly a result of radioactive decay and secondary radiation from atmospheric interactions with cosmic ray particles. However, there are other rare natural sources, such as terrestrial gamma-ray flashes, which produce gamma rays from electron action upon the nucleus. Notable artificial sources of gamma rays include fission, such as that which occurs in nuclear reactors, and high energy physics experiments, such as neutral pion decay and nuclear fusion. The energy ranges of gamma rays and X-rays overlap in the electromagnetic spectrum, so the terminology for these electromagnetic waves varies between scientific disciplines. In some fields of physics, they are distinguished by their origin: gamma rays are created by nuclear decay while X-rays originate outside the nucleus. In astrophysics, gamma rays are conventionally defined as having photon energies above 100 keV and are the subject of gamma-ray astronomy, while radiation below 100 keV is classified as X-rays and is the subject of X-ray astronomy. Gamma rays are ionizing radiation and are thus hazardous to life. They can cause DNA mutations, cancer and tumors, and at high doses burns and radiation sickness. Due to their high penetration power, they can damage bone marrow and internal organs. Unlike alpha and beta rays, they easily pass through the body and thus pose a formidable radiation protection challenge, requiring shielding made from dense materials such as lead or concrete. On Earth, the magnetosphere protects life from most types of lethal cosmic radiation other than gamma rays. History of discovery The first gamma ray source to be discovered was the radioactive decay process called gamma decay. In this type of decay, an excited nucleus emits a gamma ray almost immediately upon formation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900, while studying radiation emitted from radium. Villard knew that his described radiation was more powerful than previously described types of rays from radium, which included beta rays, first noted as \"radioactivity\" by Henri Becquerel in 1896, and alpha rays, discovered as a less penetrating form of radiation by Rutherford, in 1899. However, Villard did not consider naming them as a different fundamental type. Later, in 1903, Villard's radiation was recognized as being of a type fundamentally different from previously named rays by Ernest Rutherford, who named Villard's rays \"gamma rays\" by analogy with the beta and alpha rays that Rutherford had differentiated in 1899. The \"rays\" emitted by radioactive elements were named in order of their power to penetrate various materials, using the first three letters of the Greek alphabet: alpha rays as the least penetrating, followed by beta rays, followed by gamma rays as the most penetrating. Rutherford also noted that gamma rays were not deflected (or at least, not easily deflected) by a magnetic field, another property making them unlike alpha and beta rays. Gamma rays were first thought to be particles with mass, like alpha and beta rays. Rutherford initially believed that they might be extremely fast beta particles, but their failure to be deflected by a magnetic field indicated that they had no charge. In 1914, gamma rays were observed to be reflected from crystal surfaces, proving that they were electromagnetic radiation. Rutherford and his co-worker Edward Andrade measured the wavelengths of gamma rays from radium, and found they were similar to X-rays, but with shorter wavelengths and thus, higher frequency. This was eventually recognized as giving them more energy per photon, as soon as the latter term became generally accepted. A gamma decay was then understood to usually emit a gamma photon. Sources Natural sources of gamma rays on Earth include gamma decay from naturally occurring radioisotopes such as potassium-40, and also as a secondary radiation from various atmospheric interactions with cosmic ray particles. Natural terrestrial sources that produce gamma rays include lightning strikes and terrestrial gamma-ray flashes, which produce high energy emissions from natural high-energy voltages. Gamma rays are produced by a number of astronomical processes in which very high-energy electrons are produced. Such electrons produce secondary gamma rays by the mechanisms of bremsstrahlung, inverse Compton scattering and synchrotron radiation. A large fraction of such astronomical gamma rays are screened by Earth's atmosphere. Notable artificial sources of gamma rays include fission, such as occurs in nuclear reactors, as well as high energy physics experiments, such as neutral pion decay and nuclear fusion. A sample of gamma ray-emitting material that is used for irradiating or imaging is known as a gamma source. It is also called a radioactive source, isotope source, or radiation source, though these more general terms also apply to alpha and beta-emitting devices. Gamma sources are usually sealed to prevent radioactive contamination, and transported in heavy shielding. Radioactive decay (gamma decay) Gamma rays are produced during gamma decay, which normally occurs after other forms of decay occur, such as alpha or beta decay. A radioactive nucleus can decay by the emission of an α or β particle. The daughter nucleus that results is usually left in an excited state. It can then decay to a lower energy state by emitting a gamma ray photon, in a process called gamma decay. The emission of a gamma ray from an excited nucleus typically requires only 10−12 seconds. Gamma decay may also follow nuclear reactions such as neutron capture, nuclear fission, or nuclear fusion. Gamma decay is also a mode of relaxation of many excited states of atomic nuclei following other types of radioactive decay, such as beta decay, so long as these states possess the necessary component of nuclear spin. When high-energy gamma rays, electrons, or protons bombard materials, the excited atoms emit characteristic \"secondary\" gamma rays, which are products of the creation of excited nuclear states in the bombarded atoms. Such transitions, a form of nuclear gamma fluorescence, form a topic in nuclear physics called gamma spectroscopy. Formation of fluorescent gamma rays are a rapid subtype of radioactive gamma decay. In certain cases, the excited nuclear state that follows the emission of a beta particle or other type of excitation, may be more stable than average, and is termed a metastable excited state, if its decay takes (at least) 100 to 1000 times longer than the average 10−12 seconds. Such relatively long-lived excited nuclei are termed nuclear isomers, and their decays are termed isomeric transitions. Such nuclei have half-lifes that are more easily measurable, and rare nuclear isomers are able to stay in their excited state for minutes, hours, days, or occasionally far longer, before emitting a gamma ray. The process of isomeric transition is therefore similar to any gamma emission, but differs in that it involves the intermediate metastable excited state(s) of the nuclei. Metastable states are often characterized by high nuclear spin, requiring a change in spin of several units or more with gamma decay, instead of a single unit transition that occurs in only 10−12 seconds. The rate of gamma decay is also slowed when the energy of excitation of the nucleus is small. An emitted gamma ray from any type of excited state may transfer its energy directly to any electrons, but most probably to one of the K shell electrons of the atom, causing it to be ejected from that atom, in a process generally termed the photoelectric effect (external gamma rays and ultraviolet rays may also cause this effect). The photoelectric effect should not be confused with the internal conversion process, in which a gamma ray photon is not produced as an intermediate particle (rather, a \"virtual gamma ray\" may be thought to mediate the process). Decay schemes One example of gamma ray production due to radionuclide decay is the decay scheme for cobalt-60, as illustrated in the accompanying diagram. First, 60Co decays to excited 60Ni by beta decay emission of an electron of 0.31 MeV. Then the excited 60Ni decays to the ground state (see nuclear shell model) by emitting gamma rays in succession of 1.17 MeV followed by 1.33 MeV. This path is followed 99.88% of the time: Another example is the alpha decay of 241Am to form 237Np; which is followed by gamma emission. In some cases, the gamma emission spectrum of the daughter nucleus is quite ", "canonical_url": "https://en.wikipedia.org/wiki/Gamma_ray", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:03.184871", "metadata": {"word_count": 459, "text_length": 37975}}
{"id": "wiki_gamma_radiation", "query_word": "gamma radiation", "title": "Gamma ray", "summary": "A gamma ray, also known as gamma radiation (symbol γ), is a penetrating form of electromagnetic radiation arising from high-energy interactions like the radioactive decay of atomic nuclei or astronomical events like solar flares. It consists of the shortest wavelength electromagnetic waves, typically shorter than those of X-rays. With frequencies above 30 exahertz (3×1019 Hz) and wavelengths less than 10 picometers (1×10−11 m), gamma ray photons have the highest photon energy of any form of electromagnetic radiation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900 while studying radiation emitted by radium. In 1903, Ernest Rutherford named this radiation gamma rays based on their relatively strong penetration of matter; in 1900, he had already named two less penetrating types of decay radiation (discovered by Henri Becquerel) alpha rays and beta rays in ascending order of penetrating power. Gamma rays from radioactive decay are in the energy range from a few kiloelectronvolts (keV) to approximately 8 megaelectronvolts (MeV), corresponding to the typical energy levels in nuclei with reasonably long lifetimes. The energy spectrum of gamma rays can be used to identify the decaying radionuclides using gamma spectroscopy. Very-high-energy gamma rays in the 100–1000 teraelectronvolt (TeV) range have been observed from astronomical sources such as the Cygnus X-3 microquasar. Natural sources of gamma rays originating on Earth are mostly a result of radioactive decay and secondary radiation from atmospheric interactions with cosmic ray particles. However, there are other rare natural sources, such as terrestrial gamma-ray flashes, which produce gamma rays from electron action upon the nucleus. Notable artificial sources of gamma rays include fission, such as that which occurs in nuclear reactors, and high energy physics experiments, such as neutral pion decay and nuclear fusion. The energy ranges of gamma rays and X-rays overlap in the electromagnetic spectrum, so the terminology for these electromagnetic waves varies between scientific disciplines. In some fields of physics, they are distinguished by their origin: gamma rays are created by nuclear decay while X-rays originate outside the nucleus. In astrophysics, gamma rays are conventionally defined as having photon energies above 100 keV and are the subject of gamma-ray astronomy, while radiation below 100 keV is classified as X-rays and is the subject of X-ray astronomy. Gamma rays are ionizing radiation and are thus hazardous to life. They can cause DNA mutations, cancer and tumors, and at high doses burns and radiation sickness. Due to their high penetration power, they can damage bone marrow and internal organs. Unlike alpha and beta rays, they easily pass through the body and thus pose a formidable radiation protection challenge, requiring shielding made from dense materials such as lead or concrete. On Earth, the magnetosphere protects life from most types of lethal cosmic radiation other than gamma rays.", "text": "A gamma ray, also known as gamma radiation (symbol γ), is a penetrating form of electromagnetic radiation arising from high-energy interactions like the radioactive decay of atomic nuclei or astronomical events like solar flares. It consists of the shortest wavelength electromagnetic waves, typically shorter than those of X-rays. With frequencies above 30 exahertz (3×1019 Hz) and wavelengths less than 10 picometers (1×10−11 m), gamma ray photons have the highest photon energy of any form of electromagnetic radiation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900 while studying radiation emitted by radium. In 1903, Ernest Rutherford named this radiation gamma rays based on their relatively strong penetration of matter; in 1900, he had already named two less penetrating types of decay radiation (discovered by Henri Becquerel) alpha rays and beta rays in ascending order of penetrating power. Gamma rays from radioactive decay are in the energy range from a few kiloelectronvolts (keV) to approximately 8 megaelectronvolts (MeV), corresponding to the typical energy levels in nuclei with reasonably long lifetimes. The energy spectrum of gamma rays can be used to identify the decaying radionuclides using gamma spectroscopy. Very-high-energy gamma rays in the 100–1000 teraelectronvolt (TeV) range have been observed from astronomical sources such as the Cygnus X-3 microquasar. Natural sources of gamma rays originating on Earth are mostly a result of radioactive decay and secondary radiation from atmospheric interactions with cosmic ray particles. However, there are other rare natural sources, such as terrestrial gamma-ray flashes, which produce gamma rays from electron action upon the nucleus. Notable artificial sources of gamma rays include fission, such as that which occurs in nuclear reactors, and high energy physics experiments, such as neutral pion decay and nuclear fusion. The energy ranges of gamma rays and X-rays overlap in the electromagnetic spectrum, so the terminology for these electromagnetic waves varies between scientific disciplines. In some fields of physics, they are distinguished by their origin: gamma rays are created by nuclear decay while X-rays originate outside the nucleus. In astrophysics, gamma rays are conventionally defined as having photon energies above 100 keV and are the subject of gamma-ray astronomy, while radiation below 100 keV is classified as X-rays and is the subject of X-ray astronomy. Gamma rays are ionizing radiation and are thus hazardous to life. They can cause DNA mutations, cancer and tumors, and at high doses burns and radiation sickness. Due to their high penetration power, they can damage bone marrow and internal organs. Unlike alpha and beta rays, they easily pass through the body and thus pose a formidable radiation protection challenge, requiring shielding made from dense materials such as lead or concrete. On Earth, the magnetosphere protects life from most types of lethal cosmic radiation other than gamma rays. History of discovery The first gamma ray source to be discovered was the radioactive decay process called gamma decay. In this type of decay, an excited nucleus emits a gamma ray almost immediately upon formation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900, while studying radiation emitted from radium. Villard knew that his described radiation was more powerful than previously described types of rays from radium, which included beta rays, first noted as \"radioactivity\" by Henri Becquerel in 1896, and alpha rays, discovered as a less penetrating form of radiation by Rutherford, in 1899. However, Villard did not consider naming them as a different fundamental type. Later, in 1903, Villard's radiation was recognized as being of a type fundamentally different from previously named rays by Ernest Rutherford, who named Villard's rays \"gamma rays\" by analogy with the beta and alpha rays that Rutherford had differentiated in 1899. The \"rays\" emitted by radioactive elements were named in order of their power to penetrate various materials, using the first three letters of the Greek alphabet: alpha rays as the least penetrating, followed by beta rays, followed by gamma rays as the most penetrating. Rutherford also noted that gamma rays were not deflected (or at least, not easily deflected) by a magnetic field, another property making them unlike alpha and beta rays. Gamma rays were first thought to be particles with mass, like alpha and beta rays. Rutherford initially believed that they might be extremely fast beta particles, but their failure to be deflected by a magnetic field indicated that they had no charge. In 1914, gamma rays were observed to be reflected from crystal surfaces, proving that they were electromagnetic radiation. Rutherford and his co-worker Edward Andrade measured the wavelengths of gamma rays from radium, and found they were similar to X-rays, but with shorter wavelengths and thus, higher frequency. This was eventually recognized as giving them more energy per photon, as soon as the latter term became generally accepted. A gamma decay was then understood to usually emit a gamma photon. Sources Natural sources of gamma rays on Earth include gamma decay from naturally occurring radioisotopes such as potassium-40, and also as a secondary radiation from various atmospheric interactions with cosmic ray particles. Natural terrestrial sources that produce gamma rays include lightning strikes and terrestrial gamma-ray flashes, which produce high energy emissions from natural high-energy voltages. Gamma rays are produced by a number of astronomical processes in which very high-energy electrons are produced. Such electrons produce secondary gamma rays by the mechanisms of bremsstrahlung, inverse Compton scattering and synchrotron radiation. A large fraction of such astronomical gamma rays are screened by Earth's atmosphere. Notable artificial sources of gamma rays include fission, such as occurs in nuclear reactors, as well as high energy physics experiments, such as neutral pion decay and nuclear fusion. A sample of gamma ray-emitting material that is used for irradiating or imaging is known as a gamma source. It is also called a radioactive source, isotope source, or radiation source, though these more general terms also apply to alpha and beta-emitting devices. Gamma sources are usually sealed to prevent radioactive contamination, and transported in heavy shielding. Radioactive decay (gamma decay) Gamma rays are produced during gamma decay, which normally occurs after other forms of decay occur, such as alpha or beta decay. A radioactive nucleus can decay by the emission of an α or β particle. The daughter nucleus that results is usually left in an excited state. It can then decay to a lower energy state by emitting a gamma ray photon, in a process called gamma decay. The emission of a gamma ray from an excited nucleus typically requires only 10−12 seconds. Gamma decay may also follow nuclear reactions such as neutron capture, nuclear fission, or nuclear fusion. Gamma decay is also a mode of relaxation of many excited states of atomic nuclei following other types of radioactive decay, such as beta decay, so long as these states possess the necessary component of nuclear spin. When high-energy gamma rays, electrons, or protons bombard materials, the excited atoms emit characteristic \"secondary\" gamma rays, which are products of the creation of excited nuclear states in the bombarded atoms. Such transitions, a form of nuclear gamma fluorescence, form a topic in nuclear physics called gamma spectroscopy. Formation of fluorescent gamma rays are a rapid subtype of radioactive gamma decay. In certain cases, the excited nuclear state that follows the emission of a beta particle or other type of excitation, may be more stable than average, and is termed a metastable excited state, if its decay takes (at least) 100 to 1000 times longer than the average 10−12 seconds. Such relatively long-lived excited nuclei are termed nuclear isomers, and their decays are termed isomeric transitions. Such nuclei have half-lifes that are more easily measurable, and rare nuclear isomers are able to stay in their excited state for minutes, hours, days, or occasionally far longer, before emitting a gamma ray. The process of isomeric transition is therefore similar to any gamma emission, but differs in that it involves the intermediate metastable excited state(s) of the nuclei. Metastable states are often characterized by high nuclear spin, requiring a change in spin of several units or more with gamma decay, instead of a single unit transition that occurs in only 10−12 seconds. The rate of gamma decay is also slowed when the energy of excitation of the nucleus is small. An emitted gamma ray from any type of excited state may transfer its energy directly to any electrons, but most probably to one of the K shell electrons of the atom, causing it to be ejected from that atom, in a process generally termed the photoelectric effect (external gamma rays and ultraviolet rays may also cause this effect). The photoelectric effect should not be confused with the internal conversion process, in which a gamma ray photon is not produced as an intermediate particle (rather, a \"virtual gamma ray\" may be thought to mediate the process). Decay schemes One example of gamma ray production due to radionuclide decay is the decay scheme for cobalt-60, as illustrated in the accompanying diagram. First, 60Co decays to excited 60Ni by beta decay emission of an electron of 0.31 MeV. Then the excited 60Ni decays to the ground state (see nuclear shell model) by emitting gamma rays in succession of 1.17 MeV followed by 1.33 MeV. This path is followed 99.88% of the time: Another example is the alpha decay of 241Am to form 237Np; which is followed by gamma emission. In some cases, the gamma emission spectrum of the daughter nucleus is quite ", "canonical_url": "https://en.wikipedia.org/wiki/Gamma_ray", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:04.042519", "metadata": {"word_count": 459, "text_length": 37975}}
{"id": "wiki_spectral_region", "query_word": "spectral region", "title": "Spectral band", "summary": "Spectral bands are regions of a given spectrum, having a specific range of wavelengths or frequencies. Most often, it refers to electromagnetic bands, regions of the electromagnetic spectrum. More generally, spectral bands may also be means in the spectra of other types of signals, e.g., noise spectrum. A frequency band is an interval in the frequency domain, limited by a lower frequency and an upper frequency. For example, it may refer to a radio band, such as wireless communication standards set by the International Telecommunication Union.", "text": "Spectral bands are regions of a given spectrum, having a specific range of wavelengths or frequencies. Most often, it refers to electromagnetic bands, regions of the electromagnetic spectrum. More generally, spectral bands may also be means in the spectra of other types of signals, e.g., noise spectrum. A frequency band is an interval in the frequency domain, limited by a lower frequency and an upper frequency. For example, it may refer to a radio band, such as wireless communication standards set by the International Telecommunication Union. In nuclear physics In nuclear physics, spectral bands refer to the electromagnetic emission of polyatomic systems, including condensed materials, large molecules, etc. Each spectral line corresponds to the difference in two energy levels of an atom. In molecules, these levels can split. When the number of atoms is large, one gets a continuum of energy levels, the so-called spectral bands. They are often labeled in the same way as the monatomic lines. The bands may overlap. In general, the energy spectrum can be given by a density function, describing the number of energy levels of the quantum system for a given interval. Spectral bands have constant density, and when the bands overlap, the corresponding densities are added. Band spectra is the name given to a group of lines that are closely spaced and arranged in a regular sequence that appears to be a band. It is a colored band, separated by dark spaces on the two sides and arranged in a regular sequence. In one band, there are various sharp and wider color lines, that are closer on one side and wider on other. The intensity in each band falls off from definite limits and indistinct on the other side. In complete band spectra, there is a number lines in a band. This spectra is produced when the emitting substance is in the molecular state. Therefore, they are also called molecular spectra. It is emitted by a molecule in vacuum tube, C-arc core with metallic salt. The band spectrum is the combination of many different spectral lines, resulting from molecular vibrational, rotational, and electronic transition. Spectroscopy studies spectral bands for astronomy and other purposes. Other applications Many systems are characterized by the spectral band to which they respond. For example: Musical instruments produce different ranges of notes within the hearing range. The electromagnetic spectrum can be divided into many different ranges such as visible light, infrared or ultraviolet radiation, radio waves, X-rays and so on, and each of these ranges can in turn be divided into smaller ranges. A radio communications signal must occupy a range of frequencies carrying most of its energy, called its bandwidth. A frequency band may represent one communication channel or be subdivided into many. Allocation of radio frequency ranges to different uses is a major function of radio spectrum allocation. See also == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_band", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:04.671559", "metadata": {"word_count": 86, "text_length": 2952}}
{"id": "wiki_bandwidth", "query_word": "bandwidth", "title": "Bandwidth", "summary": "Bandwidth commonly refers to: Bandwidth (signal processing) or analog bandwidth, frequency bandwidth, or radio bandwidth, a measure of the width of a frequency range Bandwidth (computing), the rate of data transfer, bit rate or throughput Spectral linewidth, the width of an atomic or molecular spectral line Bandwidth may also refer to:", "text": "Bandwidth commonly refers to: Bandwidth (signal processing) or analog bandwidth, frequency bandwidth, or radio bandwidth, a measure of the width of a frequency range Bandwidth (computing), the rate of data transfer, bit rate or throughput Spectral linewidth, the width of an atomic or molecular spectral line Bandwidth may also refer to: Science and technology Bandwidth (linear algebra), the width of the non-zero terms around the diagonal of a matrix Kernel density estimation, the width of the convolution kernel used in statistics Graph bandwidth, in graph theory Coherence bandwidth, a frequency range over which a channel can be considered \"flat\" Power bandwidth, a frequency range for which power output of an amplifier exceeds a given fraction of full rated power Other uses Bandwidth (company), an American communications provider Bandwidth (radio program), a Canadian radio program Bandwidth, a normative expected range of linguistic behavior in language expectancy theory Bandwidth, the resources needed to complete a task or project in business jargon; see List of buzzwords See also Bit rate, in telecommunications and computing", "canonical_url": "https://en.wikipedia.org/wiki/Bandwidth", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:05.675146", "metadata": {"word_count": 51, "text_length": 1141}}
{"id": "wiki_spectral_bandwidth", "query_word": "spectral bandwidth", "title": "Bandwidth (signal processing)", "summary": "Bandwidth is the difference between the upper and lower frequencies in a continuous band of frequencies. It is typically measured in unit of hertz (symbol Hz). It may refer more specifically to two subcategories: Passband bandwidth is the difference between the upper and lower cutoff frequencies of, for example, a band-pass filter, a communication channel, or a signal spectrum. Baseband bandwidth is equal to the upper cutoff frequency of a low-pass filter or baseband signal, which includes a zero frequency. Bandwidth in hertz is a central concept in many fields, including electronics, information theory, digital communications, radio communications, signal processing, and spectroscopy and is one of the determinants of the capacity of a given communication channel. A key characteristic of bandwidth is that any band of a given width can carry the same amount of information, regardless of where that band is located in the frequency spectrum. For example, a 3 kHz band can carry a telephone conversation whether that band is at baseband (as in a POTS telephone line) or modulated to some higher frequency. However, wide bandwidths are easier to obtain and process at higher frequencies because the § Fractional bandwidth is smaller.", "text": "Bandwidth is the difference between the upper and lower frequencies in a continuous band of frequencies. It is typically measured in unit of hertz (symbol Hz). It may refer more specifically to two subcategories: Passband bandwidth is the difference between the upper and lower cutoff frequencies of, for example, a band-pass filter, a communication channel, or a signal spectrum. Baseband bandwidth is equal to the upper cutoff frequency of a low-pass filter or baseband signal, which includes a zero frequency. Bandwidth in hertz is a central concept in many fields, including electronics, information theory, digital communications, radio communications, signal processing, and spectroscopy and is one of the determinants of the capacity of a given communication channel. A key characteristic of bandwidth is that any band of a given width can carry the same amount of information, regardless of where that band is located in the frequency spectrum. For example, a 3 kHz band can carry a telephone conversation whether that band is at baseband (as in a POTS telephone line) or modulated to some higher frequency. However, wide bandwidths are easier to obtain and process at higher frequencies because the § Fractional bandwidth is smaller. Overview Bandwidth is a key concept in many telecommunications applications. In radio communications, for example, bandwidth is the frequency range occupied by a modulated carrier signal. An FM radio receiver's tuner spans a limited range of frequencies. A government agency (such as the Federal Communications Commission in the United States) may apportion the regionally available bandwidth to broadcast license holders so that their signals do not mutually interfere. In this context, bandwidth is also known as channel spacing. For other applications, there are other definitions. One definition of bandwidth, for a system, could be the range of frequencies over which the system produces a specified level of performance. A less strict and more practically useful definition will refer to the frequencies beyond which performance is degraded. In the case of frequency response, degradation could, for example, mean more than 3 dB below the maximum value or it could mean below a certain absolute value. As with any definition of the width of a function, many definitions are suitable for different purposes. In the context of, for example, the sampling theorem and Nyquist sampling rate, bandwidth typically refers to baseband bandwidth. In the context of Nyquist symbol rate or Shannon-Hartley channel capacity for communication systems it refers to passband bandwidth. The Rayleigh bandwidth of a simple radar pulse is defined as the inverse of its duration. For example, a one-microsecond pulse has a Rayleigh bandwidth of one megahertz. The essential bandwidth is defined as the portion of a signal spectrum in the frequency domain which contains most of the energy of the signal. x dB bandwidth In some contexts, the signal bandwidth in hertz refers to the frequency range in which the signal's spectral density (in W/Hz or V2/Hz) is nonzero or above a small threshold value. The threshold value is often defined relative to the maximum value, and is most commonly the 3 dB point, that is the point where the spectral density is half its maximum value (or the spectral amplitude, in V {\\displaystyle \\mathrm {V} } or V / H z {\\displaystyle \\mathrm {V/{\\sqrt {Hz}}} } , is 70.7% of its maximum). This figure, with a lower threshold value, can be used in calculations of the lowest sampling rate that will satisfy the sampling theorem. The bandwidth is also used to denote system bandwidth, for example in filter or communication channel systems. To say that a system has a certain bandwidth means that the system can process signals with that range of frequencies, or that the system reduces the bandwidth of a white noise input to that bandwidth. The 3 dB bandwidth of an electronic filter or communication channel is the part of the system's frequency response that lies within 3 dB of the response at its peak, which, in the passband filter case, is typically at or near its center frequency, and in the low-pass filter is at or near its cutoff frequency. If the maximum gain is 0 dB, the 3 dB bandwidth is the frequency range where attenuation is less than 3 dB. 3 dB attenuation is also where power is half its maximum. This same half-power gain convention is also used in spectral width, and more generally for the extent of functions as full width at half maximum (FWHM). In electronic filter design, a filter specification may require that within the filter passband, the gain is nominally 0 dB with a small variation, for example within the ±1 dB interval. In the stopband(s), the required attenuation in decibels is above a certain level, for example >100 dB. In a transition band the gain is not specified. In this case, the filter bandwidth corresponds to the passband width, which in this example is the 1 dB-bandwidth. If the filter shows amplitude ripple within the passband, the x dB point refers to the point where the gain is x dB below the nominal passband gain rather than x dB below the maximum gain. In signal processing and control theory the bandwidth is the frequency at which the closed-loop system gain drops 3 dB below peak. In communication systems, in calculations of the Shannon–Hartley channel capacity, bandwidth refers to the 3 dB-bandwidth. In calculations of the maximum symbol rate, the Nyquist sampling rate, and maximum bit rate according to the Hartley's law, the bandwidth refers to the frequency range within which the gain is non-zero. The fact that in equivalent baseband models of communication systems, the signal spectrum consists of both negative and positive frequencies, can lead to confusion about bandwidth since they are sometimes referred to only by the positive half, and one will occasionally see expressions such as B = 2 W {\\displaystyle B=2W} , where B {\\displaystyle B} is the total bandwidth (i.e. the maximum passband bandwidth of the carrier-modulated RF signal and the minimum passband bandwidth of the physical passband channel), and W {\\displaystyle W} is the positive bandwidth (the baseband bandwidth of the equivalent channel model). For instance, the baseband model of the signal would require a low-pass filter with cutoff frequency of at least W {\\displaystyle W} to stay intact, and the physical passband channel would require a passband filter of at least B {\\displaystyle B} to stay intact. Relative bandwidth The absolute bandwidth is not always the most appropriate or useful measure of bandwidth. For instance, in the field of antennas the difficulty of constructing an antenna to meet a specified absolute bandwidth is easier at a higher frequency than at a lower frequency. For this reason, bandwidth is often quoted relative to the frequency of operation which gives a better indication of the structure and sophistication needed for the circuit or device under consideration. There are two different measures of relative bandwidth in common use: fractional bandwidth ( B F {\\displaystyle B_{\\mathrm {F} }} ) and ratio bandwidth ( B R {\\displaystyle B_{\\mathrm {R} }} ). In the following, the absolute bandwidth is defined as follows, B = Δ f = f H − f L {\\displaystyle B=\\Delta f=f_{\\mathrm {H} }-f_{\\mathrm {L} }} where f H {\\displaystyle f_{\\mathrm {H} }} and f L {\\displaystyle f_{\\mathrm {L} }} are the upper and lower frequency limits respectively of the band in question. Fractional bandwidth Fractional bandwidth is defined as the absolute bandwidth divided by the center frequency ( f C {\\displaystyle f_{\\mathrm {C} }} ), B F = Δ f f C . {\\displaystyle B_{\\mathrm {F} }={\\frac {\\Delta f}{f_{\\mathrm {C} }}}\\,.} The center frequency is usually defined as the arithmetic mean of the upper and lower frequencies so that, f C = f H + f L 2 {\\displaystyle f_{\\mathrm {C} }={\\frac {f_{\\mathrm {H} }+f_{\\mathrm {L} }}{2}}\\ } and B F = 2 ( f H − f L ) f H + f L . {\\displaystyle B_{\\mathrm {F} }={\\frac {2(f_{\\mathrm {H} }-f_{\\mathrm {L} })}{f_{\\mathrm {H} }+f_{\\mathrm {L} }}}\\,.} However, the center frequency is sometimes defined as the geometric mean of the upper and lower frequencies, f C = f H f L {\\displaystyle f_{\\mathrm {C} }={\\sqrt {f_{\\mathrm {H} }f_{\\mathrm {L} }}}} and B F = f H − f L f H f L . {\\displaystyle B_{\\mathrm {F} }={\\frac {f_{\\mathrm {H} }-f_{\\mathrm {L} }}{\\sqrt {f_{\\mathrm {H} }f_{\\mathrm {L} }}}}\\,.} While the geometric mean is more rarely used than the arithmetic mean (and the latter can be assumed if not stated explicitly) the former is considered more mathematically rigorous. It more properly reflects the logarithmic relationship of fractional bandwidth with increasing frequency. For narrowband applications, there is only marginal difference between the two definitions. The geometric mean version is inconsequentially larger. For wideband applications they diverge substantially with the arithmetic mean version approaching 2 in the limit and the geometric mean version approaching infinity. Fractional bandwidth is sometimes expressed as a percentage of the center frequency (percent bandwidth, % B {\\displaystyle \\%B} ), % B F = 100 Δ f f C . {\\displaystyle \\%B_{\\mathrm {F} }=100{\\frac {\\Delta f}{f_{\\mathrm {C} }}}\\,.} Ratio bandwidth Ratio bandwidth is defined as the ratio of the upper and lower limits of the band, B R = f H f L . {\\displaystyle B_{\\mathrm {R} }={\\frac {f_{\\mathrm {H} }}{f_{\\mathrm {L} }}}\\,.} Ratio bandwidth may be notated as B R : 1 {\\displaystyle B_{\\mathrm {R} }:1} . The relationship between ratio bandwidth and fractional bandwidth is given by, B F = 2 B R − 1 B R + 1 {\\displaystyle B_{\\mathrm {F} }=2{\\frac {B_{\\mathrm {R} }-1}{B_{\\mathrm {R} }+1}}} and B R = 2 + B F 2 − B F . {\\displaystyle B_{\\mathrm {R} }={\\frac {2+B_{\\mathrm {F} }}{2-B_{\\mathrm {F} }}}\\,.} Percent bandwidth is a less meaningful measure in wideband applications. A percent bandwidth of 100% corresponds to a ratio b", "canonical_url": "https://en.wikipedia.org/wiki/Bandwidth_(signal_processing)", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:06.600682", "metadata": {"word_count": 195, "text_length": 12992}}
{"id": "wiki_narrowband", "query_word": "narrowband", "title": "Narrowband", "summary": "Narrowband signals are signals that occupy a narrow range of frequencies or that have a small fractional bandwidth. In the audio spectrum, narrowband sounds are sounds that occupy a narrow range of frequencies. In telephony, narrowband is usually considered to cover frequencies 300–3400 Hz, i.e. the voiceband. In radio communications, a narrowband channel is a channel in which the bandwidth of the message does not significantly exceed the channel's coherence bandwidth. In the study of wired channels, narrowband implies that the channel under consideration is sufficiently narrow that its frequency response can be considered flat. The message bandwidth will therefore be less than the coherence bandwidth of the channel. That is, no channel has perfectly flat fading, but the analysis of many aspects of wireless systems is greatly simplified if flat fading can be assumed.", "text": "Narrowband signals are signals that occupy a narrow range of frequencies or that have a small fractional bandwidth. In the audio spectrum, narrowband sounds are sounds that occupy a narrow range of frequencies. In telephony, narrowband is usually considered to cover frequencies 300–3400 Hz, i.e. the voiceband. In radio communications, a narrowband channel is a channel in which the bandwidth of the message does not significantly exceed the channel's coherence bandwidth. In the study of wired channels, narrowband implies that the channel under consideration is sufficiently narrow that its frequency response can be considered flat. The message bandwidth will therefore be less than the coherence bandwidth of the channel. That is, no channel has perfectly flat fading, but the analysis of many aspects of wireless systems is greatly simplified if flat fading can be assumed. Two-way radio narrowband Two-Way Radio Narrowbanding refers to a U.S. Federal Communications Commission (FCC) Order issued in December 2004 requiring all CFR 47 Part 90 VHF (150–174 MHz) and UHF (421–470 MHz) PLMR (Private Land Mobile Radio) licensees operating legacy wideband (25 kHz bandwidth) voice or data/SCADA systems to migrate to narrowband (12.5 kHz bandwidth or equivalent) systems by 1 January 2013. See also Broadband Electromagnetic interference Land mobile service Rural internet Ultra-wideband Wideband Narrowband IoT References External links \"FCC\" (govt. agency main website) – via fcc.gov. \"FCC Part 90 LMR VHF/UHF narrowbanding information and licensee resources\" – via wirelessradio.net. \"Narrowbanding resource guide\". 2013 – via narrowband.us.", "canonical_url": "https://en.wikipedia.org/wiki/Narrowband", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:07.253605", "metadata": {"word_count": 135, "text_length": 1646}}
{"id": "wiki_broadband", "query_word": "broadband", "title": "Broadband", "summary": "In telecommunications, broadband or high speed is the wide-bandwidth data transmission that uses signals at a wide spread of frequencies or several different simultaneous frequencies. It is used in fast Internet access where the transmission medium can be coaxial cable, optical fiber, wireless Internet (radio), twisted pair cable, or satellite. Originally used to mean \"using a wide-spread frequency\" and for services that were analog at the lowest level, in the context of Internet access, \"broadband\" is now often used to mean any high-speed Internet access that is seemingly always \"on\" and is faster than dial-up access over traditional analog or ISDN PSTN services. The ideal telecommunication network has the following characteristics: broadband, multi-media, multi-point, multi-rate and economical implementation for a diversity of services (multi-services). The Broadband Integrated Services Digital Network (B-ISDN) was planned to provide these characteristics. Asynchronous Transfer Mode (ATM) was promoted as a target technology for meeting these requirements.", "text": "In telecommunications, broadband or high speed is the wide-bandwidth data transmission that uses signals at a wide spread of frequencies or several different simultaneous frequencies. It is used in fast Internet access where the transmission medium can be coaxial cable, optical fiber, wireless Internet (radio), twisted pair cable, or satellite. Originally used to mean \"using a wide-spread frequency\" and for services that were analog at the lowest level, in the context of Internet access, \"broadband\" is now often used to mean any high-speed Internet access that is seemingly always \"on\" and is faster than dial-up access over traditional analog or ISDN PSTN services. The ideal telecommunication network has the following characteristics: broadband, multi-media, multi-point, multi-rate and economical implementation for a diversity of services (multi-services). The Broadband Integrated Services Digital Network (B-ISDN) was planned to provide these characteristics. Asynchronous Transfer Mode (ATM) was promoted as a target technology for meeting these requirements. Overview Different criteria for \"broad\" have been applied in different contexts and at different times. Its origin is in physics, acoustics, and radio systems engineering, where it had been used with a meaning similar to \"wideband\", or in the context of audio noise reduction systems, where it indicated a single-band rather than a multiple-audio-band system design of the compander. Later, with the advent of digital telecommunications, the term was mainly used for transmission over multiple channels. Whereas a passband signal is also modulated so that it occupies higher frequencies (compared to a baseband signal which is bound to the lowest end of the spectrum, see line coding), it is still occupying a single channel. The key difference is that what is typically considered a broadband signal in this sense is a signal that occupies multiple (non-masking, orthogonal) passbands, thus allowing for much higher throughput over a single medium but with additional complexity in the transmitter/receiver circuitry. The term became popularized through the 1990s as a marketing term for Internet access that was faster than dial-up access (dial-up being typically limited to a maximum of 56 kbit/s). This meaning is only distantly related to its original technical meaning. Since 1999, broadband Internet access has been a factor in public policy. In that year, at the World Trade Organization Biannual Conference called \"Financial Solutions to Digital Divide\" in Seattle, the term \"Meaningful Broadband\" was introduced to the world leaders, leading to the activation of a movement to close the digital divide. Fundamental aspects of this movement are to suggest that the equitable distribution of broadband is a fundamental human right. Personal computing facilitated easy access, manipulation, storage, and exchange of information, and required reliable data transmission. Communicating documents by images and the use of high-resolution graphics terminals provided a more natural and informative mode of human interaction than do voice and data alone. Video teleconferencing enhances group interaction at a distance. High-definition entertainment video improves the quality of pictures, but requires much higher transmission rates. These new data transmission requirements may require new transmission means other than the present overcrowded radio spectrum. A modern telecommunications network (such as the broadband network) must provide all these different services (multi-services) to the user. Differences from old telephony Conventional telephony communication used: the voice medium only, connected only two telephones per telephone call, and used circuits of fixed bit-rates. Modern services can be: multimedia, multi-point, and multirate. These aspects are examined individually in the following three sub-sections. Multimedia A multimedia call may communicate audio, data, still images, or full-motion video, or any combination of these media. Each medium has different demands for communication quality, such as: bandwidth requirement, signal latency within the network, and signal fidelity upon delivery by the network. The information content of each medium may affect the information generated by other media. For example, voice could be transcribed into data via voice recognition, and data commands may control the way voice and video are presented. These interactions most often occur at the communication terminals, but may also occur within the network. Multipoint Traditional voice calls are predominantly two party calls, requiring a point-to-point connection using only the voice medium. To access pictorial information in a remote database would require a point-to-point connection that sends low bit-rate queries to the database and high bit-rate video from the database. Entertainment video applications are largely point-to-multi-point connections, requiring one way communication of full motion video and audio from the program source to the viewers. Video teleconferencing involves connections among many parties, communicating voice, video, as well as data. Offering future services thus requires flexible management of the connection and media requests of a multipoint, multimedia communication call. Multirate A multirate service network is one which flexibly allocates transmission capacity to connections. A multimedia network has to support a broad range of bit-rates demanded by connections, not only because there are many communication media, but also because a communication medium may be encoded by algorithms with different bit-rates. For example, audio signals can be encoded with bit-rates ranging from less than 1 kbit/s to hundreds of kbit/s, using different encoding algorithms with a wide range of complexity and quality of audio reproduction. Similarly, full motion video signals may be encoded with bit-rates ranging from less than 1 Mbit/s to hundreds of Mbit/s. Thus a network transporting both video and audio signals may have to integrate traffic with a very broad range of bit-rates. A single network for multiple services Traditionally, different telecommunications services were carried via separate networks: voice on the telephone network, data on computer networks such as local area networks, video teleconferencing on private corporate networks, and television on broadcast radio or cable networks. These networks were largely engineered for a specific application and are not suited to other applications. For example, the traditional telephone network is too noisy and inefficient for bursty data communication. On the other hand, data networks which store and forward messages using computers had limited connectivity, usually did not have sufficient bandwidth for digitised voice and video signals, and suffer from unacceptable delays for the real-time signals. Television networks using radio or cables were largely broadcast networks with minimum switching facilities. It was desirable to have a single network for providing all these communication services to achieve the economy of sharing. This economy motivates the general idea of an integrated services network. Integration avoids the need for many overlaying networks, which complicates network management and reduces flexibility in the introduction and evolution of services. This integration was made possible with advances in broadband technologies and high-speed information processing of the 1990s. While multiple network structures were capable of supporting broadband services, an ever-increasing percentage of broadband and MSO providers opted for fibre-optic network structures to support both present and future bandwidth requirements. CATV (cable television), HDTV (high definition television), VoIP (voice over internet protocol), and broadband internet are some of the most common applications now being supported by fibre optic networks, in some cases directly to the home (FTTh – Fibre To The Home). These types of fibre optic networks incorporate a wide variety of products to support and distribute the signal from the central office to an optic node, and ultimately to the subscriber (end-user). Broadband technologies Telecommunications In telecommunications, a broadband signalling method is one that handles a wide band of frequencies. \"Broadband\" is a relative term, understood according to its context. The wider (or broader) the bandwidth of a channel, the greater the data-carrying capacity, given the same channel quality. In radio, for example, a very narrow band will carry Morse code, a broader band will carry speech, and a still broader band will carry music without losing the high audio frequencies required for realistic sound reproduction. This broad band is often divided into channels or \"frequency bins\" using passband techniques to allow frequency-division multiplexing instead of sending a higher-quality signal. In data communications, a 56k modem will transmit a data rate of 56 kilobits per second (kbit/s) over a 4-kilohertz-wide telephone line (narrowband or voiceband). In the late 1980s, the Broadband Integrated Services Digital Network (B-ISDN) used the term to refer to a broad range of bit rates, independent of physical modulation details. The various forms of digital subscriber line (DSL) services are broadband in the sense that digital information is sent over multiple channels. Each channel is at a higher frequency than the baseband voice channel, so it can support plain old telephone service on a single pair of wires at the same time. However, when that same line is converted to a non-loaded twisted-pair wire (no telephone filters), it becomes hundreds of kilohertz wide (broadband) and can carry up to 100 megabits per second using very high-bit rate digital subscriber line (VDSL or VHDSL) techniques. Modern networks have to carry integrated traffic consisting of voice, video and data. The Broadband Integrated Services Digital Ne", "canonical_url": "https://en.wikipedia.org/wiki/Broadband", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:08.141308", "metadata": {"word_count": 151, "text_length": 16491}}
{"id": "wiki_band_width", "query_word": "band width", "title": "Bandwidth", "summary": "Bandwidth commonly refers to: Bandwidth (signal processing) or analog bandwidth, frequency bandwidth, or radio bandwidth, a measure of the width of a frequency range Bandwidth (computing), the rate of data transfer, bit rate or throughput Spectral linewidth, the width of an atomic or molecular spectral line Bandwidth may also refer to:", "text": "Bandwidth commonly refers to: Bandwidth (signal processing) or analog bandwidth, frequency bandwidth, or radio bandwidth, a measure of the width of a frequency range Bandwidth (computing), the rate of data transfer, bit rate or throughput Spectral linewidth, the width of an atomic or molecular spectral line Bandwidth may also refer to: Science and technology Bandwidth (linear algebra), the width of the non-zero terms around the diagonal of a matrix Kernel density estimation, the width of the convolution kernel used in statistics Graph bandwidth, in graph theory Coherence bandwidth, a frequency range over which a channel can be considered \"flat\" Power bandwidth, a frequency range for which power output of an amplifier exceeds a given fraction of full rated power Other uses Bandwidth (company), an American communications provider Bandwidth (radio program), a Canadian radio program Bandwidth, a normative expected range of linguistic behavior in language expectancy theory Bandwidth, the resources needed to complete a task or project in business jargon; see List of buzzwords See also Bit rate, in telecommunications and computing", "canonical_url": "https://en.wikipedia.org/wiki/Bandwidth", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:10.077703", "metadata": {"word_count": 51, "text_length": 1141}}
{"id": "wiki_full_width_at_half_maximum", "query_word": "full width at half maximum", "title": "Full width at half maximum", "summary": "In a distribution, full width at half maximum (FWHM) is the difference between the two values of the independent variable at which the dependent variable is equal to half of its maximum value. In other words, it is the width of a spectrum curve measured between those points on the y-axis which are half the maximum amplitude. Half width at half maximum (HWHM) is half of the FWHM if the function is symmetric. The term full duration at half maximum (FDHM) is preferred when the independent variable is time. FWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers. The convention of \"width\" meaning \"half maximum\" is also widely used in signal processing to define bandwidth as \"width of frequency range where less than half the signal's power is attenuated\", i.e., the power is at least half the maximum. In signal processing terms, this is at most −3 dB of attenuation, called half-power point or, more specifically, half-power bandwidth. When half-power point is applied to antenna beam width, it is called half-power beam width.", "text": "In a distribution, full width at half maximum (FWHM) is the difference between the two values of the independent variable at which the dependent variable is equal to half of its maximum value. In other words, it is the width of a spectrum curve measured between those points on the y-axis which are half the maximum amplitude. Half width at half maximum (HWHM) is half of the FWHM if the function is symmetric. The term full duration at half maximum (FDHM) is preferred when the independent variable is time. FWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers. The convention of \"width\" meaning \"half maximum\" is also widely used in signal processing to define bandwidth as \"width of frequency range where less than half the signal's power is attenuated\", i.e., the power is at least half the maximum. In signal processing terms, this is at most −3 dB of attenuation, called half-power point or, more specifically, half-power bandwidth. When half-power point is applied to antenna beam width, it is called half-power beam width. Specific distributions Normal distribution If the considered function is the density of a normal distribution of the form f ( x ) = 1 σ 2 π exp ⁡ [ − ( x − x 0 ) 2 2 σ 2 ] {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}\\exp \\left} where σ is the standard deviation and x0 is the expected value, then the relationship between FWHM and the standard deviation is F W H M = 2 2 ln ⁡ 2 σ ≈ 2.355 σ . {\\displaystyle \\mathrm {FWHM} =2{\\sqrt {2\\ln 2}}\\;\\sigma \\approx 2.355\\;\\sigma .} The FWHM does not depend on the expected value x0; it is invariant under translations. The area within this FWHM is approximately 76% of the total area under the function. Other distributions In spectroscopy half the width at half maximum (here γ), HWHM, is in common use. For example, a Lorentzian/Cauchy distribution of height ⁠1/πγ⁠ can be defined by f ( x ) = 1 π γ [ 1 + ( x − x 0 γ ) 2 ] and F W H M = 2 γ . {\\displaystyle f(x)={\\frac {1}{\\pi \\gamma \\left}}\\quad {\\text{ and }}\\quad \\mathrm {FWHM} =2\\gamma .} Another important distribution function, related to solitons in optics, is the hyperbolic secant: f ( x ) = sech ⁡ ( x X ) . {\\displaystyle f(x)=\\operatorname {sech} \\left({\\frac {x}{X}}\\right).} Any translating element was omitted, since it does not affect the FWHM. For this impulse we have: F W H M = 2 arcsch ⁡ ( 1 2 ) X = 2 ln ⁡ ( 2 + 3 ) X ≈ 2.634 X {\\displaystyle \\mathrm {FWHM} =2\\operatorname {arcsch} \\left({\\tfrac {1}{2}}\\right)X=2\\ln(2+{\\sqrt {3}})\\;X\\approx 2.634\\;X} where arcsch is the inverse hyperbolic secant. See also Gaussian function Cutoff frequency Spatial resolution References This article incorporates public domain material from Federal Standard 1037C. General Services Administration. Archived from the original on 2022-01-22. External links FWHM at Wolfram Mathworld", "canonical_url": "https://en.wikipedia.org/wiki/Full_width_at_half_maximum", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:10.742507", "metadata": {"word_count": 191, "text_length": 2945}}
{"id": "wiki_FWHM", "query_word": "FWHM", "title": "Full width at half maximum", "summary": "In a distribution, full width at half maximum (FWHM) is the difference between the two values of the independent variable at which the dependent variable is equal to half of its maximum value. In other words, it is the width of a spectrum curve measured between those points on the y-axis which are half the maximum amplitude. Half width at half maximum (HWHM) is half of the FWHM if the function is symmetric. The term full duration at half maximum (FDHM) is preferred when the independent variable is time. FWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers. The convention of \"width\" meaning \"half maximum\" is also widely used in signal processing to define bandwidth as \"width of frequency range where less than half the signal's power is attenuated\", i.e., the power is at least half the maximum. In signal processing terms, this is at most −3 dB of attenuation, called half-power point or, more specifically, half-power bandwidth. When half-power point is applied to antenna beam width, it is called half-power beam width.", "text": "In a distribution, full width at half maximum (FWHM) is the difference between the two values of the independent variable at which the dependent variable is equal to half of its maximum value. In other words, it is the width of a spectrum curve measured between those points on the y-axis which are half the maximum amplitude. Half width at half maximum (HWHM) is half of the FWHM if the function is symmetric. The term full duration at half maximum (FDHM) is preferred when the independent variable is time. FWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers. The convention of \"width\" meaning \"half maximum\" is also widely used in signal processing to define bandwidth as \"width of frequency range where less than half the signal's power is attenuated\", i.e., the power is at least half the maximum. In signal processing terms, this is at most −3 dB of attenuation, called half-power point or, more specifically, half-power bandwidth. When half-power point is applied to antenna beam width, it is called half-power beam width. Specific distributions Normal distribution If the considered function is the density of a normal distribution of the form f ( x ) = 1 σ 2 π exp ⁡ [ − ( x − x 0 ) 2 2 σ 2 ] {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}\\exp \\left} where σ is the standard deviation and x0 is the expected value, then the relationship between FWHM and the standard deviation is F W H M = 2 2 ln ⁡ 2 σ ≈ 2.355 σ . {\\displaystyle \\mathrm {FWHM} =2{\\sqrt {2\\ln 2}}\\;\\sigma \\approx 2.355\\;\\sigma .} The FWHM does not depend on the expected value x0; it is invariant under translations. The area within this FWHM is approximately 76% of the total area under the function. Other distributions In spectroscopy half the width at half maximum (here γ), HWHM, is in common use. For example, a Lorentzian/Cauchy distribution of height ⁠1/πγ⁠ can be defined by f ( x ) = 1 π γ [ 1 + ( x − x 0 γ ) 2 ] and F W H M = 2 γ . {\\displaystyle f(x)={\\frac {1}{\\pi \\gamma \\left}}\\quad {\\text{ and }}\\quad \\mathrm {FWHM} =2\\gamma .} Another important distribution function, related to solitons in optics, is the hyperbolic secant: f ( x ) = sech ⁡ ( x X ) . {\\displaystyle f(x)=\\operatorname {sech} \\left({\\frac {x}{X}}\\right).} Any translating element was omitted, since it does not affect the FWHM. For this impulse we have: F W H M = 2 arcsch ⁡ ( 1 2 ) X = 2 ln ⁡ ( 2 + 3 ) X ≈ 2.634 X {\\displaystyle \\mathrm {FWHM} =2\\operatorname {arcsch} \\left({\\tfrac {1}{2}}\\right)X=2\\ln(2+{\\sqrt {3}})\\;X\\approx 2.634\\;X} where arcsch is the inverse hyperbolic secant. See also Gaussian function Cutoff frequency Spatial resolution References This article incorporates public domain material from Federal Standard 1037C. General Services Administration. Archived from the original on 2022-01-22. External links FWHM at Wolfram Mathworld", "canonical_url": "https://en.wikipedia.org/wiki/Full_width_at_half_maximum", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:11.404639", "metadata": {"word_count": 191, "text_length": 2945}}
{"id": "wiki_spectral_resolution", "query_word": "spectral resolution", "title": "Spectral resolution", "summary": "The spectral resolution of a spectrograph, or, more generally, of a frequency spectrum, is a measure of its ability to resolve features in the electromagnetic spectrum. It is usually denoted by Δ λ {\\displaystyle \\Delta \\lambda } , and is closely related to the resolving power of the spectrograph, defined as R = λ Δ λ , {\\displaystyle R={\\frac {\\lambda }{\\Delta \\lambda }},} where Δ λ {\\displaystyle \\Delta \\lambda } is the smallest difference in wavelengths that can be distinguished at a wavelength of λ {\\displaystyle \\lambda } . For example, the Space Telescope Imaging Spectrograph (STIS) can distinguish features 0.17 nm apart at a wavelength of 1000 nm, giving it a resolution of 0.17 nm and a resolving power of about 5,900. An example of a high resolution spectrograph is the Cryogenic High-Resolution IR Echelle Spectrograph (CRIRES+) installed at ESO's Very Large Telescope, which has a spectral resolving power of up to 100,000.", "text": "The spectral resolution of a spectrograph, or, more generally, of a frequency spectrum, is a measure of its ability to resolve features in the electromagnetic spectrum. It is usually denoted by Δ λ {\\displaystyle \\Delta \\lambda } , and is closely related to the resolving power of the spectrograph, defined as R = λ Δ λ , {\\displaystyle R={\\frac {\\lambda }{\\Delta \\lambda }},} where Δ λ {\\displaystyle \\Delta \\lambda } is the smallest difference in wavelengths that can be distinguished at a wavelength of λ {\\displaystyle \\lambda } . For example, the Space Telescope Imaging Spectrograph (STIS) can distinguish features 0.17 nm apart at a wavelength of 1000 nm, giving it a resolution of 0.17 nm and a resolving power of about 5,900. An example of a high resolution spectrograph is the Cryogenic High-Resolution IR Echelle Spectrograph (CRIRES+) installed at ESO's Very Large Telescope, which has a spectral resolving power of up to 100,000. Doppler effect The spectral resolution can also be expressed in terms of physical quantities, such as velocity; then it describes the difference between velocities Δ v {\\displaystyle \\Delta v} that can be distinguished through the Doppler effect. Then, the resolution is Δ v {\\displaystyle \\Delta v} and the resolving power is R = c Δ v , {\\displaystyle R={\\frac {c}{\\Delta v}},} where c {\\displaystyle c} is the speed of light. The STIS example above then has a spectral resolution of 51. IUPAC definition IUPAC defines resolution in optical spectroscopy as the minimum wavenumber, wavelength or frequency difference between two lines in a spectrum that can be distinguished. Resolving power, R, is given by the transition wavenumber, wavelength or frequency, divided by the resolution. See also Angular resolution Resolution (mass spectrometry) References Further reading Kim Quijano, J., et al. (2003), STIS Instrument Handbook, Version 7.0, (Baltimore: STScI) Frank L. Pedrotti, S.J. (2007), Introduction to optics, 3rd version, (San Francisco)", "canonical_url": "https://en.wikipedia.org/wiki/Spectral_resolution", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:12.043218", "metadata": {"word_count": 154, "text_length": 1991}}
{"id": "wiki_blue_band", "query_word": "blue band", "title": "Penn State Blue Band", "summary": "The Pennsylvania State University Marching Blue Band, known generally as the Penn State Blue Band or simply the Blue Band, is the marching band of Pennsylvania State University. Founded in 1899, it is the largest recognized student organization at the University Park campus of Penn State, with over 300 active student members. The primary function of the band is to support the Penn State Nittany Lions football team, performing for all home football games at Beaver Stadium.", "text": "The Pennsylvania State University Marching Blue Band, known generally as the Penn State Blue Band or simply the Blue Band, is the marching band of Pennsylvania State University. Founded in 1899, it is the largest recognized student organization at the University Park campus of Penn State, with over 300 active student members. The primary function of the band is to support the Penn State Nittany Lions football team, performing for all home football games at Beaver Stadium. Background The Blue Band is open to all students at the Penn State University Park Campus by competitive audition. The Blue Band accepts 290-315 student members annually; returnees must re-audition. All students who participate in the Blue Band are enrolled in a one-credit class and must remain in good standing with the university in order to maintain their eligibility with the band. The Blue Band consists of instrumentalists, Blue Band Silks (color guard), Touch of Blue (majorettes), a drum major, a Blue Sapphire (featured baton twirler), uniform managers, and student operations assistants. The Blue Band performs many times throughout the school year. In addition to home football games, the entire band travels to one or two away games per season, and there is also a small pep band that travels to other select away games. The band also takes part in other performances, including the \"Be a Part from the Start\" event at the beginning of the academic year to help welcome the incoming class of freshmen. Outside of the Blue Band's obligations as a pep band, the band occasionally performs in a concert known as \"Bandorama\", consisting of a reprisal of the year's halftime music and fight songs. Separate from the Blue Band is the Pride of the Lions Pep Band, an ensemble which performs at men's basketball, women's basketball, women's volleyball, and other various sporting events on campus. Audition is not required for this ensemble. History The Blue Band traces its history to 1899 with the formation of a six-member drum and bugle corps initiated by student George H. Deike. A donation from steel magnate and Penn State College Board of Trustees member Andrew Carnegie made possible the formation of a brass band in the summer of 1901. By 1913, the organization was known as the College Band, and the first permanent director of bands, Wilfred O. \"Tommy\" Thompson, was appointed in 1914. In 1923, blue uniforms were purchased towards replacement of the old brown military-style uniforms in use at the time. Blue uniforms were issued on the basis of ability and rank. The Blue Band made its film debut in the 1977 made-for-TV movie Something for Joey. Appearing again in the 1993 feature film Rudy, they played \"The Nittany Lion\" in the movie. In 2005, the Penn State Blue Band was honored with the Sudler Trophy. During succeeding eras in which Hummel (Hum) Fishburn (1939–1947), James W. Dunlop (1947–1975), Ned C. Deihl (1975–1996), O. Richard Bundy (1996–2015) served as directors, the name Penn State Blue Band was kept even though all members were uniformed in blue. Since 2015, the director of the Blue Band has been Gregory Drane. Performances The Blue Band has appeared at 33 bowl games, including multiple appearances in the Orange, Cotton, Sugar, Fiesta, Gator, Rose, and Citrus Bowls. The band has also performed at the Outback, Blockbuster, Holiday, Pinstripe, and Peach Bowls, and for the Buffalo Bills on Monday Night Football. The Blue Band marched in the Bicentennial Constitution Celebration Parade held in Philadelphia in 1987, and made its first appearance in the Tournament of Roses Parade in Pasadena on January 2, 1995, having made several additional appearances since. Organization Director Since July 2015, the band has been led by its sixth director, Dr. Gregory Drane. Dr. Drane previously served as Assistant Director of the Blue Band from 2005 until his appointment as director, and has been part of the Blue Band staff since 2002, when he started as a graduate assistant. Director emeritus From 1996 to 2015, the Blue Band was under the direction of O. Richard Bundy. Bundy received his undergraduate degree in music education from Penn State. After receiving a master's degree from the University of Michigan, he returned to Penn State for his doctoral degree (serving as assistant director under Ned C. Deihl from 1983 to 1996). On October 11, 2015, the Blue Band's practice facility was dedicated in Bundy's honor and renamed to the O. Richard Bundy Blue Band Building. Drum major The Penn State Blue Band has one drum major who is chosen in the spring prior to each band camp. The drum major leads the band through warm-ups and fundamentals, as well as instructing the band during rehearsals. Officers The Blue Band is also maintained in large part by student officers who are elected by the band at the end of the season for the next season. References External links Official website", "canonical_url": "https://en.wikipedia.org/wiki/Penn_State_Blue_Band", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:14.687187", "metadata": {"word_count": 77, "text_length": 4910}}
{"id": "wiki_green_band", "query_word": "green band", "title": "Motion Picture Association film rating system", "summary": "The Motion Picture Association film rating system is used in the United States and its territories to rate a motion picture's suitability for certain audiences based on its content. The system and the ratings applied to individual motion pictures are the responsibility of the Motion Picture Association (MPA), previously known as the Motion Picture Association of America (MPAA) from 1945 to 2019. The MPA rating system is a voluntary scheme that is not enforced by law; films can be exhibited without a rating, although most theaters refuse to exhibit non-rated or NC-17 rated films. Non-members of the MPA may also submit films for rating. Other media, such as television programs, music and video games, are rated by other entities such as the TV Parental Guidelines, the RIAA and the ESRB, respectively. In effect as of November 1968, following the Hays Code of the classical Hollywood cinema era, the MPA rating system is one of various motion picture rating systems that are used to help parents decide what films are appropriate for their children. It is administered by the Classification & Ratings Administration (CARA), an independent division of the MPA.", "text": "The Motion Picture Association film rating system is used in the United States and its territories to rate a motion picture's suitability for certain audiences based on its content. The system and the ratings applied to individual motion pictures are the responsibility of the Motion Picture Association (MPA), previously known as the Motion Picture Association of America (MPAA) from 1945 to 2019. The MPA rating system is a voluntary scheme that is not enforced by law; films can be exhibited without a rating, although most theaters refuse to exhibit non-rated or NC-17 rated films. Non-members of the MPA may also submit films for rating. Other media, such as television programs, music and video games, are rated by other entities such as the TV Parental Guidelines, the RIAA and the ESRB, respectively. In effect as of November 1968, following the Hays Code of the classical Hollywood cinema era, the MPA rating system is one of various motion picture rating systems that are used to help parents decide what films are appropriate for their children. It is administered by the Classification & Ratings Administration (CARA), an independent division of the MPA. Ratings MPA film ratings The MPA film ratings are as follows: In 2013, the MPA ratings were visually redesigned, with the rating displayed on a left panel and the name of the rating shown above it. A larger panel on the right provides a more detailed description of the film's content and an explanation of the rating level is placed on a horizontal bar at the bottom of the rating. Content descriptors Film ratings often have accompanying brief descriptions of the specifics behind the film's content and why it received a certain rating. They are displayed in trailers, posters, and on the backside of home video releases. Film rating content descriptors are used for films rated from PG to NC-17; they are not used for G-rated films, because the content in them is suitable for all audiences, even if it contains mildly-objectionable content. Other labels If a film has not been submitted for a rating or is an uncut version of a film that was submitted, the labels Not Rated (NR) or Unrated (UR) are often used. Uncut/extended versions of films that are labeled \"Unrated\" also contain warnings saying that the uncut version of the film contains content that differs from the theatrical release and might not be suitable for minors. If a film has not yet been assigned a final rating, the label This Film Is Not Yet Rated is used in trailers and television commercials. Regulation of promotional materials and releases The MPA also rates film trailers, print advertising, posters, and other media used to promote a film. Theatrical trailers Rating cards appear at the head of trailers in the United States which indicate how closely the trailer adheres to the MPA's (and prior to November 2019, the MPAA's) standards. Green band: When the trailer accompanies another rated feature, the wording on the green title card states, as of May 2013, \"The following preview has been approved to accompany this feature.\" For trailers hosted on the Internet, the wording is tweaked to \"The following preview has been approved for appropriate audiences.\" Until April 2009, these cards indicated that they had been approved for \"all audiences\" and often included the film's MPAA rating. This signified that the trailer adhered to the standards for motion picture advertising outlined by the MPAA, which included limitations on foul language and violent, sexual, or otherwise objectionable imagery. In April 2009, the MPAA began to permit the green band language to say that a trailer had been approved for \"appropriate\" audiences, meaning that the material would be appropriate for audiences in theaters, based on the content of the film they had come to see. In May 2013, the MPAA changed the trailer approval band from \"for appropriate audiences\" to \"to accompany this feature\", but only when accompanying a feature film; for bands not accompanying a feature film, the text of the band remained the same. The font and style of the text on the graphic bands (green and red) was also changed at the time the green band was revised in 2013. Yellow band: A yellow title card was introduced in 2007 for trailers with restricted content hosted on the Internet, with the message \"The following preview has been approved only for age-appropriate Internet users.\" The MPAA stipulated that yellow-band trailers hosted on studio websites should only be available between 9:00 p.m. and 4:00 a.m. (i.e., 21:00 through 04:00 local time), and that for other websites hosting the trailers, at least 80% of its typical user base should be adults. The yellow card was reserved for trailers previewing films rated PG-13 or stronger. An early example was a yellow-band trailer for Rob Zombie's Halloween (2007). Yellow-band trailers were not widely adopted and were apparently abandoned within a few years: in 2013, Variety reported that age-restricted trailers online were released with red bands. The 2019 edition of CARA's advertising guidelines reference only green and red bands for internet trailers. Red band: A red title card is issued to trailers which do not adhere to the MPA/CARA guidelines. It indicates that the trailer is approved for only \"restricted\" or \"mature\" audiences, and when it accompanies another feature, the wording states \"The following restricted preview has been approved to accompany this feature only.\" For trailers hosted on the Internet, the wording is tweaked to \"The following restricted preview has been approved for appropriate audiences.\" The red title card is reserved for trailers previewing R and NC-17 rated films: these trailers may include nudity, profanity, or other material deemed inappropriate for children. Such trailers are officially meant to be locked behind age verification systems. However, these \"age gates\" have been described as \"ineffective\" and an \"honor system\"; furthermore, many YouTube channels which exist to syndicate film and television trailers do not feature any check, which has led to criticism from watchdog groups like Common Sense Media. In 2007, red-band trailers were said to be virtually absent from theaters, due to worries that they would accidentally be shown before films released at a less-restrictive rating. However, by the following year, they were noted as increasingly prevalent as the adoption of digital projection had largely alleviated these concerns. These trailers may only be shown theatrically before R-rated, NC-17-rated, or unrated movies. Releases The MPA also creates blue feature tags for theatrical and home media use. Theatrical releases show the blue tag after the film, with home media releases showing it prior to the film. They feature the rating block and any content descriptors as assigned by the Classification and Rating Administration, the MPA logo, and links to MPA websites along the bottom. History Replacement of the Hays Code Jack Valenti, who had become president of the Motion Picture Association of America in May 1966, deemed the Motion Picture Production Code, which had been in place since 1930 and rigorously enforced since July 1, 1934, out of date and bearing \"the odious smell of censorship\". Filmmakers were pushing at the boundaries of the code with some even going as far as filing lawsuits against the \"Hays Code\" by invoking the First Amendment. Valenti cited examples such as Who's Afraid of Virginia Woolf?, which used prohibited language including \"hump the hostess\", and Blowup, which was denied Code approval due to nudity, resulting in Metro-Goldwyn-Mayer, then a member studio of the MPAA, releasing it through a subsidiary. Valenti revised the Code to include the \"SMA\" (Suggested for Mature Audiences) advisory as a stopgap measure. To accommodate \"the irresistible force of creators determined to make 'their films'\", and to avoid \"the possible intrusion of government into the movie arena\", he developed a set of advisory ratings which could be applied after a film was completed. On November 1, 1968, the voluntary MPAA film rating system took effect, with three organizations serving as its monitoring and guiding groups: the MPAA, the National Association of Theatre Owners (NATO), and the International Film Importers & Distributors of America (IFIDA). Only films that premiered in the United States after that date were affected by this. Walter Reade was the only one of 75 top U.S. exhibitors who refused to use the ratings. Warner Bros.-Seven Arts' The Girl on a Motorcycle was the first film to receive the X rating, and was distributed by their Claridge Pictures subsidiary. Two other films were rated X by the time the MPAA published their first weekly bulletin listing ratings: Paramount's Sin With a Stranger and Universal's Birds in Peru. Both films were subsequently released by subsidiaries. The ratings used from 1968 to 1970 were: Rated G: Suggested for general audiences. Rated M: Suggested for mature audiences – Parental discretion advised. Rated R: Restricted – Persons under 16 not admitted, unless accompanied by parent or adult guardian. Rated X: Persons under 16 not admitted. This content classification system originally was to have three ratings, with the intention of allowing parents to take their children to any film they chose. However, the National Association of Theatre Owners urged the creation of an adults-only category, fearful of possible legal problems in local jurisdictions. The \"X\" rating was not an MPAA trademark and would not receive the MPAA seal; any producer not submitting a film for MPAA rating could self-apply the \"X\" rating (or any other symbol or description that was not an MPAA trademark). From M to GP to PG In 1970, the ages for \"R\" and \"X\" were raised from 16 to 17. Also, due to confusion over whether \"M\"-rated films were suitable for children, \"M\" was renamed to \"GP\" (for General audiences, Parental guidance suggested), and in 1971, the MPAA added t", "canonical_url": "https://en.wikipedia.org/wiki/Motion_Picture_Association_film_rating_system", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:15.590302", "metadata": {"word_count": 189, "text_length": 36917}}
{"id": "wiki_red_band", "query_word": "red band", "title": "Motion Picture Association film rating system", "summary": "The Motion Picture Association film rating system is used in the United States and its territories to rate a motion picture's suitability for certain audiences based on its content. The system and the ratings applied to individual motion pictures are the responsibility of the Motion Picture Association (MPA), previously known as the Motion Picture Association of America (MPAA) from 1945 to 2019. The MPA rating system is a voluntary scheme that is not enforced by law; films can be exhibited without a rating, although most theaters refuse to exhibit non-rated or NC-17 rated films. Non-members of the MPA may also submit films for rating. Other media, such as television programs, music and video games, are rated by other entities such as the TV Parental Guidelines, the RIAA and the ESRB, respectively. In effect as of November 1968, following the Hays Code of the classical Hollywood cinema era, the MPA rating system is one of various motion picture rating systems that are used to help parents decide what films are appropriate for their children. It is administered by the Classification & Ratings Administration (CARA), an independent division of the MPA.", "text": "The Motion Picture Association film rating system is used in the United States and its territories to rate a motion picture's suitability for certain audiences based on its content. The system and the ratings applied to individual motion pictures are the responsibility of the Motion Picture Association (MPA), previously known as the Motion Picture Association of America (MPAA) from 1945 to 2019. The MPA rating system is a voluntary scheme that is not enforced by law; films can be exhibited without a rating, although most theaters refuse to exhibit non-rated or NC-17 rated films. Non-members of the MPA may also submit films for rating. Other media, such as television programs, music and video games, are rated by other entities such as the TV Parental Guidelines, the RIAA and the ESRB, respectively. In effect as of November 1968, following the Hays Code of the classical Hollywood cinema era, the MPA rating system is one of various motion picture rating systems that are used to help parents decide what films are appropriate for their children. It is administered by the Classification & Ratings Administration (CARA), an independent division of the MPA. Ratings MPA film ratings The MPA film ratings are as follows: In 2013, the MPA ratings were visually redesigned, with the rating displayed on a left panel and the name of the rating shown above it. A larger panel on the right provides a more detailed description of the film's content and an explanation of the rating level is placed on a horizontal bar at the bottom of the rating. Content descriptors Film ratings often have accompanying brief descriptions of the specifics behind the film's content and why it received a certain rating. They are displayed in trailers, posters, and on the backside of home video releases. Film rating content descriptors are used for films rated from PG to NC-17; they are not used for G-rated films, because the content in them is suitable for all audiences, even if it contains mildly-objectionable content. Other labels If a film has not been submitted for a rating or is an uncut version of a film that was submitted, the labels Not Rated (NR) or Unrated (UR) are often used. Uncut/extended versions of films that are labeled \"Unrated\" also contain warnings saying that the uncut version of the film contains content that differs from the theatrical release and might not be suitable for minors. If a film has not yet been assigned a final rating, the label This Film Is Not Yet Rated is used in trailers and television commercials. Regulation of promotional materials and releases The MPA also rates film trailers, print advertising, posters, and other media used to promote a film. Theatrical trailers Rating cards appear at the head of trailers in the United States which indicate how closely the trailer adheres to the MPA's (and prior to November 2019, the MPAA's) standards. Green band: When the trailer accompanies another rated feature, the wording on the green title card states, as of May 2013, \"The following preview has been approved to accompany this feature.\" For trailers hosted on the Internet, the wording is tweaked to \"The following preview has been approved for appropriate audiences.\" Until April 2009, these cards indicated that they had been approved for \"all audiences\" and often included the film's MPAA rating. This signified that the trailer adhered to the standards for motion picture advertising outlined by the MPAA, which included limitations on foul language and violent, sexual, or otherwise objectionable imagery. In April 2009, the MPAA began to permit the green band language to say that a trailer had been approved for \"appropriate\" audiences, meaning that the material would be appropriate for audiences in theaters, based on the content of the film they had come to see. In May 2013, the MPAA changed the trailer approval band from \"for appropriate audiences\" to \"to accompany this feature\", but only when accompanying a feature film; for bands not accompanying a feature film, the text of the band remained the same. The font and style of the text on the graphic bands (green and red) was also changed at the time the green band was revised in 2013. Yellow band: A yellow title card was introduced in 2007 for trailers with restricted content hosted on the Internet, with the message \"The following preview has been approved only for age-appropriate Internet users.\" The MPAA stipulated that yellow-band trailers hosted on studio websites should only be available between 9:00 p.m. and 4:00 a.m. (i.e., 21:00 through 04:00 local time), and that for other websites hosting the trailers, at least 80% of its typical user base should be adults. The yellow card was reserved for trailers previewing films rated PG-13 or stronger. An early example was a yellow-band trailer for Rob Zombie's Halloween (2007). Yellow-band trailers were not widely adopted and were apparently abandoned within a few years: in 2013, Variety reported that age-restricted trailers online were released with red bands. The 2019 edition of CARA's advertising guidelines reference only green and red bands for internet trailers. Red band: A red title card is issued to trailers which do not adhere to the MPA/CARA guidelines. It indicates that the trailer is approved for only \"restricted\" or \"mature\" audiences, and when it accompanies another feature, the wording states \"The following restricted preview has been approved to accompany this feature only.\" For trailers hosted on the Internet, the wording is tweaked to \"The following restricted preview has been approved for appropriate audiences.\" The red title card is reserved for trailers previewing R and NC-17 rated films: these trailers may include nudity, profanity, or other material deemed inappropriate for children. Such trailers are officially meant to be locked behind age verification systems. However, these \"age gates\" have been described as \"ineffective\" and an \"honor system\"; furthermore, many YouTube channels which exist to syndicate film and television trailers do not feature any check, which has led to criticism from watchdog groups like Common Sense Media. In 2007, red-band trailers were said to be virtually absent from theaters, due to worries that they would accidentally be shown before films released at a less-restrictive rating. However, by the following year, they were noted as increasingly prevalent as the adoption of digital projection had largely alleviated these concerns. These trailers may only be shown theatrically before R-rated, NC-17-rated, or unrated movies. Releases The MPA also creates blue feature tags for theatrical and home media use. Theatrical releases show the blue tag after the film, with home media releases showing it prior to the film. They feature the rating block and any content descriptors as assigned by the Classification and Rating Administration, the MPA logo, and links to MPA websites along the bottom. History Replacement of the Hays Code Jack Valenti, who had become president of the Motion Picture Association of America in May 1966, deemed the Motion Picture Production Code, which had been in place since 1930 and rigorously enforced since July 1, 1934, out of date and bearing \"the odious smell of censorship\". Filmmakers were pushing at the boundaries of the code with some even going as far as filing lawsuits against the \"Hays Code\" by invoking the First Amendment. Valenti cited examples such as Who's Afraid of Virginia Woolf?, which used prohibited language including \"hump the hostess\", and Blowup, which was denied Code approval due to nudity, resulting in Metro-Goldwyn-Mayer, then a member studio of the MPAA, releasing it through a subsidiary. Valenti revised the Code to include the \"SMA\" (Suggested for Mature Audiences) advisory as a stopgap measure. To accommodate \"the irresistible force of creators determined to make 'their films'\", and to avoid \"the possible intrusion of government into the movie arena\", he developed a set of advisory ratings which could be applied after a film was completed. On November 1, 1968, the voluntary MPAA film rating system took effect, with three organizations serving as its monitoring and guiding groups: the MPAA, the National Association of Theatre Owners (NATO), and the International Film Importers & Distributors of America (IFIDA). Only films that premiered in the United States after that date were affected by this. Walter Reade was the only one of 75 top U.S. exhibitors who refused to use the ratings. Warner Bros.-Seven Arts' The Girl on a Motorcycle was the first film to receive the X rating, and was distributed by their Claridge Pictures subsidiary. Two other films were rated X by the time the MPAA published their first weekly bulletin listing ratings: Paramount's Sin With a Stranger and Universal's Birds in Peru. Both films were subsequently released by subsidiaries. The ratings used from 1968 to 1970 were: Rated G: Suggested for general audiences. Rated M: Suggested for mature audiences – Parental discretion advised. Rated R: Restricted – Persons under 16 not admitted, unless accompanied by parent or adult guardian. Rated X: Persons under 16 not admitted. This content classification system originally was to have three ratings, with the intention of allowing parents to take their children to any film they chose. However, the National Association of Theatre Owners urged the creation of an adults-only category, fearful of possible legal problems in local jurisdictions. The \"X\" rating was not an MPAA trademark and would not receive the MPAA seal; any producer not submitting a film for MPAA rating could self-apply the \"X\" rating (or any other symbol or description that was not an MPAA trademark). From M to GP to PG In 1970, the ages for \"R\" and \"X\" were raised from 16 to 17. Also, due to confusion over whether \"M\"-rated films were suitable for children, \"M\" was renamed to \"GP\" (for General audiences, Parental guidance suggested), and in 1971, the MPAA added t", "canonical_url": "https://en.wikipedia.org/wiki/Motion_Picture_Association_film_rating_system", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:16.481425", "metadata": {"word_count": 189, "text_length": 36917}}
{"id": "wiki_red_edge", "query_word": "red edge", "title": "Red edge", "summary": "Red edge refers to the region of rapid change in reflectance of vegetation in the near infrared range of the electromagnetic spectrum. Chlorophyll contained in vegetation absorbs most of the light in the visible part of the spectrum but becomes almost transparent at wavelengths greater than 700 nm. The cellular structure of the vegetation then causes this infrared light to be reflected because each cell acts something like an elementary corner reflector. The change can be from 5% to 50% reflectance going from 680 nm to 730 nm. This is an advantage to plants in avoiding overheating during photosynthesis. For a more detailed explanation and a graph of the photosynthetically active radiation (PAR) spectral region, see Normalized difference vegetation index § Rationale. The phenomenon accounts for the brightness of foliage in infrared photography and is extensively utilized in the form of so-called vegetation indices (e.g. Normalized difference vegetation index). It is used in remote sensing to monitor plant activity, and it has been suggested that it could be useful to detect light-harvesting organisms on distant planets.", "text": "Red edge refers to the region of rapid change in reflectance of vegetation in the near infrared range of the electromagnetic spectrum. Chlorophyll contained in vegetation absorbs most of the light in the visible part of the spectrum but becomes almost transparent at wavelengths greater than 700 nm. The cellular structure of the vegetation then causes this infrared light to be reflected because each cell acts something like an elementary corner reflector. The change can be from 5% to 50% reflectance going from 680 nm to 730 nm. This is an advantage to plants in avoiding overheating during photosynthesis. For a more detailed explanation and a graph of the photosynthetically active radiation (PAR) spectral region, see Normalized difference vegetation index § Rationale. The phenomenon accounts for the brightness of foliage in infrared photography and is extensively utilized in the form of so-called vegetation indices (e.g. Normalized difference vegetation index). It is used in remote sensing to monitor plant activity, and it has been suggested that it could be useful to detect light-harvesting organisms on distant planets. See also Enhanced vegetation index – Index for improving vegetation monitoring Purple Earth hypothesis – Astrobiological hypothesis regarding early photosynthetic organisms Vegetation Index == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Red_edge", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:17.124485", "metadata": {"word_count": 176, "text_length": 1343}}
{"id": "wiki_panchromatic", "query_word": "panchromatic", "title": "Panchromatic film", "summary": "A panchromatic emulsion is a type of photographic emulsion that is sensitive to all wavelengths of visible light, and produces a monochrome photograph—typically black and white. Most modern commercially available film is panchromatic, and the technology is usually contrasted with earlier methods that cannot register all wavelengths, especially orthochromatic film. In digital imaging, a panchromatic sensor is an image sensor or array of sensors that combine the visible spectrum with non-visible wavelengths, such as ultraviolet or infrared. Images produced are also black and white, and the system is used for its ability to produce higher resolution images than standard digital sensors.", "text": "A panchromatic emulsion is a type of photographic emulsion that is sensitive to all wavelengths of visible light, and produces a monochrome photograph—typically black and white. Most modern commercially available film is panchromatic, and the technology is usually contrasted with earlier methods that cannot register all wavelengths, especially orthochromatic film. In digital imaging, a panchromatic sensor is an image sensor or array of sensors that combine the visible spectrum with non-visible wavelengths, such as ultraviolet or infrared. Images produced are also black and white, and the system is used for its ability to produce higher resolution images than standard digital sensors. Description A panchromatic emulsion renders a realistic reproduction of a scene as it appears to the human eye, although with no colors. Almost all modern photographic film is panchromatic. Some older types of film were orthochromatic and were not sensitive to certain wavelengths of light. As naturally prepared, a silver halide photographic emulsion is much more sensitive to blue and UV light than to green and red wavelengths. The German chemist Hermann W. Vogel found out how to extend the sensitivity into the green, and later the orange, by adding sensitising dyes to the emulsion. By the addition of erythrosine the emulsion could be made orthochromatic while some cyanine derivatives confer sensitivity to the whole visible spectrum making it panchromatic. However, his technique was not extended to achieve a fully panchromatic film until the early 1900s, shortly after his death. Kenneth Mees credits Wratten & Wainwright with preparing the first commercial plates with panchromatic emulsions. Panchromatic stock for still photographic plates became available commercially in 1906. The switch from orthochromatic film, however, was only gradual. Panchromatic plates cost two to three times as much, and had to be developed in total darkness, unlike orthochromatic—which, being insensitive to red, could be developed under a red light in the darkroom. And the process that increased the film's sensitivity to yellow and red also made it oversensitive to blue and violet, requiring a yellow-red lens filter to correct it, which in turn reduced the total amount of light and increased the necessary exposure time. Orthochromatic film proved troublesome for motion pictures, rendering blue skies as perpetually overcast, blond hair as washed-out, blue eyes nearly white, and red lips nearly black. To some degree this could be corrected by makeup, lens filters, and lighting, but never completely satisfactorily. But even those solutions were unusable for additive color motion picture systems like Kinemacolor and Prizma color, which photographed on black-and-white stock behind alternating color filters. In those cases, negative film stock after it arrived from the manufacturer had to be passed through a color-sensitizing solution, a time-consuming process that increased the film's cost from 3 cents per foot to 7 cents. Eastman Kodak, the supplier of motion picture film, introduced a panchromatic film stock in September 1913, available on special order for photographing color motion pictures in additive systems. Photographers began using it for black-and-white films too in 1918, primarily for outdoor scenes. The company introduced Kodak Panchromatic Cine Film as a regular stock in 1922. The first black-and-white feature film photographed entirely on panchromatic stock was The Headless Horseman (1922). But early panchromatic stock was more expensive, had a relatively short shelf-life, and was more difficult for laboratories to process because it required working in total darkness. Not until the prices were equalized by competition in 1926 did it become used more widely than orthochromatic stock. Kodak discontinued manufacturing general-purpose orthochromatic motion picture film in 1930. Digital panchromatic imagery of the Earth's surface is also produced by some modern satellites, such as QuickBird, Cartosat and IKONOS. This imagery is extremely useful, as it is generally of a much higher (spatial) resolution than the multispectral imagery from the same satellite. For example, the QuickBird satellite produces panchromatic imagery having a pixel equivalent to an area 0.6×0.6 m (2×2 ft), while the multispectral pixels represent an area of 2.4×2.4 m (8×8 ft). See also Black and white Orthochromatic Monochromatic color Pansharpened image == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Panchromatic_film", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:18.106126", "metadata": {"word_count": 101, "text_length": 4484}}
{"id": "wiki_continuous_spectrum", "query_word": "continuous spectrum", "title": "Spectrum (physical sciences)", "summary": "In the physical sciences, spectrum describes any continuous range of either frequency or wavelength values. The term initially referred to the range of observed colors as white light is dispersed through a prism — introduced to optics by Isaac Newton in the 17th century. The concept was later expanded to other waves, such as sound waves and sea waves that also present a variety of frequencies and wavelengths (e.g., noise spectrum, sea wave spectrum). Starting from Fourier analysis, the concept of spectrum expanded to signal theory, where the signal can be graphed as a function of frequency and information can be placed in selected ranges of frequency. Presently, any quantity directly dependent on, and measurable along the range of, a continuous independent variable can be graphed along its range or spectrum. Examples are the range of electron energy in electron spectroscopy or the range of mass-to-charge ratio in mass spectrometry.", "text": "In the physical sciences, spectrum describes any continuous range of either frequency or wavelength values. The term initially referred to the range of observed colors as white light is dispersed through a prism — introduced to optics by Isaac Newton in the 17th century. The concept was later expanded to other waves, such as sound waves and sea waves that also present a variety of frequencies and wavelengths (e.g., noise spectrum, sea wave spectrum). Starting from Fourier analysis, the concept of spectrum expanded to signal theory, where the signal can be graphed as a function of frequency and information can be placed in selected ranges of frequency. Presently, any quantity directly dependent on, and measurable along the range of, a continuous independent variable can be graphed along its range or spectrum. Examples are the range of electron energy in electron spectroscopy or the range of mass-to-charge ratio in mass spectrometry. Etymology Electromagnetic spectrum Electromagnetic spectrum refers to the full range of all frequencies of electromagnetic radiation and also to the characteristic distribution of electromagnetic radiation emitted or absorbed by that particular object. Devices used to measure an electromagnetic spectrum are called spectrograph or spectrometer. The visible spectrum is the part of the electromagnetic spectrum that can be seen by the human eye. The wavelength of visible light ranges from 390 to 700 nm. The absorption spectrum of a chemical element or chemical compound is the spectrum of frequencies or wavelengths of incident radiation that are absorbed by the compound due to electron transitions from a lower to a higher energy state. The emission spectrum refers to the spectrum of radiation emitted by the compound due to electron transitions from a higher to a lower energy state. Light from many different sources contains various colors, each with its own brightness or intensity. A rainbow, or prism, sends these component colors in different directions, making them individually visible at different angles. A graph of the intensity plotted against the frequency (showing the brightness of each color) is the frequency spectrum of the light. When all the visible frequencies are present equally, the perceived color of the light is white, and the spectrum is a flat line. Therefore, flat-line spectra in general are often referred to as white, whether they represent light or another type of wave phenomenon (sound, for example, or vibration in a structure). In radio and telecommunications, the frequency spectrum can be shared among many different broadcasters. The radio spectrum is the part of the electromagnetic spectrum corresponding to frequencies lower below 300 GHz, which corresponds to wavelengths longer than about 1 mm. The microwave spectrum corresponds to frequencies between 300 MHz (0.3 GHz) and 300 GHz and wavelengths between one meter and one millimeter. Each broadcast radio and TV station transmits a wave on an assigned frequency range, called a channel. When many broadcasters are present, the radio spectrum consists of the sum of all the individual channels, each carrying separate information, spread across a wide frequency spectrum. Any particular radio receiver will detect a single function of amplitude (voltage) vs. time. The radio then uses a tuned circuit or tuner to select a single channel or frequency band and demodulate or decode the information from that broadcaster. If we made a graph of the strength of each channel vs. the frequency of the tuner, it would be the frequency spectrum of the antenna signal. In astronomical spectroscopy, the strength, shape, and position of absorption and emission lines, as well as the overall spectral energy distribution of the continuum, reveal many properties of astronomical objects. Stellar classification is the categorisation of stars based on their characteristic electromagnetic spectra. The spectral flux density is used to represent the spectrum of a light-source, such as a star. In radiometry and colorimetry (or color science more generally), the spectral power distribution (SPD) of a light source is a measure of the power contributed by each frequency or color in a light source. The light spectrum is usually measured at points (often 31) along the visible spectrum, in wavelength space instead of frequency space, which makes it not strictly a spectral density. Some spectrophotometers can measure increments as fine as one to two nanometers and even higher resolution devices with resolutions less than 0.5 nm have been reported. the values are used to calculate other specifications and then plotted to show the spectral attributes of the source. This can be helpful in analyzing the color characteristics of a particular source. Mass spectrum A plot of ion abundance as a function of mass-to-charge ratio is called a mass spectrum. It can be produced by a mass spectrometer instrument. The mass spectrum can be used to determine the quantity and mass of atoms and molecules. Tandem mass spectrometry is used to determine molecular structure. Energy spectrum In physics, the energy spectrum of a particle is the number of particles or intensity of a particle beam as a function of particle energy. Examples of techniques that produce an energy spectrum are alpha-particle spectroscopy, electron energy loss spectroscopy, and mass-analyzed ion-kinetic-energy spectrometry. Displacement Oscillatory displacements, including vibrations, can also be characterized spectrally. For water waves, see wave spectrum and tide spectrum. Sound and non-audible acoustic waves can also be characterized in terms of its spectral density, for example, timbre and musical acoustics. Acoustical measurement In acoustics, a spectrogram is a visual representation of the frequency spectrum of sound as a function of time or another variable. A source of sound can have many different frequencies mixed. A musical tone's timbre is characterized by its harmonic spectrum. Sound in our environment that we refer to as noise includes many different frequencies. When a sound signal contains a mixture of all audible frequencies, distributed equally over the audio spectrum, it is called white noise. The spectrum analyzer is an instrument which can be used to convert the sound wave of the musical note into a visual display of the constituent frequencies. This visual display is referred to as an acoustic spectrogram. Software based audio spectrum analyzers are available at low cost, providing easy access not only to industry professionals, but also to academics, students and the hobbyist. The acoustic spectrogram generated by the spectrum analyzer provides an acoustic signature of the musical note. In addition to revealing the fundamental frequency and its overtones, the spectrogram is also useful for analysis of the temporal attack, decay, sustain, and release of the musical note. Continuous versus discrete spectra In the physical sciences, the spectrum of a physical quantity (such as energy) may be called continuous if it is non-zero over the whole spectrum domain (such as frequency or wavelength) or discrete if it attains non-zero values only in a discrete set over the independent variable, with band gaps between pairs of spectral bands or spectral lines. The classical example of a continuous spectrum, from which the name is derived, is the part of the spectrum of the light emitted by excited atoms of hydrogen that is due to free electrons becoming bound to a hydrogen ion and emitting photons, which are smoothly spread over a wide range of wavelengths, in contrast to the discrete lines due to electrons falling from some bound quantum state to a state of lower energy. As in that classical example, the term is most often used when the range of values of a physical quantity may have both a continuous and a discrete part, whether at the same time or in different situations. In quantum systems, continuous spectra (as in bremsstrahlung and thermal radiation) are usually associated with free particles, such as atoms in a gas, electrons in an electron beam, or conduction band electrons in a metal. In particular, the position and momentum of a free particle has a continuous spectrum, but when the particle is confined to a limited space its spectrum becomes discrete. Often a continuous spectrum may be just a convenient model for a discrete spectrum whose values are too close to be distinguished, as in the phonons in a crystal. The continuous and discrete spectra of physical systems can be modeled in functional analysis as different parts in the decomposition of the spectrum of a linear operator acting on a function space, such as the Hamiltonian operator. The classical example of a discrete spectrum (for which the term was first used) is the characteristic set of discrete spectral lines seen in the emission spectrum and absorption spectrum of isolated atoms of a chemical element, which only absorb and emit light at particular wavelengths. The technique of spectroscopy is based on this phenomenon. Discrete spectra are seen in many other phenomena, such as vibrating strings, microwaves in a metal cavity, sound waves in a pulsating star, and resonances in high-energy particle physics. The general phenomenon of discrete spectra in physical systems can be mathematically modeled with tools of functional analysis, specifically by the decomposition of the spectrum of a linear operator acting on a functional space. In classical mechanics In classical mechanics, discrete spectra are often associated to waves and oscillations in a bounded object or domain. Mathematically they can be identified with the eigenvalues of differential operators that describe the evolution of some continuous variable (such as strain or pressure) as a function of time and/or space. Discrete spectra are also produced by some non-linear oscillators where the relevant quantity has a non-sinusoidal waveform. Notable examples are the sound", "canonical_url": "https://en.wikipedia.org/wiki/Spectrum_(physical_sciences)", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:19.714710", "metadata": {"word_count": 150, "text_length": 11679}}
{"id": "wiki_discrete_spectrum", "query_word": "discrete spectrum", "title": "Spectrum (physical sciences)", "summary": "In the physical sciences, spectrum describes any continuous range of either frequency or wavelength values. The term initially referred to the range of observed colors as white light is dispersed through a prism — introduced to optics by Isaac Newton in the 17th century. The concept was later expanded to other waves, such as sound waves and sea waves that also present a variety of frequencies and wavelengths (e.g., noise spectrum, sea wave spectrum). Starting from Fourier analysis, the concept of spectrum expanded to signal theory, where the signal can be graphed as a function of frequency and information can be placed in selected ranges of frequency. Presently, any quantity directly dependent on, and measurable along the range of, a continuous independent variable can be graphed along its range or spectrum. Examples are the range of electron energy in electron spectroscopy or the range of mass-to-charge ratio in mass spectrometry.", "text": "In the physical sciences, spectrum describes any continuous range of either frequency or wavelength values. The term initially referred to the range of observed colors as white light is dispersed through a prism — introduced to optics by Isaac Newton in the 17th century. The concept was later expanded to other waves, such as sound waves and sea waves that also present a variety of frequencies and wavelengths (e.g., noise spectrum, sea wave spectrum). Starting from Fourier analysis, the concept of spectrum expanded to signal theory, where the signal can be graphed as a function of frequency and information can be placed in selected ranges of frequency. Presently, any quantity directly dependent on, and measurable along the range of, a continuous independent variable can be graphed along its range or spectrum. Examples are the range of electron energy in electron spectroscopy or the range of mass-to-charge ratio in mass spectrometry. Etymology Electromagnetic spectrum Electromagnetic spectrum refers to the full range of all frequencies of electromagnetic radiation and also to the characteristic distribution of electromagnetic radiation emitted or absorbed by that particular object. Devices used to measure an electromagnetic spectrum are called spectrograph or spectrometer. The visible spectrum is the part of the electromagnetic spectrum that can be seen by the human eye. The wavelength of visible light ranges from 390 to 700 nm. The absorption spectrum of a chemical element or chemical compound is the spectrum of frequencies or wavelengths of incident radiation that are absorbed by the compound due to electron transitions from a lower to a higher energy state. The emission spectrum refers to the spectrum of radiation emitted by the compound due to electron transitions from a higher to a lower energy state. Light from many different sources contains various colors, each with its own brightness or intensity. A rainbow, or prism, sends these component colors in different directions, making them individually visible at different angles. A graph of the intensity plotted against the frequency (showing the brightness of each color) is the frequency spectrum of the light. When all the visible frequencies are present equally, the perceived color of the light is white, and the spectrum is a flat line. Therefore, flat-line spectra in general are often referred to as white, whether they represent light or another type of wave phenomenon (sound, for example, or vibration in a structure). In radio and telecommunications, the frequency spectrum can be shared among many different broadcasters. The radio spectrum is the part of the electromagnetic spectrum corresponding to frequencies lower below 300 GHz, which corresponds to wavelengths longer than about 1 mm. The microwave spectrum corresponds to frequencies between 300 MHz (0.3 GHz) and 300 GHz and wavelengths between one meter and one millimeter. Each broadcast radio and TV station transmits a wave on an assigned frequency range, called a channel. When many broadcasters are present, the radio spectrum consists of the sum of all the individual channels, each carrying separate information, spread across a wide frequency spectrum. Any particular radio receiver will detect a single function of amplitude (voltage) vs. time. The radio then uses a tuned circuit or tuner to select a single channel or frequency band and demodulate or decode the information from that broadcaster. If we made a graph of the strength of each channel vs. the frequency of the tuner, it would be the frequency spectrum of the antenna signal. In astronomical spectroscopy, the strength, shape, and position of absorption and emission lines, as well as the overall spectral energy distribution of the continuum, reveal many properties of astronomical objects. Stellar classification is the categorisation of stars based on their characteristic electromagnetic spectra. The spectral flux density is used to represent the spectrum of a light-source, such as a star. In radiometry and colorimetry (or color science more generally), the spectral power distribution (SPD) of a light source is a measure of the power contributed by each frequency or color in a light source. The light spectrum is usually measured at points (often 31) along the visible spectrum, in wavelength space instead of frequency space, which makes it not strictly a spectral density. Some spectrophotometers can measure increments as fine as one to two nanometers and even higher resolution devices with resolutions less than 0.5 nm have been reported. the values are used to calculate other specifications and then plotted to show the spectral attributes of the source. This can be helpful in analyzing the color characteristics of a particular source. Mass spectrum A plot of ion abundance as a function of mass-to-charge ratio is called a mass spectrum. It can be produced by a mass spectrometer instrument. The mass spectrum can be used to determine the quantity and mass of atoms and molecules. Tandem mass spectrometry is used to determine molecular structure. Energy spectrum In physics, the energy spectrum of a particle is the number of particles or intensity of a particle beam as a function of particle energy. Examples of techniques that produce an energy spectrum are alpha-particle spectroscopy, electron energy loss spectroscopy, and mass-analyzed ion-kinetic-energy spectrometry. Displacement Oscillatory displacements, including vibrations, can also be characterized spectrally. For water waves, see wave spectrum and tide spectrum. Sound and non-audible acoustic waves can also be characterized in terms of its spectral density, for example, timbre and musical acoustics. Acoustical measurement In acoustics, a spectrogram is a visual representation of the frequency spectrum of sound as a function of time or another variable. A source of sound can have many different frequencies mixed. A musical tone's timbre is characterized by its harmonic spectrum. Sound in our environment that we refer to as noise includes many different frequencies. When a sound signal contains a mixture of all audible frequencies, distributed equally over the audio spectrum, it is called white noise. The spectrum analyzer is an instrument which can be used to convert the sound wave of the musical note into a visual display of the constituent frequencies. This visual display is referred to as an acoustic spectrogram. Software based audio spectrum analyzers are available at low cost, providing easy access not only to industry professionals, but also to academics, students and the hobbyist. The acoustic spectrogram generated by the spectrum analyzer provides an acoustic signature of the musical note. In addition to revealing the fundamental frequency and its overtones, the spectrogram is also useful for analysis of the temporal attack, decay, sustain, and release of the musical note. Continuous versus discrete spectra In the physical sciences, the spectrum of a physical quantity (such as energy) may be called continuous if it is non-zero over the whole spectrum domain (such as frequency or wavelength) or discrete if it attains non-zero values only in a discrete set over the independent variable, with band gaps between pairs of spectral bands or spectral lines. The classical example of a continuous spectrum, from which the name is derived, is the part of the spectrum of the light emitted by excited atoms of hydrogen that is due to free electrons becoming bound to a hydrogen ion and emitting photons, which are smoothly spread over a wide range of wavelengths, in contrast to the discrete lines due to electrons falling from some bound quantum state to a state of lower energy. As in that classical example, the term is most often used when the range of values of a physical quantity may have both a continuous and a discrete part, whether at the same time or in different situations. In quantum systems, continuous spectra (as in bremsstrahlung and thermal radiation) are usually associated with free particles, such as atoms in a gas, electrons in an electron beam, or conduction band electrons in a metal. In particular, the position and momentum of a free particle has a continuous spectrum, but when the particle is confined to a limited space its spectrum becomes discrete. Often a continuous spectrum may be just a convenient model for a discrete spectrum whose values are too close to be distinguished, as in the phonons in a crystal. The continuous and discrete spectra of physical systems can be modeled in functional analysis as different parts in the decomposition of the spectrum of a linear operator acting on a function space, such as the Hamiltonian operator. The classical example of a discrete spectrum (for which the term was first used) is the characteristic set of discrete spectral lines seen in the emission spectrum and absorption spectrum of isolated atoms of a chemical element, which only absorb and emit light at particular wavelengths. The technique of spectroscopy is based on this phenomenon. Discrete spectra are seen in many other phenomena, such as vibrating strings, microwaves in a metal cavity, sound waves in a pulsating star, and resonances in high-energy particle physics. The general phenomenon of discrete spectra in physical systems can be mathematically modeled with tools of functional analysis, specifically by the decomposition of the spectrum of a linear operator acting on a functional space. In classical mechanics In classical mechanics, discrete spectra are often associated to waves and oscillations in a bounded object or domain. Mathematically they can be identified with the eigenvalues of differential operators that describe the evolution of some continuous variable (such as strain or pressure) as a function of time and/or space. Discrete spectra are also produced by some non-linear oscillators where the relevant quantity has a non-sinusoidal waveform. Notable examples are the sound", "canonical_url": "https://en.wikipedia.org/wiki/Spectrum_(physical_sciences)", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:20.360044", "metadata": {"word_count": 150, "text_length": 11679}}
{"id": "wiki_atmospheric_windows", "query_word": "atmospheric windows", "title": "Atmospheric window", "summary": "An atmospheric window is a region of the electromagnetic spectrum that can pass through the atmosphere of Earth. The optical, infrared and radio windows comprise the three main atmospheric windows. The windows provide direct channels for Earth's surface to receive electromagnetic energy from the Sun, and for thermal radiation from the surface to leave to space. Atmospheric windows are useful for astronomy, remote sensing, telecommunications and other science and technology applications. In the study of the greenhouse effect, the term atmospheric window may be limited to mean the infrared window, which is the primary escape route for a fraction of the thermal radiation emitted near the surface. In other fields of science and technology, such as radio astronomy and remote sensing, the term is used as a hypernym, covering the whole electromagnetic spectrum as in the present article.", "text": "An atmospheric window is a region of the electromagnetic spectrum that can pass through the atmosphere of Earth. The optical, infrared and radio windows comprise the three main atmospheric windows. The windows provide direct channels for Earth's surface to receive electromagnetic energy from the Sun, and for thermal radiation from the surface to leave to space. Atmospheric windows are useful for astronomy, remote sensing, telecommunications and other science and technology applications. In the study of the greenhouse effect, the term atmospheric window may be limited to mean the infrared window, which is the primary escape route for a fraction of the thermal radiation emitted near the surface. In other fields of science and technology, such as radio astronomy and remote sensing, the term is used as a hypernym, covering the whole electromagnetic spectrum as in the present article. Role in Earth's energy budget Atmospheric windows, especially the optical and infrared, affect the distribution of energy flows and temperatures within Earth's energy balance. The windows are themselves dependent upon clouds, water vapor, trace greenhouse gases, and other components of the atmosphere. Out of an average 340 watts per square meter (W/m2) of solar irradiance at the top of the atmosphere, about 200 W/m2 reaches the surface via windows, mostly the optical and infrared. Also, out of about 340 W/m2 of reflected shortwave (105 W/m2) plus outgoing longwave radiation (235 W/m2), 80-100 W/m2 exits to space through the infrared window depending on cloudiness. About 40 W/m2 of this transmitted amount is emitted by the surface, while most of the remainder comes from lower regions of the atmosphere. In a complementary manner, the infrared window also transmits to the surface a portion of down-welling thermal radiation that is emitted within colder upper regions of the atmosphere. The \"window\" concept is useful to provide qualitative insight into some important features of atmospheric radiation transport. Full characterization of the absorption, emission, and scattering coefficients of the atmospheric medium is needed in order to perform a rigorous quantitative analysis (typically done with atmospheric radiative transfer codes). Application of the Beer-Lambert Law may yield sufficient quantitative estimates for wavelengths where the atmosphere is optically thin. Window properties are mostly encoded within the absorption profile. Other applications In astronomy Up until the 1940s, astronomers used optical telescopes to observe distant astronomical objects whose radiation reached the earth through the optical window. After that time, the development of radio telescopes gave rise to the more successful field of radio astronomy that is based on the analysis of observations made through the radio window. In telecommunications Communications satellites greatly depend on the atmospheric windows for the transmission and reception of signals: the satellite-ground links are established at frequencies that fall within the spectral bandwidth of atmospheric windows. Shortwave radio does the opposite, using frequencies that produce skywaves rather than those that escape through the radio windows. In remote sensing Both active (signal emitted by satellite or aircraft, reflection detected by sensor) and passive (reflection of sunlight detected by the sensor) remote sensing techniques work with wavelength ranges contained in the atmospheric windows. See also Optical window Infrared window Radio window Water window, for soft x-rays == References ==", "canonical_url": "https://en.wikipedia.org/wiki/Atmospheric_window", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:21.324472", "metadata": {"word_count": 138, "text_length": 3572}}
{"id": "wiki_atmospheric_absorption", "query_word": "atmospheric absorption", "title": "Extinction (astronomy)", "summary": "In astronomy, extinction is the absorption and scattering of electromagnetic radiation by dust and gas between an emitting astronomical object and the observer. Interstellar extinction was first documented as such in 1930 by Robert Julius Trumpler. However, its effects had been noted in 1847 by Friedrich Georg Wilhelm von Struve, and its effect on the colors of stars had been observed by a number of individuals who did not connect it with the general presence of galactic dust. For stars lying near the plane of the Milky Way which are within a few thousand parsecs of the Earth, extinction in the visual band of frequencies (photometric system) is roughly 1.8 magnitudes per kiloparsec. For Earth-bound observers, extinction arises both from the interstellar medium and the Earth's atmosphere; it may also arise from circumstellar dust around an observed object. Strong extinction in Earth's atmosphere of some wavelength regions (such as X-ray, ultraviolet, and infrared) is overcome by the use of space-based observatories. Since blue light is much more strongly attenuated than red light, extinction causes objects to appear redder than expected; this phenomenon is called interstellar reddening.", "text": "In astronomy, extinction is the absorption and scattering of electromagnetic radiation by dust and gas between an emitting astronomical object and the observer. Interstellar extinction was first documented as such in 1930 by Robert Julius Trumpler. However, its effects had been noted in 1847 by Friedrich Georg Wilhelm von Struve, and its effect on the colors of stars had been observed by a number of individuals who did not connect it with the general presence of galactic dust. For stars lying near the plane of the Milky Way which are within a few thousand parsecs of the Earth, extinction in the visual band of frequencies (photometric system) is roughly 1.8 magnitudes per kiloparsec. For Earth-bound observers, extinction arises both from the interstellar medium and the Earth's atmosphere; it may also arise from circumstellar dust around an observed object. Strong extinction in Earth's atmosphere of some wavelength regions (such as X-ray, ultraviolet, and infrared) is overcome by the use of space-based observatories. Since blue light is much more strongly attenuated than red light, extinction causes objects to appear redder than expected; this phenomenon is called interstellar reddening. Interstellar reddening Interstellar reddening is a phenomenon associated with interstellar extinction where the spectrum of electromagnetic radiation from a radiation source changes characteristics from that which the object originally emitted. Reddening occurs due to the light scattering off dust and other matter in the interstellar medium. Interstellar reddening is a different phenomenon from redshift, which is the proportional frequency shifts of spectra without distortion. Reddening preferentially removes shorter wavelength photons from a radiated spectrum while leaving behind the longer wavelength photons, leaving the spectroscopic lines unchanged. In most photometric systems, filters (passbands) are used from which readings of magnitude of light may take account of latitude and humidity among terrestrial factors. Interstellar reddening equates to the \"color excess\", defined as the difference between an object's observed color index and its intrinsic color index (sometimes referred to as its normal color index). The latter is the theoretical value which it would have if unaffected by extinction. In the first system, the UBV photometric system devised in the 1950s and its most closely related successors, the object's color excess E B − V {\\displaystyle E_{B-V}} is related to the object's B−V color (calibrated blue minus calibrated visible) by: E B − V = ( B − V ) observed − ( B − V ) intrinsic {\\displaystyle E_{B-V}=(B-V)_{\\textrm {observed}}-(B-V)_{\\textrm {intrinsic}}\\,} For an A0-type main sequence star (these have median wavelength and heat among the main sequence) the color indices are calibrated at 0 based on an intrinsic reading of such a star (± exactly 0.02 depending on which spectral point, i.e. precise passband within the abbreviated color name is in question, see color index). At least two and up to five measured passbands in magnitude are then compared by subtraction: U, B, V, I, or R during which the color excess from extinction is calculated and deducted. The name of the four sub-indices (R minus I etc.) and order of the subtraction of recalibrated magnitudes is from right to immediate left within this sequence. General characteristics Interstellar reddening occurs because interstellar dust absorbs and scatters blue light waves more than red light waves, making stars appear redder than they are. This is similar to the effect seen when dust particles in the atmosphere of Earth contribute to red sunsets. Broadly speaking, interstellar extinction is strongest at short wavelengths, generally observed by using techniques from spectroscopy. Extinction results in a change in the shape of an observed spectrum. Superimposed on this general shape are absorption features (wavelength bands where the intensity is lowered) that have a variety of origins and can give clues as to the chemical composition of the interstellar material, e.g. dust grains. Known absorption features include the 2175 Å bump, the diffuse interstellar bands, the 3.1 μm water ice feature, and the 10 and 18 μm silicate features. In the solar neighborhood, the rate of interstellar extinction in the Johnson–Cousins V-band (visual filter) averaged at a wavelength of 540 nm is usually taken to be 0.7–1.0 mag/kpc−simply an average due to the clumpiness of interstellar dust. In general, however, this means that a star will have its brightness reduced by about a factor of 2 in the V-band viewed from a good night sky vantage point on earth for every kiloparsec (3,260 light years) it is farther away from us. The amount of extinction can be significantly higher than this in specific directions. For example, some regions of the Galactic Center are awash with obvious intervening dark dust from our spiral arm (and perhaps others) and themselves in a bulge of dense matter, causing as much as more than 30 magnitudes of extinction in the optical, meaning that less than 1 optical photon in 1012 passes through. This results in the zone of avoidance, where our view of the extra-galactic sky is severely hampered, and background galaxies, such as Dwingeloo 1, were only discovered recently through observations in radio and infrared. The general shape of the ultraviolet through near-infrared (0.125 to 3.5 μm) extinction curve (plotting extinction in magnitude against wavelength, often inverted) looking from our vantage point at other objects in the Milky Way, is fairly well characterized by the stand-alone parameter of relative visibility (of such visible light) R(V) (which is different along different lines of sight), but there are known deviations from this characterization. Extending the extinction law into the mid-infrared wavelength range is difficult due to the lack of suitable targets and various contributions by absorption features. R(V) compares aggregate and particular extinctions. It is A V / E ( B − V ) {\\displaystyle A_{V}/E(B-V)\\,} Restated, it is the total extinction, A(V) divided by the selective total extinction (A(B)−A(V)) of those two wavelengths (bands). A(B) and A(V) are the total extinction at the B and V filter bands. Another measure used in the literature is the absolute extinction A(λ)/A(V) at wavelength λ, comparing the total extinction at that wavelength to that at the V band. R(V) is known to be correlated with the average size of the dust grains causing the extinction. For the Milky Way Galaxy, the typical value for R(V) is 3.1, but is found to vary considerably across different lines of sight. As a result, when computing cosmic distances it can be advantageous to move to star data from the near-infrared (of which the filter or passband Ks is quite standard) where the variations and amount of extinction are significantly less, and similar ratios as to R(Ks): 0.49±0.02 and 0.528±0.015 were found respectively by independent groups. Those two more modern findings differ substantially relative to the commonly referenced historical value ≈0.7. The relationship between the total extinction, A(V) (measured in magnitudes), and the column density of neutral hydrogen atoms column, NH (usually measured in cm−2), shows how the gas and dust in the interstellar medium are related. From studies using ultraviolet spectroscopy of reddened stars and X-ray scattering halos in the Milky Way, Predehl and Schmitt found the relationship between NH and A(V) to be approximately: N H A ( V ) ≈ 1.8 × 10 21 atoms cm − 2 mag − 1 {\\displaystyle {\\frac {N_{H}}{A(V)}}\\approx 1.8\\times 10^{21}~{\\mbox{atoms}}~{\\mbox{cm}}^{-2}~{\\mbox{mag}}^{-1}} (see also:). Astronomers have determined the three-dimensional distribution of extinction in the \"solar circle\" (our region of our galaxy), using visible and near-infrared stellar observations and a model of distribution of stars. The dust causing extinction mainly lies along the spiral arms, as observed in other spiral galaxies. Measuring extinction towards an object To measure the extinction curve for a star, the star's spectrum is compared to the observed spectrum of a similar star known not to be affected by extinction (unreddened). It is also possible to use a theoretical spectrum instead of the observed spectrum for the comparison, but this is less common. In the case of emission nebulae, it is common to look at the ratio of two emission lines which should not be affected by the temperature and density in the nebula. For example, the ratio of hydrogen-alpha to hydrogen-beta emission is always around 2.85 under a wide range of conditions prevailing in nebulae. A ratio other than 2.85 must therefore be due to extinction, and the amount of extinction can thus be calculated. The 2175-angstrom feature One prominent feature in measured extinction curves of many objects within the Milky Way is a broad 'bump' at about 2175 Å, well into the ultraviolet region of the electromagnetic spectrum. This feature was first observed in the 1960s, but its origin is still not well understood. Several models have been presented to account for this bump which include graphitic grains with a mixture of PAH molecules. Investigations of interstellar grains embedded in interplanetary dust particles (IDP) observed this feature and identified the carrier with organic carbon and amorphous silicates present in the grains. Extinction curves of other galaxies The form of the standard extinction curve depends on the composition of the ISM, which varies from galaxy to galaxy. In the Local Group, the best-determined extinction curves are those of the Milky Way, the Small Magellanic Cloud (SMC) and the Large Magellanic Cloud (LMC). In the LMC, there is significant variation in the characteristics of the ultraviolet extinction with a weaker 2175 Å bump and stronger far-UV extinction in the region associated with the LMC2 supershell (near the 30 Doradus starbursting regio", "canonical_url": "https://en.wikipedia.org/wiki/Extinction_(astronomy)", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:22.511094", "metadata": {"word_count": 186, "text_length": 13538}}
{"id": "wiki_bandpass_filter", "query_word": "bandpass filter", "title": "Band-pass filter", "summary": "A band-pass filter or bandpass filter (BPF) is a device that passes frequencies within a certain range and rejects (attenuates) frequencies outside that range. It is the inverse of a band-stop filter.", "text": "A band-pass filter or bandpass filter (BPF) is a device that passes frequencies within a certain range and rejects (attenuates) frequencies outside that range. It is the inverse of a band-stop filter. Description In electronics and signal processing, a filter is usually a two-port circuit or device which removes frequency components of a signal (an alternating voltage or current). A band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. This contrasts with a high-pass filter, which allows through components with frequencies above a specific frequency, and a low-pass filter, which allows through components with frequencies below a specific frequency. In digital signal processing, in which signals represented by digital numbers are processed by computer programs, a band-pass filter is a computer algorithm that performs the same function. The term band-pass filter is also used for optical filters, sheets of colored material which allow through a specific band of light frequencies, commonly used in photography and theatre lighting, and acoustic filters which allow through sound waves of a specific band of frequencies. An example of an analogue electronic band-pass filter is an RLC circuit (a resistor–inductor–capacitor circuit). These filters can also be created by combining a low-pass filter with a high-pass filter. A bandpass signal is a signal containing a band of frequencies not adjacent to zero frequency, such as a signal that comes out of a bandpass filter. An ideal bandpass filter would have a completely flat passband: all frequencies within the passband would be passed to the output without amplification or attenuation, and would completely attenuate all frequencies outside the passband. In practice, no bandpass filter is ideal. The filter does not attenuate all frequencies outside the desired frequency range completely; in particular, there is a region just outside the intended passband where frequencies are attenuated, but not rejected. This is known as the filter roll-off, and it is usually expressed in dB of attenuation per octave or decade of frequency. Generally, the design of a filter seeks to make the roll-off as narrow as possible, thus allowing the filter to perform as close as possible to its intended design. Often, this is achieved at the expense of pass-band or stop-band ripple. The bandwidth of the filter is simply the difference between the upper and lower cutoff frequencies. The shape factor is the ratio of bandwidths measured using two different attenuation values to determine the cutoff frequency, e.g., a shape factor of 2:1 at 30/3 dB means the bandwidth measured between frequencies at 30 dB attenuation is twice that measured between frequencies at 3 dB attenuation. Q factor A band-pass filter can be characterized by its Q factor. The Q-factor is the reciprocal of the fractional bandwidth. A high-Q filter will have a narrow passband and a low-Q filter will have a wide passband. These are respectively referred to as narrow-band and wide-band filters. Applications Bandpass filters are widely used in wireless transmitters and receivers. The main function of such a filter in a transmitter is to limit the bandwidth of the output signal to the band allocated for the transmission. This prevents the transmitter from interfering with other stations. In a receiver, a bandpass filter allows signals within a selected range of frequencies to be heard or decoded, while preventing signals at unwanted frequencies from getting through. Signals at frequencies outside the band which the receiver is tuned at, can either saturate or damage the receiver. Additionally they can create unwanted mixing products that fall in band and interfere with the signal of interest. Wideband receivers are particularly susceptible to such interference. A bandpass filter also optimizes the signal-to-noise ratio and sensitivity of a receiver. In both transmitting and receiving applications, well-designed bandpass filters, having the optimum bandwidth for the mode and speed of communication being used, maximize the number of signal transmitters that can exist in a system, while minimizing the interference or competition among signals. Outside of electronics and signal processing, one example of the use of band-pass filters is in the atmospheric sciences. It is common to band-pass filter recent meteorological data with a period range of, for example, 3 to 10 days, so that only cyclones remain as fluctuations in the data fields. Loudspeaker enclosures Compound or band-pass A 4th order electrical bandpass filter can be simulated by a vented box in which the contribution from the rear face of the driver cone is trapped in a sealed box, and the radiation from the front surface of the cone is into a ported chamber. This modifies the resonance of the driver. In its simplest form a compound enclosure has two chambers. The dividing wall between the chambers holds the driver; typically only one chamber is ported. If the enclosure on each side of the woofer has a port in it then the enclosure yields a 6th order band-pass response. These are considerably harder to design and tend to be very sensitive to driver characteristics. As in other reflex enclosures, the ports may generally be replaced by passive radiators if desired. An eighth order bandpass box is another variation which also has a narrow frequency range. They are often used in sound pressure level competitions, in which case a bass tone of a specific frequency would be used versus anything musical. They are complicated to build and must be done quite precisely in order to perform nearly as intended. Economics Bandpass filters can also be used outside of engineering-related disciplines. A leading example is the use of bandpass filters to extract the business cycle component in economic time series. This reveals more clearly the expansions and contractions in economic activity that dominate the lives of the public and the performance of diverse firms, and therefore is of interest to a wide audience of economists and policy-makers, among others. Economic data usually has quite different statistical properties than data in say, electrical engineering. It is very common for a researcher to directly carry over traditional methods such as the \"ideal\" filter, which has a perfectly sharp gain function in the frequency domain. However, in doing so, substantial problems can arise that can cause distortions and make the filter output extremely misleading. As a poignant and simple case, the use of an \"ideal\" filter on white noise (which could represent for example stock price changes) creates a false cycle. The use of the nomenclature \"ideal\" implicitly involves a greatly fallacious assumption except on scarce occasions. Nevertheless, the use of the \"ideal\" filter remains common despite its limitations. Fortunately, band-pass filters are available that steer clear of such errors, adapt to the data series at hand, and yield more accurate assessments of the business cycle fluctuations in major economic series like Real GDP, Investment, and Consumption - as well as their sub-components. An early work, published in the Review of Economics and Statistics in 2003, more effectively handles the kind of data (stochastic rather than deterministic) arising in macroeconomics. In this paper entitled \"General Model-Based Filters for Extracting Trends and Cycles in Economic Time Series\", Andrew Harvey and Thomas Trimbur develop a class of adaptive band pass filters. These have been successfully applied in various situations involving business cycle movements in myriad nations in the international economy. 4G and 5G wireless communications Band pass filters can be implemented in 4G and 5G wireless communication systems. Hussaini et al.(2015) stated that, in the application of wireless communication, radio frequency noise is a major concern. In the current development of 5G technology, planer band pass filters are used to suppress RF noises and removing unwanted signals. Combine, hairpin, parallel-coupled line, step impedance and stub impedance are the designs of experimenting the band pass filter to achieve low insertion loss with a compact size. The necessity of adopting asymmetric frequency response is in behalf of reducing the number of resonators, insertion loss, size and cost of circuit production. 4-pole cross-coupled band pass filter is designed by Hussaini et al.(2015). This band pass filter is designed to cover the 2.5-2.6 GHz and 3.4-3.7 GHz spectrum for the 4G and 5G wireless communication applications respectively. It is developed and extended from 3-pole single-band band pass filter, where an additional resonator is applied to a 3-pole single-band band pass filter. The advanced band pass filter has a compact size with a simple structure, which is convenient for implementation. Moreover, the stop band rejection and selectivity present a good performance in RF noise suppression. Insertion loss is very low when covering the 4G and 5G spectrum, while providing good return loss and group delay. Energy scavengers Energy scavengers are devices that search for energy from the environment efficiently. Band pass filters can be implemented to energy scavengers by converting energy generated from vibration into electric energy. The band pass filter designed by Shahruz (2005), is an ensemble of cantilever beams, which is called the beam-mass system. Ensemble of beam-mass systems can be transformed into a band pass filter when appropriate dimensions of beams and masses are chosen. Although the process of designing a mechanical band pass filter is advanced, further study and work are still required to design more flexible band pass filters to suit large frequency intervals. This mechanical band pass filter could be used on vibration sources with distinct peak-power frequencies. Other fields In neuroscie", "canonical_url": "https://en.wikipedia.org/wiki/Band-pass_filter", "source": "wikipedia", "language": "en", "collected_at": "2025-12-03T17:12:23.517353", "metadata": {"word_count": 32, "text_length": 10575}}
